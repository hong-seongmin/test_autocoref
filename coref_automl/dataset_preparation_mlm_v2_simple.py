# coref_automl/dataset_preparation_mlm_v2_simple.py
"""
ì´ˆê°„ì†Œí™” ë°ì´í„°ì…‹ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (wiki + HPLT ì „ìš©)
- í’ˆì§ˆ ë¶„ì„ ì—†ìŒ
- ì²­í‚¹ ì—†ìŒ
- ë‹¨ìˆœ truncationë§Œ
- ë¹ ë¥¸ ë°ì´í„°ì…‹ ìƒì„±
"""

from __future__ import annotations
import os
import multiprocessing
from dataclasses import dataclass
from typing import List, Optional

from datasets import load_dataset, concatenate_datasets, disable_caching, Dataset
from transformers import AutoTokenizer
from tqdm import tqdm

disable_caching()


@dataclass
class DatasetSource:
    """ë°ì´í„°ì…‹ ì†ŒìŠ¤ ì„¤ì •"""
    name: str
    source: str
    subset: Optional[str]
    split: str
    domain: str
    description: str
    is_streaming: bool = False


def get_simple_dataset_sources() -> List[DatasetSource]:
    """ê°„ì†Œí™”: wiki + HPLTë§Œ"""
    return [
        # Wikipedia - ê¸´ ë¬¸ì„œ, ë‹¤ì–‘í•œ ì£¼ì œ
        DatasetSource(
            name="wiki_ko",
            source="wikimedia/wikipedia",
            subset="20231101.ko",
            split="train",
            domain="encyclopedia",
            description="í•œêµ­ì–´ ìœ„í‚¤ë°±ê³¼"
        ),

        # HPLT Korean (ëŒ€ê·œëª¨ ì›¹ í¬ë¡¤)
        DatasetSource(
            name="hplt_korean",
            source="HPLT/HPLT2.0_cleaned",
            subset="kor_Hang",
            split="train",
            domain="hplt_general",
            description="HPLT ëŒ€ê·œëª¨ í•œêµ­ì–´ ì›¹ í¬ë¡¤",
            is_streaming=True
        ),
    ]


def simple_preprocess(
    dataset,
    source: DatasetSource,
    tokenizer,
    target_seq_len: int,
    min_length: int = 50
) -> List[str]:
    """
    ì´ˆê°„ì†Œí™” ì „ì²˜ë¦¬
    - í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - ìµœì†Œ ê¸¸ì´ í•„í„°ë§
    - truncationë§Œ
    """

    processed_texts = []
    skipped = 0
    truncated = 0

    print(f"  ğŸ“ ì „ì²˜ë¦¬ ì¤‘...", flush=True)

    for item in tqdm(dataset, desc=f"  ì²˜ë¦¬", unit="ìƒ˜í”Œ"):
        try:
            # ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if source.domain == "encyclopedia":
                text = item.get("text", "").strip()
            elif source.domain == "hplt_general":
                text = item.get("text", "").strip()
            else:
                text = item.get("text", "").strip()

            # ê¸°ë³¸ í•„í„°ë§: ìµœì†Œ ê¸¸ì´ë§Œ ì²´í¬
            if not text or len(text) < min_length:
                skipped += 1
                continue

            # í† í° ê¸¸ì´ ì²´í¬
            tokens = tokenizer.encode(text, add_special_tokens=False, truncation=False)
            token_len = len(tokens)

            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if token_len < 50:
                skipped += 1
                continue

            # ì ì ˆí•œ ê¸¸ì´: ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if token_len <= target_seq_len:
                processed_texts.append(text)
            # ê¸´ ê²½ìš°: truncateë§Œ
            else:
                truncated_ids = tokens[:target_seq_len]
                truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)
                processed_texts.append(truncated_text)
                truncated += 1

        except Exception as e:
            skipped += 1
            continue

    print(f"  âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_texts):,}ê°œ ìƒì„± (ìŠ¤í‚µ: {skipped:,}, ì˜ë¦¼: {truncated:,})")
    return processed_texts


def load_simple_dataset(
    source: DatasetSource,
    tokenizer,
    target_seq_len: int,
    limit: Optional[int] = None
) -> Optional[Dataset]:
    """ê°„ì†Œí™”: ë‹¨ìˆœ ë¡œë“œ ë° ì „ì²˜ë¦¬"""

    try:
        print(f"\nğŸ“¥ {source.name}: {source.description}")

        # ë°ì´í„° ë¡œë“œ
        load_kwargs = {"split": source.split}
        if source.subset:
            load_kwargs["name"] = source.subset

        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        if source.is_streaming:
            load_kwargs["streaming"] = True
            dataset_stream = load_dataset(source.source, **load_kwargs)

            # limitì´ ì§€ì •ëœ ê²½ìš°ì—ë§Œ ì œí•œ
            if limit:
                print(f"  ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ ì¤‘... (ìµœëŒ€ {limit:,}ê°œ)")
                samples = []
                for i, sample in enumerate(dataset_stream):
                    if i >= limit:
                        break
                    samples.append(sample)

                    if (i + 1) % 10000 == 0:
                        print(f"\r  â³ {i+1:,}/{limit:,} ({(i+1)/limit*100:.1f}%)", end="", flush=True)

                print(f"\r  âœ… ë¡œë“œ ì™„ë£Œ: {len(samples):,} ìƒ˜í”Œ" + " " * 20)
            else:
                # ë¬´ì œí•œì´ì–´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê¸°ë³¸ ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                DEFAULT_STREAMING_LIMIT = 500000
                print(f"  ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ ì¤‘... (ê¸°ë³¸ ì œí•œ: {DEFAULT_STREAMING_LIMIT:,}ê°œ)")
                samples = []
                for i, sample in enumerate(dataset_stream):
                    if i >= DEFAULT_STREAMING_LIMIT:
                        break
                    samples.append(sample)

                    if (i + 1) % 10000 == 0:
                        print(f"\r  â³ ë¡œë“œ ì¤‘: {i+1:,}/{DEFAULT_STREAMING_LIMIT:,} ({(i+1)/DEFAULT_STREAMING_LIMIT*100:.1f}%)", end="", flush=True)

                print(f"\r  âœ… ë¡œë“œ ì™„ë£Œ: {len(samples):,} ìƒ˜í”Œ (HPLT ì œí•œ ì ìš©)" + " " * 20)

            dataset = Dataset.from_list(samples)
        else:
            dataset = load_dataset(source.source, **load_kwargs)

            # ìƒ˜í”Œ ì œí•œ
            if limit and limit < len(dataset):
                dataset = dataset.select(range(limit))

            print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(dataset):,} ìƒ˜í”Œ")

        # ì „ì²˜ë¦¬
        processed_texts = simple_preprocess(dataset, source, tokenizer, target_seq_len)

        if not processed_texts:
            print(f"  âš ï¸ ì „ì²˜ë¦¬ í›„ ìƒ˜í”Œ ì—†ìŒ")
            return None

        # Datasetìœ¼ë¡œ ë³€í™˜
        return Dataset.from_list([{"text": text} for text in processed_texts])

    except Exception as e:
        print(f"  âŒ {source.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def prepare_simple_datasets(
    model_name: str = "kakaobank/kf-deberta-base",
    seq_lengths: List[int] = [1536, 2048],
    save_path: str = "./prepared_datasets_simple",
    max_samples_per_length: Optional[int] = None
):
    """
    ì´ˆê°„ì†Œí™” ë°ì´í„°ì…‹ ì¤€ë¹„
    - wiki + HPLTë§Œ
    - í’ˆì§ˆ ë¶„ì„ ì—†ìŒ
    - ë¹ ë¥¸ ìƒì„±
    """

    os.makedirs(save_path, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print("ğŸš€ ì´ˆê°„ì†Œí™” ë°ì´í„°ì…‹ ì¤€ë¹„ (wiki + HPLT)")
    print(f"ğŸ¯ ëª¨ë¸: {model_name}")
    print(f"ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_lengths}")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")
    if max_samples_per_length:
        print(f"ğŸ“Š ìµœëŒ€ ìƒ˜í”Œ: {max_samples_per_length:,}ê°œ/ê¸¸ì´")
    else:
        print(f"ğŸ“Š ìµœëŒ€ ìƒ˜í”Œ: ë¬´ì œí•œ")
    print("=" * 80)

    sources = get_simple_dataset_sources()

    for seq_len in seq_lengths:
        print(f"\n{'='*80}")
        print(f"ğŸ¯ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len} í† í°")
        print(f"{'='*80}")

        # í† í¬ë‚˜ì´ì € max_length ë™ì  í™•ì¥
        original_max_length = tokenizer.model_max_length
        tokenizer.model_max_length = seq_len
        print(f"   Tokenizer max_length: {original_max_length} â†’ {seq_len}")

        all_datasets = []
        total_samples = 0

        # ê° ì†ŒìŠ¤ë³„ limit ê³„ì‚°
        if max_samples_per_length:
            limit_per_source = max_samples_per_length // len(sources)
            print(f"   ì†ŒìŠ¤ë‹¹ ì œí•œ: {limit_per_source:,}ê°œ")
        else:
            limit_per_source = None
            print(f"   ì†ŒìŠ¤ë‹¹ ì œí•œ: ì—†ìŒ")

        # ë°ì´í„° ë¡œë“œ
        for source in sources:
            dataset = load_simple_dataset(source, tokenizer, seq_len, limit=limit_per_source)

            if dataset and len(dataset) > 0:
                all_datasets.append(dataset)
                total_samples += len(dataset)
                print(f"  ğŸ“Š ëˆ„ì : {total_samples:,} ìƒ˜í”Œ")

        # ë°ì´í„° í†µí•©
        if all_datasets:
            print(f"\nğŸ”„ ë°ì´í„°ì…‹ í†µí•© ì¤‘... ({total_samples:,} ìƒ˜í”Œ)")

            combined_dataset = concatenate_datasets(all_datasets)
            combined_dataset = combined_dataset.shuffle(seed=42)

            print(f"  âœ… í†µí•© ì™„ë£Œ: {len(combined_dataset):,} ìƒ˜í”Œ")

            # ìµœì¢… ìƒ˜í”Œ ì œí•œ
            if max_samples_per_length and len(combined_dataset) > max_samples_per_length:
                print(f"  âœ‚ï¸  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ë¡œ ì œí•œ: {max_samples_per_length:,}")
                combined_dataset = combined_dataset.select(range(max_samples_per_length))

            # í† í°í™”
            print(f"\nğŸ”¤ í† í°í™” ì¤‘... ({len(combined_dataset):,} ìƒ˜í”Œ)")

            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=seq_len
                )

            # ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨)
            if seq_len >= 2048:
                batch_size = 100
                num_proc = 4
            elif seq_len >= 1536:
                batch_size = 200
                num_proc = 8
            else:
                batch_size = 500
                num_proc = 8

            print(f"  âš™ï¸  í† í°í™” ì„¤ì •: batch_size={batch_size}, num_proc={num_proc}")

            tokenized_dataset = combined_dataset.map(
                tokenize_function,
                batched=True,
                batch_size=batch_size,
                remove_columns=["text"],
                num_proc=num_proc,
                desc="í† í°í™”"
            )

            # ì €ì¥
            save_file = f"{save_path}/{model_name.replace('/', '_')}_{seq_len}_simple.arrow"
            print(f"\nğŸ’¾ ì €ì¥ ì¤‘: {save_file}")
            tokenized_dataset.save_to_disk(save_file)

            print(f"âœ… ì €ì¥ ì™„ë£Œ!")
            print(f"ğŸ“Š ìµœì¢…: {len(tokenized_dataset):,} ìƒ˜í”Œ Ã— {seq_len} í† í°")

        else:
            print(f"âš ï¸ {seq_len} í† í° ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")

    print("\n" + "=" * 80)
    print("ğŸŠ ëª¨ë“  ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {save_path}")
    print("ğŸš€ ì´ì œ run_combined_experiment_v2.pyë¡œ í›ˆë ¨í•˜ì„¸ìš”!")
    print("=" * 80)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ì´ˆê°„ì†Œí™” ë°ì´í„°ì…‹ ì¤€ë¹„ (wiki + HPLT)")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument(
        "--seq-len",
        nargs="+",
        type=int,
        default=[1536, 2048],
        help="ì‹œí€€ìŠ¤ ê¸¸ì´ë“¤ (ì˜ˆ: --seq-len 1536 2048)"
    )
    parser.add_argument("--save-path", default="./prepared_datasets_simple", help="ì €ì¥ ê²½ë¡œ")
    parser.add_argument(
        "--max-samples",
        type=lambda x: None if x.lower() == 'none' else int(x),
        default=None,
        help="ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (None=ë¬´ì œí•œ, ì˜ˆ: --max-samples 500000)"
    )

    args = parser.parse_args()

    print(f"\nì‹œì‘ ì„¤ì •:")
    print(f"  - ëª¨ë¸: {args.model}")
    print(f"  - ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_len}")
    print(f"  - ì €ì¥ ê²½ë¡œ: {args.save_path}")
    print(f"  - ìµœëŒ€ ìƒ˜í”Œ: {args.max_samples if args.max_samples else 'ë¬´ì œí•œ'}")
    print()

    prepare_simple_datasets(
        model_name=args.model,
        seq_lengths=args.seq_len,
        save_path=args.save_path,
        max_samples_per_length=args.max_samples
    )

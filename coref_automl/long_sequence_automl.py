# coref_automl/long_sequence_automl.py
"""
DeBERTa Long Sequence AutoML System
- Max length: 1024-2048 tokens
- Enhanced datasets with quality filtering
- Memory-efficient batch configuration
- Advanced hyperparameter optimization
"""

from __future__ import annotations
import os
import gc
import math
import random
import time
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple
from functools import lru_cache

import numpy as np
import optuna
import torch
from datasets import load_dataset, concatenate_datasets, disable_caching
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForMaskedLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    pipeline,
)

from .coref_utils import is_noun
from .callback import LiveMetricsCallback
from .bus import BUS
from .tune import (
    build_eval_from_lambada,
    build_coref_eval_set,
    eval_lambada_topk,
    eval_coref_f1,
    eval_coref_recall_topk,
    DynCollator
)

disable_caching()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enhanced Dataset Loading for Long Sequences
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class DatasetConfig:
    """ë°ì´í„°ì…‹ ì„¤ì •"""
    source: str
    subset: Optional[str]
    split: str
    domain: str
    quality_weight: float
    min_length: int
    max_length: int

def get_long_sequence_dataset_configs(target_seq_len: int) -> List[DatasetConfig]:
    """ê¸´ ì‹œí€€ìŠ¤ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ì„¤ì • - KLUE MRCë§Œ ì‚¬ìš© (NLI ì œì™¸)"""

    base_configs = [
        # KLUE MRCë§Œ ì‚¬ìš© (NLIëŠ” 0% í†µê³¼ìœ¨ë¡œ ì œì™¸)
        DatasetConfig(
            source="klue",
            subset="mrc",
            split="train",
            domain="qa_long",
            quality_weight=1.0,
            min_length=int(target_seq_len * 0.4),  # MRC contextëŠ” ë³´í†µ ê¸¸ì–´ì„œ ë‚®ì€ ë¹„ìœ¨
            max_length=int(target_seq_len * 1.8)
        ),
    ]

    return base_configs

def load_single_dataset(args):
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    config, tokenizer, target_seq_len, limit, skip_quality_analysis = args

    try:
        # ë°ì´í„° ë¡œë“œ
        load_kwargs = {"split": config.split}
        if config.subset:
            load_kwargs["name"] = config.subset

        dataset = load_dataset(config.source, **load_kwargs)

        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if limit and limit <= 10:
            max_samples = min(100, len(dataset))
            dataset = dataset.select(range(max_samples))

        # ë„ë©”ì¸ë³„ ì „ì²˜ë¦¬
        processed_dataset = preprocess_domain_data(dataset, config, tokenizer, target_seq_len, limit, skip_quality_analysis)

        return processed_dataset if processed_dataset and len(processed_dataset) > 0 else None

    except Exception as e:
        print(f"    âŒ Failed to load {config.source}: {e}")
        return None

def load_prepared_dataset(dataset_path: str) -> Dict[str, Any]:
    """ì¤€ë¹„ëœ ë°ì´í„°ì…‹ ë¡œë“œ"""
    from datasets import load_from_disk

    print(f"ğŸ“‚ Loading prepared dataset from {dataset_path}...")
    dataset = load_from_disk(dataset_path)
    print(f"âœ… Loaded {len(dataset)} samples from prepared dataset")

    return dataset

def load_enhanced_dataset(tokenizer, target_seq_len: int, limit: Optional[int] = None, skip_quality_analysis: bool = False) -> Dict[str, Any]:
    """í–¥ìƒëœ ë°ì´í„°ì…‹ ë¡œë”© - ë³‘ë ¬ ì²˜ë¦¬ ê°•í™”"""

    configs = get_long_sequence_dataset_configs(target_seq_len)

    print(f"ğŸ“Š Loading {len(configs)} datasets for {target_seq_len} tokens...")

    # ë³‘ë ¬ ë°ì´í„° ë¡œë“œ (ThreadPoolExecutor ì‚¬ìš©)
    max_workers = min(len(configs), multiprocessing.cpu_count(), 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤

    print(f"ğŸš€ Parallel dataset loading: {max_workers} workers")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        args_list = [(config, tokenizer, target_seq_len, limit, skip_quality_analysis) for config in configs]
        results = list(executor.map(load_single_dataset, args_list))

    # ìœ íš¨í•œ ë°ì´í„°ì…‹ë§Œ í•„í„°ë§
    all_datasets = [ds for ds in results if ds is not None]

    if not all_datasets:
        raise RuntimeError("No datasets could be loaded")

    print(f"ğŸ”„ Combining {len(all_datasets)} datasets...")
    # ë°ì´í„° í†µí•©
    combined_dataset = concatenate_datasets(all_datasets)
    combined_dataset = combined_dataset.shuffle(seed=42)

    original_size = len(combined_dataset)
    if limit and len(combined_dataset) > limit:
        combined_dataset = combined_dataset.select(range(limit))
        print(f"âœ‚ï¸  Limited to {len(combined_dataset)}/{original_size} samples")

    print(f"ğŸ”¤ Tokenizing {len(combined_dataset)} samples...")

    # í† í°í™” í•¨ìˆ˜
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=target_seq_len,
            return_tensors="pt"
        )

    # ì‹œìŠ¤í…œ ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
    num_proc = min(multiprocessing.cpu_count(), 16)  # ìµœëŒ€ 16ê°œ í”„ë¡œì„¸ìŠ¤
    batch_size = 1000  # ë°°ì¹˜ í¬ê¸° ì¦ê°€

    print(f"ğŸ”„ Tokenizing with {num_proc} processes, batch_size={batch_size}...")

    tokenized_dataset = combined_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=batch_size,
        remove_columns=["text"],
        num_proc=num_proc
    )

    print(f"ğŸ‰ Dataset ready: {len(tokenized_dataset)} samples Ã— {target_seq_len} tokens")
    return tokenized_dataset

@lru_cache(maxsize=5000)
def analyze_coref_quality_cached(text: str) -> Dict[str, float]:
    """Kiwië¥¼ ì‚¬ìš©í•œ Coreference í’ˆì§ˆ ë¶„ì„ (ìºì‹± ì ìš©)"""
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
    except ImportError:
        # Kiwiê°€ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        return {
            'pronoun_density': calculate_simple_pronoun_density(text),
            'entity_density': calculate_simple_entity_density(text),
            'coref_score': calculate_simple_coref_score(text),
            'pronoun_count': 0,
            'entity_count': 0,
            'total_words': len(text.split())
        }

    # í˜•íƒœì†Œ ë¶„ì„
    tokens = kiwi.tokenize(text)

    # ëŒ€ëª…ì‚¬ì™€ ê°œì²´ ë¶„ì„
    pronouns = []
    entities = []

    for token in tokens:
        # Kiwi íƒœê·¸ ì„¤ëª…:
        # NP: ëŒ€ëª…ì‚¬ (ê·¸, ê·¸ë…€, ì´ê²ƒ ë“±)
        # NNG/NNP: ì¼ë°˜ëª…ì‚¬/ê³ ìœ ëª…ì‚¬
        # NNB: ì˜ì¡´ëª…ì‚¬
        if token.tag == 'NP':  # ëŒ€ëª…ì‚¬
            pronouns.append(token.form)
        elif token.tag in ['NNG', 'NNP', 'NNB']:  # ëª…ì‚¬ë¥˜
            if len(token.form) > 1:  # í•œ ê¸€ì ì œì™¸
                entities.append(token.form)

    # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)
    meaningful_words = len([t for t in tokens if t.tag.startswith(('N', 'V', 'M', 'VA', 'VV'))])

    pronoun_density = len(pronouns) / max(1, meaningful_words)
    entity_density = len(entities) / max(1, meaningful_words)

    # Coreference ì ìˆ˜ ê³„ì‚° (ëŒ€ëª…ì‚¬-ê°œì²´ ìƒí˜¸ì‘ìš©)
    coref_score = min(1.0, (pronoun_density * 20) + (entity_density * 3) + (pronoun_density * entity_density * 50))

    return {
        'pronoun_density': pronoun_density,
        'entity_density': entity_density,
        'coref_score': coref_score,
        'pronoun_count': len(pronouns),
        'entity_count': len(entities),
        'total_words': meaningful_words
    }

def analyze_coref_quality(text: str) -> Dict[str, float]:
    """ìºì‹±ëœ í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜"""
    return analyze_coref_quality_cached(text)

def batch_analyze_coref_quality(texts: List[str], batch_size: int = 50, max_workers: Optional[int] = None) -> List[Dict[str, float]]:
    """ë°°ì¹˜ ë‹¨ìœ„ í’ˆì§ˆ ë¶„ì„ (ThreadPoolExecutor ì‚¬ìš©)"""
    if max_workers is None:
        max_workers = min(multiprocessing.cpu_count(), 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤

    results = []

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            batch_results = list(executor.map(analyze_coref_quality_cached, batch_texts))
            results.extend(batch_results)

    return results

def calculate_simple_pronoun_density(text: str) -> float:
    """Kiwi ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ëŒ€ëª…ì‚¬ ë°€ë„ ê³„ì‚°"""
    simple_pronouns = ['ê·¸', 'ê·¸ë…€', 'ê·¸ê²ƒ', 'ì´', 'ê·¸ì˜', 'ê·¸ë…€ì˜', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ë“¤', 'ì´ë“¤']
    words = text.split()
    if not words:
        return 0.0
    pronoun_count = sum(1 for word in words if word in simple_pronouns)
    return pronoun_count / len(words)

def calculate_simple_entity_density(text: str) -> float:
    """ê°„ë‹¨í•œ ê°œì²´ ë°€ë„ ê³„ì‚°"""
    # í•œêµ­ì–´ ê°œì²´ ë‹¨ì„œ (ë§¤ìš° ë‹¨ìˆœí™”)
    entity_indicators = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ']
    words = text.split()
    if not words:
        return 0.0
    entity_count = sum(1 for word in words if any(ind in word for ind in entity_indicators))
    return entity_count / len(words)

def calculate_simple_coref_score(text: str) -> float:
    """ê°„ë‹¨í•œ coreference ì ìˆ˜ ê³„ì‚°"""
    pronoun_density = calculate_simple_pronoun_density(text)
    entity_density = calculate_simple_entity_density(text)
    # ëŒ€ëª…ì‚¬-ê°œì²´ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•œ ì ìˆ˜
    return min(1.0, (pronoun_density * 20) + (entity_density * 3) + (pronoun_density * entity_density * 50))

def preprocess_domain_data(dataset, config: DatasetConfig, tokenizer, target_seq_len: int, limit: Optional[int] = None, skip_quality_analysis: bool = False):
    """ë„ë©”ì¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬ - Coreference íŠ¹í™” (ì‹¤ì‹œê°„ ì§„í–‰ í‘œì‹œ)"""

    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False

    processed_texts = []
    quality_stats = {'total': 0, 'passed': 0, 'avg_pronoun_density': 0, 'avg_coref_score': 0}

    # ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    if limit and limit <= 10:  # ì•„ì£¼ ì‘ì€ limitì¸ ê²½ìš° ê° ë°ì´í„°ì…‹ë„ ì œí•œ
        max_samples = min(100, len(dataset))  # ê° ë°ì´í„°ì…‹ë‹¹ ìµœëŒ€ 100ê°œ
    else:
        max_samples = 5000 if config.domain == "wiki_coref" else len(dataset)
    dataset = dataset.select(range(min(max_samples, len(dataset))))

    print(f"  ğŸ” Processing {len(dataset)} samples from {config.domain}...")

    # ì§„í–‰ í‘œì‹œ ì„¤ì •
    iterator = tqdm(dataset, desc=f"    {config.domain}", unit="samples") if use_tqdm else dataset

    print(f"    [DEBUG] Starting to process {len(dataset)} items")
    for item in iterator:
        if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
            print(f"    [DEBUG] Processing item {quality_stats['total'] + 1}")
        try:
            quality_stats['total'] += 1

            # ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if config.domain == "wiki_coref":
                text = item.get("text", "").strip()
                # ì¸ë¬¼/ì‚¬ê±´ ì¤‘ì‹¬ ë¬¸ì„œ í•„í„°ë§
                if not is_coref_rich_wiki(text):
                    continue
            elif config.domain == "news_long":
                # KLUE ynat ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
                if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    print(f"    [DEBUG] YNAT item keys: {list(item.keys())}")
                title = item.get("title", "")
                content = item.get("content", item.get("text", ""))
                text = f"{title} {content}".strip()
                # ê¸´ ë‰´ìŠ¤ ê¸°ì‚¬ ìœ„ì£¼ (500ì ì´ìƒ)
                if len(text) < 500:
                    if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                        print(f"    [DEBUG] YNAT text too short: {len(text)}")
                    continue
            elif config.domain == "qa_long":
                context = item.get("context", "")
                question = item.get("question", "")
                answer = item.get("answers", {}).get("text", [""])[0] if item.get("answers") else ""
                text = f"{context} {question} {answer}".strip()
            elif config.domain == "news_sts":
                # KLUE STS ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
                if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    print(f"    [DEBUG] STS item keys: {list(item.keys())}")
                sentence1 = item.get("sentence1", item.get("text", ""))
                sentence2 = item.get("sentence2", "")
                text = f"{sentence1} {sentence2}".strip()
            elif config.domain == "nli_long":
                # KLUE NLI ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸ ë° ì²˜ë¦¬
                if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    print(f"    [DEBUG] NLI item keys: {list(item.keys())}")
                # KLUE NLI ë°ì´í„°ì…‹ì€ premise/hypothesis êµ¬ì¡°
                premise = item.get("premise", "")
                hypothesis = item.get("hypothesis", "")
                if not premise or not hypothesis:
                    if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                        print(f"    [DEBUG] NLI missing premise/hypothesis: premise='{premise[:50] if premise else 'None'}...', hypothesis='{hypothesis[:50] if hypothesis else 'None'}...'")
                    continue  # premiseë‚˜ hypothesisê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                text = f"{premise} {hypothesis}".strip()
            elif config.domain == "news_topic":
                title = item.get("title", "")
                content = item.get("description", "") + " " + item.get("body", "")
                text = f"{title} {content}".strip()
            else:
                text = item.get("text", "").strip()

            if not text or len(text) < 50:  # STS ë°ì´í„°ì…‹ìš©ìœ¼ë¡œ ìµœì†Œ ê¸¸ì´ ë‚®ì¶¤
                if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    print(f"    [DEBUG] Filtered out due to text length: {len(text) if text else 0}")
                continue

            # í’ˆì§ˆ í•„í„°ë§ ìƒëµ - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ëª¨ë“  ìƒ˜í”Œ í†µê³¼
            # í’ˆì§ˆ í†µê³„ ì—…ë°ì´íŠ¸ (ë”ë¯¸ ê°’ ì‚¬ìš©)
            quality_stats['avg_pronoun_density'] += 0.01
            quality_stats['avg_coref_score'] += 0.5

            # ë””ë²„ê¹…: í†µê³„ ì—…ë°ì´íŠ¸ í™•ì¸
            if quality_stats['total'] <= 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"    [DEBUG] Sample {quality_stats['total']}: stats updated")

            # ê¸¸ì´ í•„í„°ë§
            tokens = tokenizer.encode(text)
            token_len = len(tokens)
            if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                print(f"    [DEBUG] Text length: {len(text)}, Token length: {token_len}, Range: {config.min_length}-{config.max_length}")
            if token_len < config.min_length or token_len > config.max_length:
                if quality_stats['total'] <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    print(f"    [DEBUG] Filtered out due to length")
                continue

            # ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹ (coreference ë§¥ë½ ìœ ì§€)
            if len(tokens) > target_seq_len * 1.2:
                chunks = chunk_with_coref_preservation(text, target_seq_len, tokenizer)
                processed_texts.extend(chunks)
            else:
                processed_texts.append(text)

            quality_stats['passed'] += 1

            # tqdm ì§„í–‰ í‘œì‹œ ì—…ë°ì´íŠ¸
            if use_tqdm and hasattr(iterator, 'set_postfix'):
                pass_rate = quality_stats['passed'] / quality_stats['total']
                iterator.set_postfix({
                    'passed': f"{quality_stats['passed']}/{quality_stats['total']}",
                    'rate': f"{pass_rate:.1%}"
                })

        except Exception as e:
            continue

    if use_tqdm:
        iterator.close()

    # í’ˆì§ˆ í†µê³„ ì¶œë ¥
    if quality_stats['total'] > 0:
        avg_pronoun = quality_stats['avg_pronoun_density'] / quality_stats['total']
        avg_coref = quality_stats['avg_coref_score'] / quality_stats['total']
        pass_rate = quality_stats['passed'] / quality_stats['total']

        print(f"  âœ… {config.domain}: {quality_stats['passed']}/{quality_stats['total']} passed ({pass_rate:.1%})")
        print(f"    ğŸ“Š Avg pronoun density: {avg_pronoun:.3f}, Avg coref score: {avg_coref:.3f}")
    else:
        print(f"  âš ï¸  {config.domain}: No samples processed")

    # í’ˆì§ˆ ê¸°ë°˜ ìƒ˜í”Œë§ (í’ˆì§ˆ ë¶„ì„ ê±´ë„ˆë›°ê¸° ì˜µì…˜)
    if len(processed_texts) > 1000 and not skip_quality_analysis:
        print(f"  ğŸ¯ Quality sampling {len(processed_texts)} â†’ 1000 samples...")

        # ë°°ì¹˜ í¬ê¸° ë° ì›Œì»¤ ìˆ˜ ì„¤ì •
        batch_size = 100  # ë°°ì¹˜ë‹¹ 100ê°œ í…ìŠ¤íŠ¸
        max_workers = min(multiprocessing.cpu_count(), 12)  # ìµœëŒ€ 12ê°œ ì›Œì»¤

        # ë¶„ì„í•  í…ìŠ¤íŠ¸ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        analyze_limit = min(2000, len(processed_texts))
        texts_to_analyze = processed_texts[:analyze_limit]

        print(f"    ğŸš€ Parallel quality analysis: {len(texts_to_analyze)} texts, {max_workers} workers, batch_size={batch_size}")

        # ë°°ì¹˜ ë‹¨ìœ„ ë³‘ë ¬ í’ˆì§ˆ ë¶„ì„ (ì‹¤ì‹œê°„ ì§„í–‰ í‘œì‹œ + ëŒ€ì‹œë³´ë“œ ì—°ë™)
        print(f"    ğŸ” Analyzing {len(texts_to_analyze)} texts for quality...")
        quality_scores = []

        # ì§„í–‰ í‘œì‹œë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
        total_batches = (len(texts_to_analyze) + batch_size - 1) // batch_size
        completed_batches = 0

        for i in range(0, len(texts_to_analyze), batch_size):
            batch_texts = texts_to_analyze[i:i + batch_size]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                batch_results = list(executor.map(analyze_coref_quality_cached, batch_texts))
                quality_scores.extend(batch_results)

            completed_batches += 1
            progress = completed_batches / total_batches
            processed_count = len(quality_scores)

            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"    ğŸ“ˆ Quality analysis: {completed_batches}/{total_batches} batches ({progress:.1%}) - {processed_count}/{len(texts_to_analyze)} texts")

            # ëŒ€ì‹œë³´ë“œì— ì§„í–‰ ìƒí™© ì „ì†¡ (10%ë§ˆë‹¤)
            if completed_batches % max(1, total_batches // 10) == 0 or completed_batches == total_batches:
                BUS.log(
                    section="dataset_progress",
                    stage="quality_analysis",
                    completed=processed_count,
                    total=len(texts_to_analyze),
                    progress=progress,
                    seq_len=target_seq_len,
                    domain=config.domain
                )

        # coref_scoreë§Œ ì¶”ì¶œ
        quality_scores = [result['coref_score'] for result in quality_scores]

        print(f"    âœ… Quality analysis completed: {len(quality_scores)} scores calculated")

        selected_indices = quality_weighted_sample_indices(quality_scores, min(1000, len(processed_texts)))
        processed_texts = [processed_texts[i] for i in selected_indices]
    elif len(processed_texts) > 1000 and skip_quality_analysis:
        print(f"  â­ï¸  Skipping quality analysis, using first {min(1000, len(processed_texts))} samples...")
        processed_texts = processed_texts[:min(1000, len(processed_texts))]

    # Dataset ê°ì²´ë¡œ ë³€í™˜
    from datasets import Dataset
    return Dataset.from_list([{"text": text} for text in processed_texts])

def is_coref_rich_wiki(text: str) -> bool:
    """Wikipedia í…ìŠ¤íŠ¸ê°€ coreferenceì— ì í•©í•œì§€ íŒë‹¨"""
    coref_indicators = [
        'ì‚¬ëŒ', 'ì¸ë¬¼', 'ë°°ìš°', 'ì •ì¹˜ì¸', 'ì‘ê°€', 'í™”ê°€', 'ìŒì•…ê°€',
        'íšŒì‚¬', 'ê¸°ì—…', 'ì¡°ì§', 'ë‹¨ì²´', 'êµ­ê°€', 'ë„ì‹œ',
        'ì´ì•¼ê¸°', 'ì—­ì‚¬', 'ì‚¬ê±´', 'ë°œìƒ', 'ì°¸ì—¬', 'ê´€ë ¨'
    ]

    text_lower = text.lower()
    indicator_count = sum(1 for indicator in coref_indicators if indicator in text_lower)

    # ìµœì†Œ 2ê°œ ì´ìƒì˜ coref ì§€í‘œê°€ ìˆì–´ì•¼ í•¨
    return indicator_count >= 2

def chunk_with_coref_preservation(text: str, chunk_size: int, tokenizer) -> List[str]:
    """Coreference ë§¥ë½ì„ ìœ ì§€í•˜ë©° ì²­í‚¹"""
    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ë‹¨ìˆœí™”)
    sentences = []
    current = ""
    for char in text:
        current += char
        if char in ['.', '!', '?', 'ë‹¤.'] and len(current) > 10:
            sentences.append(current.strip())
            current = ""

    if current.strip():
        sentences.append(current.strip())

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for sentence in sentences:
        sentence_tokens = len(tokenizer.encode(sentence))

        if current_tokens + sentence_tokens > chunk_size and current_chunk:
            if current_tokens > chunk_size * 0.6:  # ìµœì†Œ ê¸¸ì´
                chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_tokens = sentence_tokens
        else:
            current_chunk += " " + sentence
            current_tokens += sentence_tokens

    if current_chunk and current_tokens > chunk_size * 0.6:
        chunks.append(current_chunk.strip())

    return chunks

def chunk_long_text(text: str, chunk_size: int, tokenizer, overlap: int = 200) -> List[str]:
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹"""
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), chunk_size - overlap):
        chunk_tokens = tokens[i:i + chunk_size]
        if len(chunk_tokens) >= chunk_size * 0.7:  # ìµœì†Œ ê¸¸ì´
            chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)

    return chunks

def quality_weighted_sample_indices(scores: List[float], k: int) -> List[int]:
    """í’ˆì§ˆ ê¸°ë°˜ ìƒ˜í”Œë§"""
    total_score = sum(scores)
    probabilities = [score / total_score for score in scores]

    return np.random.choice(len(scores), size=k, p=probabilities, replace=False).tolist()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Memory-Efficient Batch Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_optimal_batch_config(seq_len: int, model_name: str, available_memory_gb: float = 80) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  ë°°ì¹˜ ì„¤ì •"""

    # ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ (GB per sample)
    memory_per_sample = {
        512: 0.1,
        768: 0.25,
        1024: 0.5,
        1280: 0.8,
        1536: 1.2,
        1792: 1.7,
        2048: 2.5
    }

    base_memory = memory_per_sample.get(seq_len, 2.0)
    max_samples_per_batch = int(available_memory_gb / base_memory)

    # ëª¨ë¸ë³„ ìµœì í™”
    if "deberta" in model_name.lower():
        # DeBERTaëŠ” ë” íš¨ìœ¨ì 
        max_samples_per_batch = int(max_samples_per_batch * 1.2)

    # ë°°ì¹˜ í¬ê¸° ë° grad_acc ê³„ì‚°
    if seq_len <= 512:
        bs_candidates = [32, 16, 8, 4]
        grad_acc_candidates = [1, 2]
    elif seq_len <= 1024:
        bs_candidates = [16, 8, 4, 2]
        grad_acc_candidates = [2, 4, 8]
    elif seq_len <= 1536:
        bs_candidates = [8, 4, 2, 1]
        grad_acc_candidates = [4, 8, 16]
    else:  # 2048
        bs_candidates = [4, 2, 1]
        grad_acc_candidates = [8, 16, 32]

    # ìµœì  ì¡°í•© ì°¾ê¸°
    best_config = {"bs": 1, "grad_acc": 32, "effective_bs": 32, "memory_usage": 0}

    for bs in bs_candidates:
        for grad_acc in grad_acc_candidates:
            effective_bs = bs * grad_acc
            memory_usage = base_memory * bs

            # ë©”ëª¨ë¦¬ ì œì•½ í™•ì¸
            if memory_usage > available_memory_gb * 0.8:  # 80% ì´ë‚´
                continue

            # ë” í° effective batch size ì„ í˜¸
            if effective_bs > best_config["effective_bs"]:
                best_config = {
                    "bs": bs,
                    "grad_acc": grad_acc,
                    "effective_bs": effective_bs,
                    "memory_usage": memory_usage
                }

    return best_config


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Advanced Hyperparameter Optimization
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_length_specific_hpo_space(seq_len: int) -> Dict[str, Tuple]:
    """ê¸¸ì´ì— ë”°ë¥¸ ìµœì  HPO ê³µê°„"""

    if seq_len <= 512:
        return {
            "lr": (1e-5, 5e-4),
            "warmup_ratio": (0.0, 0.2),
            "weight_decay": (0.0, 0.1),
            "min_prob": (0.05, 0.15),
            "max_prob": (0.20, 0.35),
        }
    elif seq_len <= 1024:
        return {
            "lr": (5e-6, 2e-4),
            "warmup_ratio": (0.05, 0.25),
            "weight_decay": (0.01, 0.08),
            "min_prob": (0.08, 0.18),
            "max_prob": (0.25, 0.40),
        }
    elif seq_len <= 1536:
        return {
            "lr": (7.5e-5, 9.0e-5),
            "warmup_ratio": (0.15, 0.17),
            "weight_decay": (0.038, 0.046),
            "min_prob": (0.11, 0.125),
            "max_prob": (0.35, 0.38),
        }
    else:  # 2048
        return {
            "lr": (2.1e-5, 2.6e-5),
            "warmup_ratio": (0.19, 0.21),
            "weight_decay": (0.044, 0.048),
            "min_prob": (0.13, 0.145),
            "max_prob": (0.46, 0.49),
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Long Sequence AutoML Objective
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def long_sequence_automl_objective(
    trial: optuna.Trial,
    model_name: str,
    seq_len: int,
    train_limit: Optional[int] = None,
    skip_quality_analysis: bool = False,
    prepared_dataset_path: Optional[str] = None,
    dataset_paths: Optional[List[str]] = None,
    epoch_choices: Optional[List[int]] = None,
) -> float:
    """ê¸´ ì‹œí€€ìŠ¤ íŠ¹í™” AutoML objective"""

    # 1. ê¸¸ì´ë³„ HPO ê³µê°„ ì„¤ì •
    hpo_space = get_length_specific_hpo_space(seq_len)

    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    lr = trial.suggest_float("lr", *hpo_space["lr"], log=True)
    warmup = trial.suggest_float("warmup_ratio", *hpo_space["warmup_ratio"])
    wd = trial.suggest_float("weight_decay", *hpo_space["weight_decay"])
    min_p = trial.suggest_float("min_prob", *hpo_space["min_prob"])
    max_p = trial.suggest_float("max_prob", *hpo_space["max_prob"])

    # 3. ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ ì„¤ì •
    batch_config = compute_optimal_batch_config(seq_len, model_name)
    per_device_bs = batch_config["bs"]

    actual_grad_acc = min(32, batch_config["grad_acc"])

    print(f"Trial {trial.number}: seq_len={seq_len}, bs={per_device_bs}, grad_acc={actual_grad_acc}, lr={lr:.2e}")

    # 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tok = AutoTokenizer.from_pretrained(model_name)
    mdl = AutoModelForMaskedLM.from_pretrained(model_name)

    # ê¸´ ì‹œí€€ìŠ¤ ì§€ì›ì„ ìœ„í•´ position embeddings í™•ì¥ (í•„ìš”ì‹œ)
    if seq_len > mdl.config.max_position_embeddings:
        print(f"Extending position embeddings from {mdl.config.max_position_embeddings} to {seq_len}")
        mdl.config.max_position_embeddings = seq_len
        # position embeddings ì¬ì´ˆê¸°í™”
        mdl.deberta.embeddings.position_embeddings = torch.nn.Embedding(seq_len, mdl.config.hidden_size).to(mdl.device)

    # 5. ê¸´ ì‹œí€€ìŠ¤ íŠ¹í™” ë°ì´í„°ì…‹
    def resolve_dataset_path(base_path: Path) -> Optional[Path]:
        if not base_path.exists():
            return None
        if base_path.is_dir():
            expected = base_path / f"{model_name.replace('/', '_')}_{seq_len}_coref_optimized.arrow"
            if expected.exists():
                return expected
            if (base_path / "dataset_info.json").exists():
                return base_path
            for child in base_path.iterdir():
                if child.is_dir() and (child / "dataset_info.json").exists() and str(seq_len) in child.name:
                    return child
            return None
        return base_path

    dataset_candidates: Dict[str, Dict[str, Any]] = {}
    dataset_paths = dataset_paths or []
    prepared_dataset_path = prepared_dataset_path or None

    has_user_choices = False

    if dataset_paths:
        for raw_path in dataset_paths:
            resolved = resolve_dataset_path(Path(raw_path))
            if resolved is None:
                continue
            label = f"prepared[{Path(raw_path).name}_{seq_len}]"
            dataset_candidates[label] = {"path": str(resolved)}
            has_user_choices = True

    if prepared_dataset_path and not has_user_choices:
        resolved_default = resolve_dataset_path(Path(prepared_dataset_path))
        if resolved_default is not None:
            dataset_candidates.setdefault(
                f"prepared_default_{seq_len}", {"path": str(resolved_default)}
            )

    # ì‚¬ìš©ì ì§€ì • í›„ë³´ê°€ ì—†ë‹¤ë©´ ê¸°ì¡´ enhanced ëª¨ë“œ ì¶”ê°€
    if not dataset_candidates:
        dataset_candidates["enhanced_limit_30000_skip"] = {
            "limit": 30000,
            "skip_quality_analysis": True,
        }
        dataset_candidates["enhanced_limit_20000_skip"] = {
            "limit": 20000,
            "skip_quality_analysis": True,
        }

    choice_key = trial.suggest_categorical(
        "dataset_mode_choice", list(dataset_candidates.keys())
    )
    selected_options = dataset_candidates[choice_key]
    dataset_meta_info = {"mode": choice_key}

    if "path" in selected_options:
        tokenized = load_prepared_dataset(selected_options["path"])
        dataset_meta_info["samples"] = len(tokenized)
        dataset_meta_info["source"] = selected_options["path"]
    else:
        limit = selected_options["limit"]
        skip_local = selected_options["skip_quality_analysis"]
        tokenized = load_enhanced_dataset(
            tok,
            seq_len,
            limit=limit,
            skip_quality_analysis=skip_local,
        )
        dataset_meta_info["limit"] = limit
        dataset_meta_info["skip_quality_analysis"] = skip_local

    # 6. ì½œë ˆì´í„° ì„¤ì •
    collator = DynCollator(tokenizer=tok, mlm=True, min_prob=min_p, max_prob=max_p, max_length=seq_len)

    # 7. í•™ìŠµ ì„¤ì •
    epoch_options = epoch_choices or [1, 2, 3]
    epochs = trial.suggest_categorical("num_epochs", epoch_options)
    steps_per_epoch = max(1, math.ceil(len(tokenized) / max(1, per_device_bs * actual_grad_acc)))
    total_steps = steps_per_epoch * epochs

    # 8. ì˜µí‹°ë§ˆì´ì € ì„¤ì • (Layer-wise LR Decay)
    no_decay = ["bias", "LayerNorm.weight"]
    named = list(mdl.named_parameters())
    total_layers = sum(1 for n, _ in named if "encoder.layer" in n) or 12

    groups = []
    for n, p in named:
        lr_here = lr
        if "encoder.layer." in n:
            try:
                k = int(n.split("encoder.layer.")[1].split(".")[0])
                # ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë” ê°•í•œ layer decay
                decay_factor = 0.9 if seq_len <= 1024 else 0.85
                lr_here = lr * (decay_factor ** (total_layers - 1 - k))
            except Exception:
                pass
        wd_here = 0.0 if any(nd in n for nd in no_decay) else wd
        groups.append({"params": [p], "weight_decay": wd_here, "lr": lr_here})

    opt = torch.optim.AdamW(groups, lr=lr, weight_decay=wd)

    # 9. í•™ìŠµ ì¸ì
    hp = {
        "seq_len": seq_len,
        "per_device_bs": per_device_bs,
        "grad_acc": actual_grad_acc,
        "effective_bs": per_device_bs * actual_grad_acc,
        "warmup_ratio": warmup,
        "lr": lr,
        "weight_decay": wd,
        "min_prob": min_p,
        "max_prob": max_p,
        "bf16": torch.cuda.is_available(),
        "train_limit": dataset_meta_info.get("limit") or train_limit,
        "dataset_mode": choice_key,
        "num_epochs": epochs,
    }

    # í•™ìŠµ ì‹œì‘ ì´ë²¤íŠ¸
    BUS.log(event="trial_begin", model=model_name, trial=trial.number, **hp)

    args = TrainingArguments(
        output_dir=f"./runs/{model_name.replace('/', '_')}_long_{seq_len}/{trial.number}",
        per_device_train_batch_size=per_device_bs,
        per_device_eval_batch_size=max(1, per_device_bs // 2),
        gradient_accumulation_steps=actual_grad_acc,
        learning_rate=lr,
        weight_decay=wd,
        warmup_ratio=warmup,
        num_train_epochs=epochs,
        logging_steps=50,
        save_strategy="no",
        report_to=[],
        seed=42,
        dataloader_drop_last=False,
        bf16=torch.cuda.is_available(),
        # ê¸´ ì‹œí€€ìŠ¤ íŠ¹í™” ì„¤ì • (DeBERTaëŠ” gradient checkpointing ë¹„í™œì„±í™”)
        max_grad_norm=0.5 if seq_len > 1024 else 1.0,
        gradient_checkpointing=False,  # DeBERTa í˜¸í™˜ì„± ë¬¸ì œë¡œ ë¹„í™œì„±í™”
    )

    # 10. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    tr = Trainer(
        model=mdl,
        args=args,
        train_dataset=tokenized,
        data_collator=collator,
        tokenizer=tok,
        optimizers=(opt, None),
    )

    # ì½œë°± ì¶”ê°€
    tr.add_callback(
        LiveMetricsCallback(
            model_name=f"{model_name}_long_{seq_len}",
            trial_id=trial.number,
            hp=hp,
            dataset_meta={"seq_len": seq_len, **dataset_meta_info},
            total_steps=total_steps,
        )
    )

    # 11. í•™ìŠµ ì‹¤í–‰
    tr.train()

    # 12. í‰ê°€
    fill = pipeline("fill-mask", model=mdl, tokenizer=tok, device=0 if torch.cuda.is_available() else -1)

    # LAMBADA í‰ê°€
    eval_lbd = build_eval_from_lambada(limit=600)
    l_t1 = eval_lambada_topk(
        fill,
        eval_lbd,
        mask_token=tok.mask_token or "[MASK]",
        k=1,
        batch_size=32,
        seq_len=seq_len,
    )

    # Coref í‰ê°€ (ê¸´ ì‹œí€€ìŠ¤ íŠ¹í™”)
    eval_coref = build_coref_eval_set(limit=800, max_seq_len=seq_len)  # ë” ë§ì€ ë°ì´í„°ë¡œ í‰ê°€
    c_f1 = eval_coref_f1(
        fill,
        eval_coref,
        mask_token=tok.mask_token or "[MASK]",
        k=5,
        batch_size=32,
        seq_len=seq_len,
    )
    c_t5 = eval_coref_recall_topk(
        fill,
        eval_coref,
        mask_token=tok.mask_token or "[MASK]",
        k=5,
        batch_size=32,
        seq_len=seq_len,
    )

    # 13. ì¢…í•© ìŠ¤ì½”ì–´ ê³„ì‚°
    score = 0.4 * c_f1 + 0.3 * c_t5 + 0.3 * l_t1

    # ê²°ê³¼ ë¡œê¹…
    BUS.log(
        section="eval_stream",
        model=model_name,
        trial=trial.number,
        seq_len=seq_len,
        lbd_top1=l_t1,
        coref_f1=c_f1,
        coref_top5=c_t5,
        score=score
    )

    trial.set_user_attr("lbd_top1", l_t1)
    trial.set_user_attr("coref_f1", c_f1)
    trial.set_user_attr("coref_top5", c_t5)
    trial.set_user_attr("seq_len", seq_len)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del mdl, tr
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return float(score)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Long Sequence AutoML Runner
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_long_sequence_automl(
    model_name: str = "kakaobank/kf-deberta-base",
    seq_lengths: List[int] = [1536, 2048],
    trials_per_length: int = 20,
    train_limit: Optional[int] = None,
    skip_quality_analysis: bool = False,
    prepared_dataset_path: Optional[str] = None,
    dataset_paths: Optional[List[str]] = None,
    epoch_choices: Optional[List[int]] = None,
) -> Dict[str, Any]:
    """ê¸´ ì‹œí€€ìŠ¤ AutoML ì‹¤í–‰"""

    results = {}
    total_trials = len(seq_lengths) * trials_per_length
    overall_elapsed: List[float] = []

    for seq_len in seq_lengths:
        print(f"\n{'='*60}")
        print(f"Starting Long Sequence AutoML: {model_name} @ {seq_len} tokens")
        print(f"{'='*60}")

        # ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ study ìƒì„±
        study_name = f"LongSeq_{model_name.replace('/', '_')}_{seq_len}"
        sampler = optuna.samplers.TPESampler(seed=42)
        pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)

        study = optuna.create_study(
            direction="maximize",
            sampler=sampler,
            pruner=pruner,
            study_name=study_name
        )

        elapsed: List[float] = []

        def objective(trial):
            t0 = time.time()
            result = long_sequence_automl_objective(
                trial,
                model_name,
                seq_len,
                train_limit,
                skip_quality_analysis,
                prepared_dataset_path,
                dataset_paths,
                epoch_choices=epoch_choices,
            )
            dt = time.time() - t0
            elapsed.append(dt)
            overall_elapsed.append(dt)
            done = len(elapsed)
            remaining = max(0, trials_per_length - done)
            avg = sum(elapsed) / done
            eta = avg * remaining
            overall_done = len(overall_elapsed)
            overall_remaining = max(0, total_trials - overall_done)
            overall_eta = (sum(overall_elapsed) / overall_done * overall_remaining) if overall_done else 0.0
            print(
                "[ETA]"
                f" seq_len={seq_len} trial={trial.number}"
                f" | this_trial={dt:.1f}s"
                f" | seq_remaining={remaining} (~{eta/60:.1f}m)"
                f" | overall {overall_done}/{total_trials} done â†’ ~{overall_eta/60:.1f}m left"
            )
            return result

        # ìµœì í™” ì‹¤í–‰
        study.optimize(objective, n_trials=trials_per_length, show_progress_bar=True)

        # ê²°ê³¼ ì €ì¥
        results[seq_len] = {
            "study": study,
            "best_trial": study.best_trial,
            "best_score": study.best_value,
            "best_params": study.best_params
        }

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*40}")
        print(f"Results for {seq_len} tokens:")
        print(f"Best Score: {study.best_value:.4f}")
        print(f"Best Params: {study.best_params}")
        print(f"Best Trial: {study.best_trial.number}")

    return results


def prepare_datasets_with_kiwi(model_name: str = "kakaobank/kf-deberta-base", seq_lengths: List[int] = [1024, 1536, 2048], save_path: str = "./prepared_datasets"):
    """Kiwi í’ˆì§ˆ ë¶„ì„ì„ ì ìš©í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ì„ ë¯¸ë¦¬ ì¤€ë¹„ (ìƒí˜¸ì°¸ì¡° ìµœì í™”)"""
    import os
    os.makedirs(save_path, exist_ok=True)

    from transformers import AutoTokenizer
    tok = AutoTokenizer.from_pretrained(model_name)

    print(f"ğŸš€ Preparing Long Sequence Datasets with Kiwi Quality Analysis")
    print(f"ğŸ¯ Target: Better Coreference Resolution for Fill-Mask Model")
    print(f"ğŸ“ Save path: {save_path}")
    print(f"ğŸ”¢ Sequence lengths: {seq_lengths}")

    for seq_len in seq_lengths:
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Preparing {seq_len} tokens dataset for Coreference Training")
        print('='*80)

        try:
            print(f"ğŸ“Š Loading enhanced dataset with Kiwi quality filtering...")
            # ë” ê¸´ ì‹œí€€ìŠ¤ì˜ ê²½ìš° ë” ì—„ê²©í•œ í’ˆì§ˆ í•„í„°ë§ ì ìš©
            dataset = load_enhanced_dataset(tok, seq_len, limit=None)  # ì „ì²´ ë°ì´í„° ì‚¬ìš©

            print(f"âœ… Loaded {len(dataset)} raw samples")

            # ìƒí˜¸ì°¸ì¡° í’ˆì§ˆì´ ë†’ì€ ìƒ˜í”Œë§Œ ì„ ë³„ (ê¸´ ì‹œí€€ìŠ¤ ìµœì í™”)
            print("ğŸ” Analyzing coreference quality for long sequences...")

            high_quality_samples = []
            quality_stats = {'pronoun_density': [], 'entity_density': [], 'coref_score': []}

            # ìƒ˜í”Œë§í•˜ì—¬ í’ˆì§ˆ ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            sample_size = min(500, len(dataset))  # ìµœëŒ€ 500ê°œ ìƒ˜í”Œ ë¶„ì„
            indices = list(range(0, len(dataset), max(1, len(dataset)//sample_size)))

            for i, idx in enumerate(indices):
                if i >= sample_size:
                    break
                sample = dataset[idx]
                input_ids = sample['input_ids']
                # ê¸´ ì‹œí€€ìŠ¤ì˜ ê²½ìš° ë” ë§ì€ í† í°ìœ¼ë¡œ í’ˆì§ˆ ë¶„ì„
                analysis_length = min(300, len(input_ids))  # ìµœëŒ€ 300í† í° ë¶„ì„
                clean_ids = [x for x in input_ids if x != tok.pad_token_id][:analysis_length]
                text = tok.decode(clean_ids, skip_special_tokens=True)

                quality = analyze_coref_quality(text)
                quality_stats['pronoun_density'].append(quality['pronoun_density'])
                quality_stats['entity_density'].append(quality['entity_density'])
                quality_stats['coref_score'].append(quality['coref_score'])

                # ìƒí˜¸ì°¸ì¡° í’ˆì§ˆì´ ë†’ì€ ìƒ˜í”Œë§Œ ì„ íƒ (ê¸´ ì‹œí€€ìŠ¤ì— ì í•©í•œ ê¸°ì¤€)
                if quality['pronoun_density'] > 0.008 and quality['coref_score'] > 0.3:
                    high_quality_samples.append(idx)

                if (i + 1) % 50 == 0:
                    print(f"  ğŸ“ˆ Analyzed {i + 1}/{sample_size} samples...")

            # í’ˆì§ˆ í†µê³„ ê³„ì‚°
            avg_pronoun_density = sum(quality_stats['pronoun_density']) / len(quality_stats['pronoun_density'])
            avg_entity_density = sum(quality_stats['entity_density']) / len(quality_stats['entity_density'])
            avg_coref_score = sum(quality_stats['coref_score']) / len(quality_stats['coref_score'])

            print("\nğŸ“Š Quality Analysis Results:")
            print(".3f")
            print(".3f")
            print(".3f")
            print(f"  ğŸ¯ High-quality samples selected: {len(high_quality_samples)}/{len(dataset)}")

            # ê³ í’ˆì§ˆ ìƒ˜í”Œë§Œìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
            if high_quality_samples:
                selected_dataset = dataset.select(high_quality_samples)

                # ì €ì¥
                save_file = f"{save_path}/{model_name.replace('/', '_')}_{seq_len}_coref_optimized.arrow"
                selected_dataset.save_to_disk(save_file)

                print(f"âœ… Saved {len(selected_dataset)} high-quality samples to {save_file}")

                # ìµœì¢… í’ˆì§ˆ ê²€ì¦
                final_quality_check = []
                for i in range(min(50, len(selected_dataset))):
                    sample = selected_dataset[i]
                    input_ids = sample['input_ids']
                    clean_ids = [x for x in input_ids if x != tok.pad_token_id][:200]
                    text = tok.decode(clean_ids, skip_special_tokens=True)
                    quality = analyze_coref_quality(text)
                    final_quality_check.append(quality)

                final_pronoun_density = sum(q['pronoun_density'] for q in final_quality_check) / len(final_quality_check)
                final_coref_score = sum(q['coref_score'] for q in final_quality_check) / len(final_quality_check)

                print("ğŸ‰ Final Dataset Quality:")
                print(".3f")
                print(".3f")
                print("  âœ¨ Optimized for Coreference Resolution!")
            else:
                print("âš ï¸ No high-quality samples found. Saving original dataset...")
                save_file = f"{save_path}/{model_name.replace('/', '_')}_{seq_len}_fallback.arrow"
                dataset.save_to_disk(save_file)
                print(f"âœ… Saved fallback dataset to {save_file}")

        except Exception as e:
            print(f"âŒ Failed to prepare {seq_len} tokens dataset: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nğŸŠ Long Sequence Dataset Preparation Completed!")
    print(f"ğŸ“‚ Files saved to: {save_path}")
    print("ğŸ¯ Ready for Coreference-Optimized Fill-Mask Training!")
    print("\nğŸš€ Next: Run training with prepared datasets")
    print("   uv run python -m coref_automl.long_sequence_automl --model kakaobank/kf-deberta-base --seq-lengths 1024 1536 2048 --trials 10 --train-limit 50000")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Long Sequence DeBERTa AutoML")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base", help="Model name")
    parser.add_argument("--seq-lengths", nargs="+", type=int, default=[1536, 2048], help="Sequence lengths to test")
    parser.add_argument("--trials", type=int, default=20, help="Trials per sequence length")
    parser.add_argument("--train-limit", type=int, default=30000, help="Training data limit")
    parser.add_argument("--skip-quality-analysis", action="store_true", help="Skip quality analysis for faster dataset loading")
    parser.add_argument("--prepare-datasets", action="store_true", help="Prepare datasets with Kiwi quality analysis")
    parser.add_argument("--save-path", default="./prepared_datasets", help="Path to save prepared datasets")
    parser.add_argument("--prepared-dataset", help="Path to prepared dataset file or directory")
    parser.add_argument(
        "--dataset-choice",
        action="append",
        dest="dataset_choices",
        help="Prepared dataset directory or file to consider (can be repeated)",
    )
    parser.add_argument(
        "--epoch-choices",
        type=int,
        nargs="+",
        help="Candidate epoch counts to sample (default: 1 2 3)",
    )

    args = parser.parse_args()

    if args.prepare_datasets:
        # ë°ì´í„°ì…‹ ì¤€ë¹„ ëª¨ë“œ
        prepare_datasets_with_kiwi(
            model_name=args.model,
            seq_lengths=args.seq_lengths,
            save_path=args.save_path
        )
    else:
        # ì¼ë°˜ AutoML ëª¨ë“œ
        results = run_long_sequence_automl(
            model_name=args.model,
            seq_lengths=args.seq_lengths,
            trials_per_length=args.trials,
            train_limit=args.train_limit,
            skip_quality_analysis=args.skip_quality_analysis,
            prepared_dataset_path=args.prepared_dataset,
            dataset_paths=args.dataset_choices,
            epoch_choices=args.epoch_choices,
        )

        print("\n" + "="*80)
        print("FINAL RESULTS SUMMARY")
        print("="*80)

        for seq_len, result in results.items():
            print(f"\n{seq_len} tokens:")
            print(".4f")
            print(f"  Best Params: {result['best_params']}")

        print("\nAutoML completed! Check the data/ directory for detailed logs.")

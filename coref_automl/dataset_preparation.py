# coref_automl/dataset_preparation.py
"""
ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„ ì‹œìŠ¤í…œ
- ê¸´ ì‹œí€€ìŠ¤(1024~2048) ì§€ì›
- Kiwi í’ˆì§ˆ ë¶„ì„ ê¸°ë°˜ í•„í„°ë§
- ìƒí˜¸ì°¸ì¡° ìµœì í™”
"""

from __future__ import annotations
import os
import gc
import math
import random
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple
from functools import lru_cache

import numpy as np
import torch
from datasets import load_dataset, concatenate_datasets, disable_caching
from transformers import AutoTokenizer
from kiwipiepy import Kiwi

from .coref_utils import is_noun
from .bus import BUS

disable_caching()

# Kiwi ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
KIWI = Kiwi()

@dataclass
class DatasetSource:
    """ë°ì´í„°ì…‹ ì†ŒìŠ¤ ì„¤ì •"""
    name: str
    source: str
    subset: Optional[str]
    split: str
    domain: str
    quality_weight: float
    description: str

def get_all_dataset_sources() -> List[DatasetSource]:
    """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ì†ŒìŠ¤"""
    return [
        # Wikipedia - ê¸´ ë¬¸ì„œ, ë‹¤ì–‘í•œ ì£¼ì œ
        DatasetSource(
            name="wiki_ko",
            source="wikimedia/wikipedia",
            subset="20231101.ko",
            split="train",
            domain="encyclopedia",
            quality_weight=0.8,
            description="í•œêµ­ì–´ ìœ„í‚¤ë°±ê³¼ - ë‹¤ì–‘í•œ ì£¼ì œì˜ ê¸´ ë¬¸ì„œ"
        ),

        # KLUE MRC - ì§ˆë¬¸-ë‹µë³€, ê¸´ ë§¥ë½
        DatasetSource(
            name="klue_mrc",
            source="klue",
            subset="mrc",
            split="train",
            domain="qa_long",
            quality_weight=1.0,
            description="KLUE MRC - ê¸´ ë§¥ë½ì˜ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°"
        ),

        # KLUE YNAT - ë‰´ìŠ¤ ê¸°ì‚¬
        DatasetSource(
            name="klue_ynat",
            source="klue",
            subset="ynat",
            split="train",
            domain="news_topic",
            quality_weight=0.9,
            description="KLUE YNAT - ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ê³¼ ë‚´ìš©"
        ),

        # KorQuAD - ì§ˆë¬¸-ë‹µë³€
        DatasetSource(
            name="korquad",
            source="squad_kor_v1",
            subset=None,
            split="train",
            domain="qa_general",
            quality_weight=0.7,
            description="KorQuAD - í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹"
        ),

        # KLUE STS - ë¬¸ì¥ ìœ ì‚¬ë„ (ê¸´ í…ìŠ¤íŠ¸ ì¡°í•©ìš©)
        DatasetSource(
            name="klue_sts",
            source="klue",
            subset="sts",
            split="train",
            domain="similarity",
            quality_weight=0.6,
            description="KLUE STS - ë¬¸ì¥ ìœ ì‚¬ë„ (ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ìš©)"
        ),

        # KLUE NLI - ì¶”ë¡  (ê¸´ í…ìŠ¤íŠ¸ ì¡°í•©ìš©)
        DatasetSource(
            name="klue_nli",
            source="klue",
            subset="nli",
            split="train",
            domain="inference",
            quality_weight=0.5,
            description="KLUE NLI - ìì—°ì–´ ì¶”ë¡  (ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ìš©)"
        ),
    ]

@lru_cache(maxsize=10000)
def analyze_coref_quality_cached(text: str) -> Dict[str, float]:
    """Kiwië¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ Coreference í’ˆì§ˆ ë¶„ì„ (ìºì‹± ì ìš©)"""

    # í˜•íƒœì†Œ ë¶„ì„
    tokens = KIWI.tokenize(text)

    # ëŒ€ëª…ì‚¬ì™€ ê°œì²´ ë¶„ì„
    pronouns = []
    entities = []
    verbs = []
    meaningful_words = []

    for token in tokens:
        # Kiwi íƒœê·¸ ì„¤ëª…:
        # NP: ëŒ€ëª…ì‚¬ (ê·¸, ê·¸ë…€, ì´ê²ƒ ë“±)
        # NNG/NNP/NNB: ì¼ë°˜ëª…ì‚¬/ê³ ìœ ëª…ì‚¬/ì˜ì¡´ëª…ì‚¬
        # VV/VX/VA: ë™ì‚¬/ë³´ì¡°ë™ì‚¬/í˜•ìš©ì‚¬
        if token.tag == 'NP':  # ëŒ€ëª…ì‚¬
            pronouns.append(token.form)
        elif token.tag in ['NNG', 'NNP', 'NNB']:  # ëª…ì‚¬ë¥˜
            if len(token.form) > 1:  # í•œ ê¸€ì ì œì™¸
                entities.append(token.form)
        elif token.tag.startswith(('V', 'VA')):  # ë™ì‚¬/í˜•ìš©ì‚¬
            verbs.append(token.form)

        # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)
        if token.tag.startswith(('N', 'V', 'M', 'VA', 'VV')):
            meaningful_words.append(token.form)

    # í†µê³„ ê³„ì‚°
    pronoun_count = len(pronouns)
    entity_count = len(entities)
    verb_count = len(verbs)
    total_words = len(meaningful_words)

    # ë°€ë„ ê³„ì‚°
    pronoun_density = pronoun_count / max(1, total_words)
    entity_density = entity_count / max(1, total_words)
    verb_density = verb_count / max(1, total_words)

    # ê³ ê¸‰ Coreference ì ìˆ˜ ê³„ì‚°
    # 1. ëŒ€ëª…ì‚¬-ê°œì²´ ìƒí˜¸ì‘ìš© ì ìˆ˜
    coref_interaction = min(1.0, (pronoun_density * 20) + (entity_density * 3) + (pronoun_density * entity_density * 50))

    # 2. í…ìŠ¤íŠ¸ ë³µì¡ë„ ì ìˆ˜ (ë‹¤ì–‘í•œ í’ˆì‚¬ ì‚¬ìš©)
    complexity_score = min(1.0, (entity_density * 10) + (verb_density * 5) + (pronoun_density * 15))

    # 3. Coreference ì í•©ì„± ì ìˆ˜ (ëŒ€ëª…ì‚¬ì™€ ê°œì²´ì˜ ê· í˜•)
    balance_score = 1.0 - abs(pronoun_density - entity_density * 0.3)  # ì´ìƒì ì¸ ë¹„ìœ¨

    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
    quality_score = (coref_interaction * 0.5) + (complexity_score * 0.3) + (balance_score * 0.2)

    return {
        'pronoun_density': pronoun_density,
        'entity_density': entity_density,
        'verb_density': verb_density,
        'coref_score': coref_interaction,
        'complexity_score': complexity_score,
        'balance_score': balance_score,
        'quality_score': quality_score,
        'pronoun_count': pronoun_count,
        'entity_count': entity_count,
        'verb_count': verb_count,
        'total_words': total_words,
        'unique_pronouns': len(set(pronouns)),
        'unique_entities': len(set(entities)),
    }

def analyze_coref_quality(text: str) -> Dict[str, float]:
    """ìºì‹±ëœ í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜"""
    return analyze_coref_quality_cached(text)

def batch_analyze_quality(texts: List[str], batch_size: int = 100, max_workers: Optional[int] = None) -> List[Dict[str, float]]:
    """ë°°ì¹˜ ë‹¨ìœ„ ê³ ì„±ëŠ¥ í’ˆì§ˆ ë¶„ì„"""
    if max_workers is None:
        max_workers = min(16, multiprocessing.cpu_count())  # ìµœëŒ€ 16ê°œ ì›Œì»¤

    results = []

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            batch_results = list(executor.map(analyze_coref_quality_cached, batch_texts))
            results.extend(batch_results)

    return results

def load_and_preprocess_dataset(source: DatasetSource, tokenizer, target_seq_len: int, limit: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""

    try:
        print(f"ğŸ“¥ Loading {source.name}: {source.description}")

        # ë°ì´í„° ë¡œë“œ
        load_kwargs = {"split": source.split}
        if source.subset:
            load_kwargs["name"] = source.subset

        dataset = load_dataset(source.source, **load_kwargs)

        # ìƒ˜í”Œ ì œí•œ
        if limit and limit < len(dataset):
            dataset = dataset.select(range(limit))

        print(f"  âœ… Loaded {len(dataset)} raw samples")

        # ë„ë©”ì¸ë³„ ì „ì²˜ë¦¬
        processed_texts = preprocess_domain_texts(dataset, source, tokenizer, target_seq_len)

        if not processed_texts:
            print(f"  âš ï¸ No valid texts after preprocessing")
            return None

        print(f"  âœ… Preprocessed {len(processed_texts)} texts")

        # Dataset ê°ì²´ë¡œ ë³€í™˜
        from datasets import Dataset
        return Dataset.from_list([{"text": text} for text in processed_texts])

    except Exception as e:
        print(f"  âŒ Failed to load {source.name}: {e}")
        return None

def preprocess_domain_texts(dataset, source: DatasetSource, tokenizer, target_seq_len: int) -> List[str]:
    """ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""

    processed_texts = []

    for item in dataset:
        try:
            # ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if source.domain == "encyclopedia":
                text = item.get("text", "").strip()
            elif source.domain == "qa_long":
                context = item.get("context", "")
                question = item.get("question", "")
                answer = item.get("answers", {}).get("text", [""])[0] if item.get("answers") else ""
                text = f"{context} {question} {answer}".strip()
            elif source.domain == "news_topic":
                title = item.get("title", "")
                content = item.get("content", item.get("description", ""))
                text = f"{title} {content}".strip()
            elif source.domain == "qa_general":
                context = item.get("context", "")
                question = item.get("question", "")
                answer = item.get("answers", {}).get("text", [""])[0] if item.get("answers") else ""
                text = f"{context} {question} {answer}".strip()
            elif source.domain == "similarity":
                sentence1 = item.get("sentence1", item.get("text", ""))
                sentence2 = item.get("sentence2", "")
                text = f"{sentence1} {sentence2}".strip()
            elif source.domain == "inference":
                premise = item.get("premise", "")
                hypothesis = item.get("hypothesis", "")
                if premise and hypothesis:
                    text = f"{premise} {hypothesis}".strip()
                else:
                    continue
            else:
                text = item.get("text", "").strip()

            # ê¸°ë³¸ í•„í„°ë§
            if not text or len(text) < 50:
                continue

            # í† í° ê¸¸ì´ í•„í„°ë§ (ê¸´ ì‹œí€€ìŠ¤ ëŒ€ë¹„)
            tokens = tokenizer.encode(text)
            if len(tokens) < target_seq_len * 0.3:  # ìµœì†Œ ê¸¸ì´
                continue
            if len(tokens) > target_seq_len * 2.0:  # ìµœëŒ€ ê¸¸ì´ (ì²­í‚¹ ê³ ë ¤)
                # ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹
                chunks = chunk_text_with_coref_preservation(text, target_seq_len, tokenizer)
                processed_texts.extend(chunks)
            else:
                processed_texts.append(text)

        except Exception as e:
            continue

    return processed_texts

def chunk_text_with_coref_preservation(text: str, chunk_size: int, tokenizer, overlap: int = 200) -> List[str]:
    """Coreference ë§¥ë½ì„ ìœ ì§€í•˜ë©° ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹"""

    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    sentences = []
    current = ""
    for char in text:
        current += char
        if char in ['.', '!', '?', 'ë‹¤.'] and len(current) > 10:
            sentences.append(current.strip())
            current = ""

    if current.strip():
        sentences.append(current.strip())

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for sentence in sentences:
        sentence_tokens = len(tokenizer.encode(sentence))

        if current_tokens + sentence_tokens > chunk_size and current_chunk:
            if current_tokens > chunk_size * 0.6:  # ìµœì†Œ ê¸¸ì´
                chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_tokens = sentence_tokens
        else:
            current_chunk += " " + sentence
            current_tokens += sentence_tokens

    if current_chunk and current_tokens > chunk_size * 0.6:
        chunks.append(current_chunk.strip())

    return chunks

def prepare_coref_optimized_datasets(
    model_name: str = "kakaobank/kf-deberta-base",
    seq_lengths: List[int] = [1024, 1536, 2048],
    save_path: str = "./prepared_datasets",
    quality_threshold: float = 0.6,
    max_samples_per_length: int = 50000
):
    """
    ìƒí˜¸ì°¸ì¡° ìµœì í™”ëœ ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„
    - Kiwi í’ˆì§ˆ ë¶„ì„ ê¸°ë°˜ í•„í„°ë§
    - ê¸´ ì‹œí€€ìŠ¤ ì§€ì›
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
    """

    import os
    os.makedirs(save_path, exist_ok=True)

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print("ğŸš€ ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„ ì‹œìŠ¤í…œ")
    print(f"ğŸ¯ ëª¨ë¸: {model_name}")
    print(f"ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_lengths}")
    print(f"â­ í’ˆì§ˆ ì„ê³„ê°’: {quality_threshold}")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")
    print("=" * 80)

    sources = get_all_dataset_sources()

    for seq_len in seq_lengths:
        print(f"\nğŸ¯ {seq_len} í† í° ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

        all_datasets = []
        total_samples = 0

        # ê° ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        for source in sources:
            try:
                print(f"\nğŸ“¥ ì²˜ë¦¬ ì¤‘: {source.name}")

                # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
                dataset = load_and_preprocess_dataset(source, tokenizer, seq_len, limit=max_samples_per_length // len(sources))

                if dataset is None or len(dataset) == 0:
                    continue

                print(f"  ğŸ” í’ˆì§ˆ ë¶„ì„ ì¤‘... ({len(dataset)} ìƒ˜í”Œ)")

                # í’ˆì§ˆ ë¶„ì„ ë° í•„í„°ë§
                high_quality_samples = []
                quality_stats = {'analyzed': 0, 'passed': 0, 'avg_quality': 0}

                # ìƒ˜í”Œë§í•˜ì—¬ ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                sample_size = min(1000, len(dataset))
                indices = list(range(0, len(dataset), max(1, len(dataset) // sample_size)))

                for idx in indices:
                    sample = dataset[idx]
                    text = sample['text']

                    quality = analyze_coref_quality(text)
                    quality_stats['analyzed'] += 1
                    quality_stats['avg_quality'] += quality['quality_score']

                    # í’ˆì§ˆ í•„í„°ë§
                    if quality['quality_score'] >= quality_threshold:
                        high_quality_samples.append(idx)
                        quality_stats['passed'] += 1

                quality_stats['avg_quality'] /= max(1, quality_stats['analyzed'])

                print(f"  âœ… í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ: {quality_stats['passed']}/{quality_stats['analyzed']} í†µê³¼")
                print(f"    ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {quality_stats['avg_quality']:.3f}")
                # ê³ í’ˆì§ˆ ìƒ˜í”Œë§Œ ì„ íƒ
                if high_quality_samples:
                    filtered_dataset = dataset.select(high_quality_samples)
                    all_datasets.append(filtered_dataset)
                    total_samples += len(filtered_dataset)

                    print(f"  ğŸ¯ ì„ íƒëœ ìƒ˜í”Œ: {len(filtered_dataset)}")

            except Exception as e:
                print(f"  âŒ {source.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ë°ì´í„° í†µí•©
        if all_datasets:
            print(f"\nğŸ”„ {seq_len} í† í° ë°ì´í„°ì…‹ í†µí•© ì¤‘... ({total_samples} ìƒ˜í”Œ)")

            combined_dataset = concatenate_datasets(all_datasets)
            combined_dataset = combined_dataset.shuffle(seed=42)

            # ìµœì¢… ìƒ˜í”Œ ì œí•œ
            if len(combined_dataset) > max_samples_per_length:
                combined_dataset = combined_dataset.select(range(max_samples_per_length))

            # í† í°í™”
            print(f"ğŸ”¤ í† í°í™” ì¤‘... ({len(combined_dataset)} ìƒ˜í”Œ)")

            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=seq_len,
                    return_tensors="pt"
                )

            tokenized_dataset = combined_dataset.map(
                tokenize_function,
                batched=True,
                batch_size=1000,
                remove_columns=["text"],
                num_proc=min(16, multiprocessing.cpu_count())
            )

            # ì €ì¥
            save_file = f"{save_path}/{model_name.replace('/', '_')}_{seq_len}_coref_optimized.arrow"
            tokenized_dataset.save_to_disk(save_file)

            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_file}")
            print(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹: {len(tokenized_dataset)} ìƒ˜í”Œ Ã— {seq_len} í† í°")

            # ìµœì¢… í’ˆì§ˆ ê²€ì¦
            print("ğŸ¯ ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì¤‘...")

            final_qualities = []
            for i in range(min(100, len(tokenized_dataset))):
                sample = tokenized_dataset[i]
                input_ids = sample['input_ids']
                clean_ids = [x for x in input_ids if x != tokenizer.pad_token_id][:300]
                text = tokenizer.decode(clean_ids, skip_special_tokens=True)
                quality = analyze_coref_quality(text)
                final_qualities.append(quality)

            avg_final_quality = sum(q['quality_score'] for q in final_qualities) / len(final_qualities)
            avg_pronoun_density = sum(q['pronoun_density'] for q in final_qualities) / len(final_qualities)
            avg_entity_density = sum(q['entity_density'] for q in final_qualities) / len(final_qualities)

            print("ğŸ“Š ìµœì¢… í’ˆì§ˆ ì§€í‘œ:")
            print(".3f")
            print(".3f")
            print(".3f")

            rating = 'â­ ìš°ìˆ˜' if avg_final_quality > 0.8 else 'âœ… ì–‘í˜¸' if avg_final_quality > 0.6 else 'âš ï¸ ë³´í†µ'
            print(f"ğŸ† í’ˆì§ˆ ë“±ê¸‰: {rating}")

        else:
            print(f"âš ï¸ {seq_len} í† í° ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")

    print("\nğŸŠ ëª¨ë“  ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {save_path}")
    print("ğŸš€ ì´ì œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    import argparse
    import multiprocessing

    parser = argparse.ArgumentParser(description="ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--seq-lengths", nargs="+", type=int, default=[1024, 1536, 2048], help="ì‹œí€€ìŠ¤ ê¸¸ì´ë“¤")
    parser.add_argument("--save-path", default="./prepared_datasets", help="ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--quality-threshold", type=float, default=0.6, help="í’ˆì§ˆ ì„ê³„ê°’")
    parser.add_argument("--max-samples", type=int, default=50000, help="ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")

    args = parser.parse_args()

    prepare_coref_optimized_datasets(
        model_name=args.model,
        seq_lengths=args.seq_lengths,
        save_path=args.save_path,
        quality_threshold=args.quality_threshold,
        max_samples_per_length=args.max_samples
    )
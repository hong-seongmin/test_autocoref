# coref_automl/tune.py
from __future__ import annotations
import os
import gc
import json
import math
import random
import time
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import optuna
import torch
from datasets import load_dataset, disable_caching
from transformers import (
    AutoTokenizer,
    AutoModelForMaskedLM,
    AutoConfig,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    pipeline,
)

from .coref_utils import is_noun
from .callback import LiveMetricsCallback
from .bus import BUS

disable_caching()  # ìºì‹± ë¹„í™œì„±í™”(ìš”êµ¬ì‚¬í•­)

CANDIDATE_MODELS = [
    "kakaobank/kf-deberta-base",
    "kykim/bert-kor-base",
    "google-bert/bert-base-multilingual-cased",
]

MASK_TOKEN_FALLBACK = "[MASK]"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Checkpoint Sequence Length Detection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_seq_len_from_checkpoint(model_path: str) -> Optional[int]:
    """
    Detect the sequence length (max_position_embeddings) from a checkpoint.

    Args:
        model_path: Path to checkpoint directory or HuggingFace model ID

    Returns:
        Detected sequence length, or None if detection fails
    """
    # Try local checkpoint first
    config_path = Path(model_path) / "config.json"
    if config_path.exists():
        try:
            with open(config_path, encoding="utf-8") as f:
                config = json.load(f)
            seq_len = config.get("max_position_embeddings")
            if seq_len:
                print(f"âœ“ Detected seq_len={seq_len} from local checkpoint: {model_path}")
                return int(seq_len)
        except Exception as e:
            print(f"Warning: Failed to read config from {config_path}: {e}")

    # Try HuggingFace model config
    try:
        config = AutoConfig.from_pretrained(model_path)
        seq_len = getattr(config, "max_position_embeddings", None)
        if seq_len:
            print(f"âœ“ Detected seq_len={seq_len} from HuggingFace model: {model_path}")
            return int(seq_len)
    except Exception as e:
        print(f"Info: Could not load HuggingFace config for {model_path}: {e}")

    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ko-LAMBADA ë¡œë” (gated â†’ token ìë™ ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_ko_lambada_split(prefer=("validation", "dev", "test")):
    hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN") or os.getenv("HF_TOKEN")
    last_err = None
    for sp in prefer:
        try:
            kwargs = dict(split=sp)
            if hf_token:
                kwargs["token"] = hf_token
            return load_dataset("thunder-research-group/SNU_Ko-LAMBADA", **kwargs)
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"Ko-LAMBADA split ë¡œë“œ ì‹¤íŒ¨: {last_err}")


def build_eval_from_lambada(limit=1000, seed=42) -> List[Dict[str, str]]:
    ds = load_ko_lambada_split(prefer=("validation", "dev", "test"))
    rnd = random.Random(seed)
    idxs = list(range(len(ds)))
    rnd.shuffle(idxs)
    items = []
    for i in idxs:
        ex = ds[i]
        text = ex.get("text") or ex.get("context") or ""
        target = ex.get("target") or ex.get("answer") or ""
        if not text or not target:
            continue
        if text.strip().endswith(target):
            masked = text[: len(text) - len(target)] + MASK_TOKEN_FALLBACK
        else:
            if target in text:
                masked = text.replace(target, MASK_TOKEN_FALLBACK, 1)
            else:
                masked = text + " " + MASK_TOKEN_FALLBACK
        items.append({"masked": masked, "target": target})
        if len(items) >= limit:
            break
    return items


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Real Entity Coref í‰ê°€ ì…‹
#  - ëŒ€ëª…ì‚¬ ì—†ì´ ê°™ì€ ëª…ì‚¬ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©
#  - 2ë²ˆì§¸ ëª…ì‚¬ë¥¼ [MASK]ë¡œ ë³€í™˜í•˜ê³ , ì •ë‹µì€ í•´ë‹¹ ëª…ì‚¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_real_coref_eval_set(
    limit=2000,
    seed=123,
    max_seq_len: int = 512,
) -> List[Dict[str, Any]]:
    """
    ëŒ€ëª…ì‚¬ ì—†ê³  ê°™ì€ ëª…ì‚¬ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ì‹¤ì œ ìƒí˜¸ì°¸ì¡° í‰ê°€ ë°ì´í„° ìƒì„±

    Returns:
        List of dicts with keys:
        - masked: 2ë²ˆì§¸ ëª…ì‚¬ë¥¼ [MASK]ë¡œ ë°”ê¾¼ í…ìŠ¤íŠ¸
        - target: ì •ë‹µ ëª…ì‚¬
        - full_text: ì›ë³¸ ì „ì²´ í…ìŠ¤íŠ¸
    """
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    rnd = random.Random(seed)

    # ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤
    sources = [
        ("wikimedia/wikipedia", "20231101.ko", "train"),
        ("klue", "ynat", "validation"),
    ]

    PRONOUN_POS = {"NP"}
    NOUN_POS = {"NNG", "NNP"}

    items: List[Dict[str, Any]] = []
    scale = max(1, max_seq_len // 512)
    effective_limit = max(limit, int(limit * scale))
    target_per_source = max(1, math.ceil(effective_limit / len(sources)))

    for source, subset, split in sources:
        try:
            ds = load_dataset(source, subset, split=split, streaming=True)
            ds = ds.shuffle(seed=seed, buffer_size=10000)

            processed_count = 0
            for example in ds:
                if processed_count >= target_per_source:
                    break

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if source == "wikimedia/wikipedia":
                    text = example["text"]
                elif source == "klue":
                    text = f"{example['title']} {example.get('content', '')}".strip()
                else:
                    continue

                if not text or len(text) < 100:
                    continue

                # í˜•íƒœì†Œ ë¶„ì„
                tokens = kiwi.tokenize(text)

                # ëŒ€ëª…ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                has_pronoun = any(tk.tag in PRONOUN_POS for tk in tokens)
                if has_pronoun:
                    continue  # ëŒ€ëª…ì‚¬ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ

                # ëª…ì‚¬ ë“±ì¥ ìœ„ì¹˜ ì¶”ì 
                noun_positions = defaultdict(list)  # {ëª…ì‚¬: [(start, end, index), ...]}
                for idx, tk in enumerate(tokens):
                    if tk.tag in NOUN_POS and len(tk.form) >= 2:  # 2ê¸€ì ì´ìƒ ëª…ì‚¬ë§Œ
                        noun_positions[tk.form].append((tk.start, tk.end, idx))

                # 2ë²ˆ ì´ìƒ ë“±ì¥í•œ ëª…ì‚¬ ì°¾ê¸°
                repeated_nouns = {noun: positions for noun, positions in noun_positions.items()
                                 if len(positions) >= 2}

                if not repeated_nouns:
                    continue

                # ê° ë°˜ë³µ ëª…ì‚¬ì— ëŒ€í•´ ìƒ˜í”Œ ìƒì„± (í…ìŠ¤íŠ¸ë‹¹ ìµœëŒ€ 3ê°œ)
                samples_from_text = 0
                for noun, positions in repeated_nouns.items():
                    if samples_from_text >= 3:
                        break

                    # 2ë²ˆì§¸ ë“±ì¥ ìœ„ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹
                    second_pos = positions[1]
                    start, end, idx = second_pos

                    # [MASK] ìƒì„±
                    masked_text = text[:start] + MASK_TOKEN_FALLBACK + text[end:]

                    items.append({
                        "masked": masked_text,
                        "target": noun,
                        "full_text": text,
                    })

                    samples_from_text += 1
                    processed_count += 1

                    if processed_count >= target_per_source:
                        break

        except Exception as e:
            print(f"Warning: Failed to load {source}: {e}")
            continue

    # ì…”í”Œ ë° ì œí•œ
    rnd.shuffle(items)
    return items[:effective_limit]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MLM íŒŒì¸íŠœë‹ ë°ì´í„°(ì‹¤ë°ì´í„°: ìœ„í‚¤)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class DynCollator(DataCollatorForLanguageModeling):
    min_prob: float = 0.10
    max_prob: float = 0.25
    max_length: int = 512

    def __call__(self, examples):
        lengths = [len(ex["input_ids"]) for ex in examples]
        avg = float(np.mean(lengths)) if lengths else 0.0
        self.mlm_probability = float(self.min_prob + (self.max_prob - self.min_prob) * min(1.0, avg / self.max_length))
        return super().__call__(examples)


def build_mlm_dataset(tokenizer, max_length=512, split="train", limit=None):
    from datasets import concatenate_datasets

    datasets = []

    # 1. Wikipedia (ê¸°ë³¸)
    try:
        wiki_ds = load_dataset("wikimedia/wikipedia", "20231101.ko", split=split)
        datasets.append(wiki_ds)
    except Exception as e:
        print(f"Warning: Failed to load Wikipedia: {e}")

    # 2. KLUE ë‰´ìŠ¤ ë°ì´í„°
    try:
        klue_ds = load_dataset("klue", "ynat", split=split)
        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
        def convert_klue(example):
            return {"text": f"{example['title']} {example.get('content', '')}".strip()}
        klue_ds = klue_ds.map(convert_klue, remove_columns=klue_ds.column_names)
        datasets.append(klue_ds)
    except Exception as e:
        print(f"Warning: Failed to load KLUE: {e}")

    # 3. KorQuAD ë°ì´í„° (ì§ˆë¬¸-ë‹µë³€)
    try:
        korquad_ds = load_dataset("squad_kor_v1", split=split)
        def convert_korquad(example):
            context = example.get("context", "")
            question = example.get("question", "")
            answer = example.get("answers", {}).get("text", [""])[0] if example.get("answers") else ""
            return {"text": f"{context} {question} {answer}".strip()}
        korquad_ds = korquad_ds.map(convert_korquad, remove_columns=korquad_ds.column_names)
        datasets.append(korquad_ds)
    except Exception as e:
        print(f"Warning: Failed to load KorQuAD: {e}")

    # ë°ì´í„° í†µí•©
    if not datasets:
        raise RuntimeError("No datasets could be loaded")

    combined_ds = concatenate_datasets(datasets)

    # ì…”í”Œ ë° ì œí•œ
    combined_ds = combined_ds.shuffle(seed=42)
    if limit is not None and len(combined_ds) > limit:
        combined_ds = combined_ds.select(range(limit))

    def tok(batch):
        return tokenizer(batch["text"], truncation=True, max_length=max_length)

    tokenized = combined_ds.map(tok, batched=True, remove_columns=["text"])
    return tokenized


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸¸ì´ ì´ˆê³¼ ë°©ì§€(íŒŒì´í”„ë¼ì¸ì— truncation kwargs ê¸ˆì§€ â†’ ë¬¸ì ìœˆë„ìš°ë¡œ ì˜ë¼ì„œ ì „ë‹¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clip_around_mask(
    text: str,
    mask_token: str,
    left_chars: int = 200,
    right_chars: int = 200,
    seq_len: Optional[int] = None,
) -> str:
    try:
        i = text.index(mask_token)
        scale = 1.0
        if seq_len:
            scale = max(1.0, seq_len / 512)
        window_left = int(left_chars * scale)
        window_right = int(right_chars * scale)
        s = max(0, i - window_left)
        e = min(len(text), i + len(mask_token) + window_right)
        return text[s:e]
    except ValueError:
        # [MASK]ê°€ ì—†ë‹¤ë©´ ê·¸ëŒ€ë¡œ
        return text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì´í”„ë¼ì¸ í‰ê°€: LAMBADA ì •í™•/ Coref ë¦¬ì½œ (ë°°ì¹˜/ëª…ì‚¬í•„í„°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def batched_fill_and_filter_nouns(
    fill,
    masked_texts: List[str],
    k: int,
    mask_token: str,
    batch_size: int = 64,
    seq_len: Optional[int] = None,
    show_progress: bool = True,
) -> List[List[str]]:
    """list ì…ë ¥ â†’ list[list[pred_token]] ë°˜í™˜ (ëª…ì‚¬ í•„í„°ë§, ê¸¸ì´ ì•ˆì „)"""
    clipped = [
        clip_around_mask(t, mask_token, seq_len=seq_len)
        for t in masked_texts
    ]

    # ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€
    if show_progress:
        try:
            from tqdm import tqdm
            num_batches = (len(clipped) + batch_size - 1) // batch_size
            print(f"      Processing {len(clipped)} samples in {num_batches} batches (batch_size={batch_size})...")

            preds_all: List[List[str]] = []
            with tqdm(total=len(clipped), desc="      Inference", unit="sample", ncols=100, leave=False) as pbar:
                for i in range(0, len(clipped), batch_size):
                    batch = clipped[i:i+batch_size]
                    outs = fill(batch, top_k=max(50, k), batch_size=len(batch))

                    # ê²°ê³¼ ì²˜ë¦¬
                    if not isinstance(outs[0], list):
                        outs = [outs]

                    for item in outs:
                        cand: List[str] = []
                        for p in item:
                            token_str = p.get("token_str", "").strip().replace("##", "")
                            if token_str and is_noun(token_str):
                                cand.append(token_str)
                            if len(cand) >= k:
                                break
                        preds_all.append(cand)

                    pbar.update(len(batch))

            return preds_all
        except ImportError:
            # tqdm ì—†ìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬
            pass

    # ê¸°ë³¸ ì²˜ë¦¬ (ì§„í–‰ë¥  ì—†ìŒ)
    outs = fill(clipped, top_k=max(50, k), batch_size=batch_size)
    preds_all: List[List[str]] = []
    for item in outs:
        cand: List[str] = []
        for p in item:
            token_str = p.get("token_str", "").strip().replace("##", "")
            if token_str and is_noun(token_str):
                cand.append(token_str)
            if len(cand) >= k:
                break
        preds_all.append(cand)
    return preds_all


def eval_lambada_topk(
    fill,
    eval_items: List[Dict[str, str]],
    mask_token: str,
    k: int = 5,
    batch_size: int = 64,
    seq_len: Optional[int] = None,
) -> float:
    masked = [it["masked"] for it in eval_items]
    golds = [it["target"] for it in eval_items]
    preds = batched_fill_and_filter_nouns(
        fill,
        masked,
        k=k,
        mask_token=mask_token,
        batch_size=batch_size,
        seq_len=seq_len,
    )
    ok = 0
    for g, cands in zip(golds, preds):
        if not cands:
            continue
        # ì •í™•íˆ ì¼ì¹˜ or í¬í•¨ê´€ê³„(ì–´ì ˆ/ì„œë¸Œì›Œë“œ) í—ˆìš©
        hit = (g in cands) or any((g in c) or (c in g) for c in cands)
        if hit:
            ok += 1
    return ok / max(1, len(golds))


def eval_real_coref_top1(
    fill,
    eval_items: List[Dict[str, Any]],
    mask_token: str,
    batch_size: int = 64,
    seq_len: Optional[int] = None,
) -> float:
    """
    Real Coref Top-1 ì •í™•ë„
    2ë²ˆì§¸ ëª…ì‚¬ë¥¼ ë§ˆìŠ¤í‚¹í–ˆì„ ë•Œ top-1 ì˜ˆì¸¡ì´ ì •ë‹µ ëª…ì‚¬ì¸ì§€ í™•ì¸
    """
    masked = [it["masked"] for it in eval_items]
    targets = [it["target"] for it in eval_items]
    preds = batched_fill_and_filter_nouns(
        fill,
        masked,
        k=1,
        mask_token=mask_token,
        batch_size=batch_size,
        seq_len=seq_len,
    )

    ok = 0
    for target, cands in zip(targets, preds):
        if not cands:
            continue
        # Top-1 ì˜ˆì¸¡ì´ ì •ë‹µì¸ì§€ í™•ì¸ (ì •í™•íˆ ì¼ì¹˜ or í¬í•¨ê´€ê³„)
        hit = (target == cands[0]) or (target in cands[0]) or (cands[0] in target)
        if hit:
            ok += 1

    return ok / max(1, len(eval_items))


def eval_real_coref_top5(
    fill,
    eval_items: List[Dict[str, Any]],
    mask_token: str,
    batch_size: int = 64,
    seq_len: Optional[int] = None,
) -> float:
    """
    Real Coref Top-5 ì •í™•ë„
    2ë²ˆì§¸ ëª…ì‚¬ë¥¼ ë§ˆìŠ¤í‚¹í–ˆì„ ë•Œ top-5 ì˜ˆì¸¡ì— ì •ë‹µ ëª…ì‚¬ê°€ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
    """
    masked = [it["masked"] for it in eval_items]
    targets = [it["target"] for it in eval_items]
    preds = batched_fill_and_filter_nouns(
        fill,
        masked,
        k=5,
        mask_token=mask_token,
        batch_size=batch_size,
        seq_len=seq_len,
    )

    ok = 0
    for target, cands in zip(targets, preds):
        if not cands:
            continue
        # Top-5 ì¤‘ì— ì •ë‹µì´ ìˆëŠ”ì§€ í™•ì¸ (ì •í™•íˆ ì¼ì¹˜ or í¬í•¨ê´€ê³„)
        hit = any((target == c) or (target in c) or (c in target) for c in cands)
        if hit:
            ok += 1

    return ok / max(1, len(eval_items))


def eval_real_coref_combined(
    fill,
    eval_items: List[Dict[str, Any]],
    mask_token: str,
    batch_size: int = 64,
    seq_len: Optional[int] = None,
) -> Tuple[float, float]:
    """
    Real Coref Top-1ê³¼ Top-5ë¥¼ í•œ ë²ˆì— ê³„ì‚° (ìµœì í™”)

    í•œ ë²ˆì˜ ëª¨ë¸ ì¶”ë¡ ìœ¼ë¡œ top-5ë¥¼ ê°€ì ¸ì˜¨ í›„ Real@1ê³¼ Real@5ë¥¼ ë™ì‹œì— ê³„ì‚°í•˜ì—¬
    ì¶”ë¡  ì‹œê°„ì„ ì•½ 50% ì ˆê°í•©ë‹ˆë‹¤.

    Returns:
        Tuple[real1, real5]: (Real@1 ì •í™•ë„, Real@5 ì •í™•ë„)
    """
    masked = [it["masked"] for it in eval_items]
    targets = [it["target"] for it in eval_items]

    # í•œ ë²ˆë§Œ ì¶”ë¡  (top-5)
    preds = batched_fill_and_filter_nouns(
        fill,
        masked,
        k=5,
        mask_token=mask_token,
        batch_size=batch_size,
        seq_len=seq_len,
    )

    real1_ok = 0
    real5_ok = 0
    for target, cands in zip(targets, preds):
        if not cands:
            continue

        # Real@1: top-1ë§Œ í™•ì¸
        if (target == cands[0]) or (target in cands[0]) or (cands[0] in target):
            real1_ok += 1

        # Real@5: top-5 ì¤‘ì— ìˆëŠ”ì§€ í™•ì¸
        if any((target == c) or (target in c) or (c in target) for c in cands):
            real5_ok += 1

    total = max(1, len(eval_items))
    return real1_ok / total, real5_ok / total


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ëª¨ë¦¬ í•œë„ ë‚´ ìµœëŒ€ per-device BS íƒìƒ‰ + grad_acc ì‚°ì •(throughput ê·¹ëŒ€í™”)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _sample_texts_for_probe(num=256) -> List[str]:
    try:
        ds = load_ko_lambada_split(prefer=("validation", "dev", "test"))
        out = []
        for i in range(min(num, len(ds))):
            ex = ds[i]
            t = ex.get("text") or ex.get("context") or ""
            target = ex.get("target") or ex.get("answer") or ""
            if t and target:
                out.append((t + " ").strip() + MASK_TOKEN_FALLBACK)
        return out if out else ["í•œêµ­ì–´ ë¬¸ë§¥ì—ì„œ [MASK]ì„(ë¥¼) ì˜ˆì¸¡í•©ë‹ˆë‹¤."]
    except Exception:
        return ["í•œêµ­ì–´ ë¬¸ë§¥ì—ì„œ [MASK]ì„(ë¥¼) ì˜ˆì¸¡í•©ë‹ˆë‹¤."] * min(8, num)


def _try_one_step(model, tokenizer, texts, seq_len):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    inputs = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=seq_len,
        return_tensors="pt",
    ).to(device)
    labels = inputs["input_ids"].clone()
    probs = torch.rand_like(labels.float())
    labels[probs > 0.15] = -100
    labels[(labels == tokenizer.pad_token_id)] = -100
    out = model(**inputs, labels=labels)
    loss = out.loss
    loss.backward()
    if device == "cuda":
        torch.cuda.synchronize()


def find_max_bs(model, tokenizer, seq_len, start=2, limit=256):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    texts = _sample_texts_for_probe(num=512)
    bs = start
    last_ok = None

    def runner(n):
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()
        _try_one_step(model, tokenizer, texts[:n], seq_len)

    # ì§€ìˆ˜ ì¦ê°€
    while bs <= limit:
        try:
            runner(bs)
            last_ok = bs
            bs *= 2
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                break
            raise

    # í•˜í–¥ ì„ í˜• íƒìƒ‰
    if last_ok is None:
        bs = max(1, start // 2)
        while bs >= 1:
            try:
                runner(bs)
                last_ok = bs
                break
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    bs //= 2
                else:
                    raise
    return last_ok or 1


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Optuna Objective
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def objective(trial: optuna.Trial, model_name: str, planned_trials: int, train_limit: Optional[int] = None, seed: int = 42):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rng = np.random.RandomState(seed)

    # HPO ê³µê°„ (LR ë²”ìœ„ í™•ëŒ€)
    lr = trial.suggest_float("learning_rate", 5e-6, 2e-4, log=True)
    wd = trial.suggest_float("weight_decay", 0.0, 0.1)
    warmup = trial.suggest_float("warmup_ratio", 0.0, 0.2)
    min_prob = trial.suggest_float("min_prob", 0.05, 0.15)
    max_prob = trial.suggest_float("max_prob", 0.20, 0.35)

    # Detect seq_len from checkpoint if available, otherwise use Optuna
    detected_seq_len = detect_seq_len_from_checkpoint(model_name)
    if detected_seq_len:
        seq_len = detected_seq_len
        print(f"â†’ Using detected seq_len={seq_len} from checkpoint")
    else:
        # Expanded choices to support various sequence lengths
        seq_len = trial.suggest_categorical("max_length", [256, 384, 512, 1536, 2048])
        print(f"â†’ Using Optuna-suggested seq_len={seq_len}")

    # DeBERTaì™€ gradient checkpointing í˜¸í™˜ì„± ë¬¸ì œë¡œ ë¹„í™œì„±í™”
    # Check both model_name and if it's a checkpoint (which might be DeBERTa)
    is_deberta = "deberta" in model_name.lower() or detected_seq_len is not None
    grad_ckpt = False if is_deberta else trial.suggest_categorical("gradient_checkpointing", [False, True])
    llrd = trial.suggest_float("llrd", 0.85, 1.0)

    # ë¡œë“œ - For checkpoints with resized embeddings, handle specially
    tok = AutoTokenizer.from_pretrained(model_name)

    # Update tokenizer's model_max_length to match seq_len
    if detected_seq_len:
        tok.model_max_length = detected_seq_len
        print(f"â†’ Updated tokenizer model_max_length to {detected_seq_len}")

    if detected_seq_len:
        # Load model with ignore_mismatched_sizes to handle rel_embeddings size difference
        print(f"â†’ Loading checkpoint with ignore_mismatched_sizes=True...")
        mdl = AutoModelForMaskedLM.from_pretrained(model_name, ignore_mismatched_sizes=True)

        # Now manually load the resized rel_embeddings from the checkpoint
        from safetensors import safe_open
        checkpoint_path = Path(model_name) / "model.safetensors"
        if checkpoint_path.exists():
            with safe_open(str(checkpoint_path), framework='pt', device='cpu') as f:
                if 'deberta.encoder.rel_embeddings.weight' in f.keys():
                    rel_embed_weight = f.get_tensor('deberta.encoder.rel_embeddings.weight')
                    print(f"â†’ Manually loading rel_embeddings with shape {rel_embed_weight.shape}")

                    # Create new embedding with the checkpoint's size
                    new_size, embed_dim = rel_embed_weight.shape
                    new_rel = torch.nn.Embedding(new_size, embed_dim)
                    new_rel.weight.data = rel_embed_weight.clone()
                    mdl.deberta.encoder.rel_embeddings = new_rel.to(mdl.device)

                    # Also handle position_embeddings if present
                    if 'deberta.embeddings.position_embeddings.weight' in f.keys():
                        pos_embed_weight = f.get_tensor('deberta.embeddings.position_embeddings.weight')
                        print(f"â†’ Manually loading position_embeddings with shape {pos_embed_weight.shape}")
                        pos_size, pos_dim = pos_embed_weight.shape
                        new_pos = torch.nn.Embedding(pos_size, pos_dim)
                        new_pos.weight.data = pos_embed_weight.clone()
                        mdl.deberta.embeddings.position_embeddings = new_pos.to(mdl.device)

        mdl.config.max_position_embeddings = detected_seq_len
        print(f"âœ“ Loaded checkpoint with seq_len={detected_seq_len}")
    else:
        # Fresh model or HuggingFace model - load normally
        mdl = AutoModelForMaskedLM.from_pretrained(model_name)

        # Resize position embeddings if we need a different seq_len than the model default
        if seq_len > mdl.config.max_position_embeddings:
            print(f"â†’ Resizing position embeddings from {mdl.config.max_position_embeddings} to {seq_len}")

            # DebertaV2 uses relative position embeddings in encoder.rel_embeddings
            position_embed = getattr(mdl.deberta.embeddings, "position_embeddings", None)
            if position_embed is not None:
                old_num, dim = position_embed.weight.shape
                new_embed = torch.nn.Embedding(seq_len, dim)
                new_embed.weight.data[:old_num] = position_embed.weight.data.clone()
                if seq_len > old_num:
                    new_embed.weight.data[old_num:] = position_embed.weight.data[-1:].repeat(seq_len - old_num, 1)
                mdl.deberta.embeddings.position_embeddings = new_embed.to(mdl.device)

            rel_embeddings = getattr(mdl.deberta.encoder, "rel_embeddings", None)
            if rel_embeddings is not None:
                old_rel_num, rel_dim = rel_embeddings.weight.shape
                new_rel = torch.nn.Embedding(seq_len, rel_dim)
                new_rel.weight.data[:old_rel_num] = rel_embeddings.weight.data.clone()
                if seq_len > old_rel_num:
                    new_rel.weight.data[old_rel_num:] = rel_embeddings.weight.data[-1:].repeat(seq_len - old_rel_num, 1)
                mdl.deberta.encoder.rel_embeddings = new_rel.to(mdl.device)

            mdl.config.max_position_embeddings = seq_len
            print(f"âœ“ Position embeddings resized successfully")

    if grad_ckpt:
        mdl.gradient_checkpointing_enable()

    # ë©”ëª¨ë¦¬ ê¸°ë°˜ BS/grad_acc ìë™ ì‚°ì •
    max_bs = find_max_bs(mdl, tok, seq_len, start=2, limit=256)
    # ë„ˆë¬´ ì‘ì€ ê²½ìš° í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ í•˜í•œ 2
    per_device_bs = max(1, max_bs // 2)  # 50% ë§ˆì§„
    # throughput ê·¹ëŒ€í™”: grad_accì€ 1ë¡œ ì‹œì‘(ì‹¤ì œ step timeì€ ì½œë°±ì—ì„œ ê´€ì¸¡)
    grad_acc = 1

    # ë°ì´í„°/ì½œë ˆì´í„°
    tokenized = build_mlm_dataset(tok, max_length=seq_len, split="train", limit=train_limit)
    collator = DynCollator(tokenizer=tok, mlm=True, min_prob=min_prob, max_prob=max_prob, max_length=seq_len)

    # ì´ ìŠ¤í…(ëŒ€ì‹œë³´ë“œ ETA í‘œì‹œìš©)
    epochs = 1
    total_steps = math.ceil(len(tokenized) / (per_device_bs) / max(1, grad_acc)) * epochs

    # ì˜µí‹°ë§ˆì´ì €(Layer-wise LR Decay)
    no_decay = ["bias", "LayerNorm.weight"]
    named = list(mdl.named_parameters())
    total_layers = sum(1 for n, _ in named if "encoder.layer" in n) or 12
    groups = []
    for n, p in named:
        lr_here = lr
        if "encoder.layer." in n:
            try:
                k = int(n.split("encoder.layer.")[1].split(".")[0])
            except Exception:
                k = total_layers - 1
            lr_here = lr * (llrd ** (total_layers - 1 - k))
        wd_here = 0.0 if any(nd in n for nd in no_decay) else wd
        groups.append({"params": [p], "weight_decay": wd_here, "lr": lr_here})
    opt = torch.optim.AdamW(groups, lr=lr, weight_decay=wd)

    # HP/ë°ì´í„°ì…‹ ë©”íƒ€ (ëŒ€ì‹œë³´ë“œ í‘œì¶œìš©)
    hp = {
        "seq_len": seq_len,
        "per_device_bs": per_device_bs,
        "grad_acc": grad_acc,
        "effective_tokens_per_update": per_device_bs * grad_acc * seq_len,
        "warmup_ratio": warmup,
        "lr": lr,
        "weight_decay": wd,
        "min_prob": min_prob,
        "max_prob": max_prob,
        "bf16": torch.cuda.is_available(),
        "train_limit": train_limit,
    }
    dataset_meta = {"corpus": "wikimedia/wikipedia", "subset": "20231101.ko", "split": "train"}

    # í•™ìŠµ ì‹œì‘ ì´ë²¤íŠ¸ (Study ETA ê³„ì‚°ì„ ìœ„í•´)
    BUS.log(event="trial_begin", model=model_name, trial=trial.number, ts=time.time(), study_trials_total=planned_trials)

    args = TrainingArguments(
        output_dir=f"./runs/{model_name.replace('/','_')}/{trial.number}",
        per_device_train_batch_size=per_device_bs,
        per_device_eval_batch_size=max(2, per_device_bs),
        gradient_accumulation_steps=grad_acc,
        learning_rate=lr,
        weight_decay=wd,
        warmup_ratio=warmup,
        num_train_epochs=epochs,
        logging_steps=50,
        save_strategy="no",
        report_to=[],
        seed=seed,
        dataloader_drop_last=False,
        bf16=torch.cuda.is_available(),
    )

    tr = Trainer(
        model=mdl,
        args=args,
        train_dataset=tokenized,
        eval_dataset=None,
        data_collator=collator,
        tokenizer=tok,
        optimizers=(opt, None),
    )

    # ë¼ì´ë¸Œ ì½œë°±(ìŠ¤í… ì‹œê°„/ì²˜ë¦¬ëŸ‰/ì†ì‹¤/HP/ETA)
    tr.add_callback(
        LiveMetricsCallback(
            model_name=model_name,
            trial_id=trial.number,
            hp=hp,
            dataset_meta=dataset_meta,
            total_steps=total_steps,
            emit_every_steps=10,
        )
    )

    # í•™ìŠµ
    tr.train()

    # ===== í‰ê°€(ì‹¤ë°ì´í„° ê¸°ë°˜) =====
    print(f"\n{'â”€'*80}")
    print("ğŸ“Š Starting evaluation phase...")
    print(f"{'â”€'*80}\n")

    # Fill-Mask íŒŒì´í”„ë¼ì¸ (ì£¼ì˜: truncation/max_length kwargs ê¸ˆì§€)
    print("ğŸ”§ Creating fill-mask pipeline...")
    fill = pipeline("fill-mask", model=mdl, tokenizer=tok, device=0 if device == "cuda" else -1)

    # Ko-LAMBADA ì •í™•ë„
    print("ğŸ“– [1/2] Evaluating LAMBADA (600 samples)...")
    eval_lbd = build_eval_from_lambada(limit=600)
    l_t1 = eval_lambada_topk(
        fill,
        eval_lbd,
        mask_token=tok.mask_token or MASK_TOKEN_FALLBACK,
        k=1,
        batch_size=64,
        seq_len=seq_len,
    )
    print(f"   âœ“ LAMBADA@1 = {l_t1:.4f}")

    # Real Coref í‰ê°€ (ëŒ€ëª…ì‚¬ ì—†ê³  ê°™ì€ ëª…ì‚¬ 2ë²ˆ ì´ìƒ)
    print("ğŸ”— [2/2] Building real coref evaluation set (1600 samples)...")
    eval_coref = build_real_coref_eval_set(limit=1600, max_seq_len=seq_len, seed=999)
    print(f"   âœ“ Real coref set built: {len(eval_coref)} samples")

    print("ğŸ”— Evaluating real coref metrics...")
    real1 = eval_real_coref_top1(
        fill,
        eval_coref,
        mask_token=tok.mask_token or MASK_TOKEN_FALLBACK,
        batch_size=64,
        seq_len=seq_len,
    )
    print(f"   âœ“ Real@1 = {real1:.4f}")

    real5 = eval_real_coref_top5(
        fill,
        eval_coref,
        mask_token=tok.mask_token or MASK_TOKEN_FALLBACK,
        batch_size=64,
        seq_len=seq_len,
    )
    print(f"   âœ“ Real@5 = {real5:.4f}")

    print(f"\n{'â”€'*80}")
    print(f"âœ… Evaluation complete!")
    print(f"{'â”€'*80}\n")

    # ëŒ€ì‹œë³´ë“œ ì†¡ì‹ 
    BUS.log(section="eval_stream", model=model_name, trial=trial.number, lbd_top1=l_t1, real1=real1, real5=real5)

    # ìŠ¤ì½”ì–´ (Real1, Real5 ê¸°ë°˜)
    score = 0.4 * real1 + 0.3 * real5 + 0.3 * l_t1
    trial.set_user_attr("lbd_top1", l_t1)
    trial.set_user_attr("real1", real1)
    trial.set_user_attr("real5", real5)

    # í•™ìŠµ ì¢…ë£Œ ì´ë²¤íŠ¸
    BUS.log(event="trial_end", model=model_name, trial=trial.number, ts=time.time())

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del mdl
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return float(score)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Study ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TeeLogger:
    """Duplicate stdout to both console and file"""
    def __init__(self, file_path):
        self.terminal = __import__('sys').stdout
        self.log = open(file_path, 'w', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def close(self):
        self.log.close()


def run_study(model_name: str, n_trials: int = 15, seed: int = 42, train_limit: Optional[int] = None):
    import sys
    from datetime import datetime, timedelta

    # Setup automatic logging
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path("./runs") / "tune_logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / f"tune_{model_name.replace('/', '_')}_{timestamp}.log"

    print(f"ğŸ“ Logging to: {log_file}")
    tee = TeeLogger(str(log_file))
    sys.stdout = tee

    # Track timing for ETA
    trial_times = []
    study_start_time = time.time()

    sampler = optuna.samplers.TPESampler(seed=seed)
    pruner = optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=0)
    study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner, study_name=f"HPO-{model_name}")

    def _obj(t: optuna.Trial):
        trial_start = time.time()

        # Print trial header
        avg_time = sum(trial_times) / len(trial_times) if trial_times else None
        if avg_time:
            est_trial_time = timedelta(seconds=int(avg_time))
            est_remaining = timedelta(seconds=int(avg_time * (n_trials - t.number)))
            print(f"\n{'='*80}")
            print(f"ğŸš€ [Trial {t.number + 1}/{n_trials}] Starting...")
            print(f"   Estimated time per trial: {est_trial_time}")
            print(f"   Estimated remaining time: {est_remaining}")
            print(f"{'='*80}\n")
        else:
            print(f"\n{'='*80}")
            print(f"ğŸš€ [Trial {t.number + 1}/{n_trials}] Starting (first trial, no ETA yet)...")
            print(f"{'='*80}\n")

        result = objective(t, model_name, planned_trials=n_trials, train_limit=train_limit, seed=seed)

        trial_elapsed = time.time() - trial_start
        trial_times.append(trial_elapsed)

        # Print trial summary
        elapsed_str = str(timedelta(seconds=int(trial_elapsed)))

        # Get best score and attrs safely
        try:
            best_score = study.best_value
            best_attrs = study.best_trial.user_attrs
        except (ValueError, AttributeError):
            # First trial or no completed trials yet
            best_score = result
            best_attrs = t.user_attrs

        print(f"\n{'='*80}")
        print(f"âœ… [Trial {t.number + 1}/{n_trials}] Completed in {elapsed_str}")
        print(f"   Score: {result:.4f}")
        print(f"   Metrics: LAMBADA@1={t.user_attrs.get('lbd_top1', 0):.4f} | "
              f"Real@1={t.user_attrs.get('real1', 0):.4f} | "
              f"Real@5={t.user_attrs.get('real5', 0):.4f}")
        print(f"   Best so far: {best_score:.4f} "
              f"(LAMBADA@1={best_attrs.get('lbd_top1', 0):.4f}, "
              f"Real@5={best_attrs.get('real5', 0):.4f})")

        # Overall progress
        completed = t.number + 1
        progress_pct = (completed / n_trials) * 100
        avg_time_per_trial = sum(trial_times) / len(trial_times)
        remaining_trials = n_trials - completed
        est_remaining = timedelta(seconds=int(avg_time_per_trial * remaining_trials))

        print(f"   Overall progress: {completed}/{n_trials} trials ({progress_pct:.1f}%) | "
              f"Estimated remaining: {est_remaining}")
        print(f"{'='*80}\n")

        return result

    study.optimize(_obj, n_trials=n_trials, show_progress_bar=False)

    total_elapsed = time.time() - study_start_time
    total_elapsed_str = str(timedelta(seconds=int(total_elapsed)))

    print(f"\n{'='*80}")
    print(f"ğŸ‰ Study completed in {total_elapsed_str}")
    print(f"{'='*80}")
    print("=== Best Trial ===")
    bt = study.best_trial
    print(f"Score: {bt.value:.4f}")
    print(f"Params: {bt.params}")
    print(f"Metrics: LAMBADA@1={bt.user_attrs.get('lbd_top1', 0):.4f} | "
          f"Real@1={bt.user_attrs.get('real1', 0):.4f} | "
          f"Real@5={bt.user_attrs.get('real5', 0):.4f}")
    print(f"{'='*80}\n")

    # Restore stdout and close log file
    sys.stdout = tee.terminal
    tee.close()
    print(f"âœ… Log saved to: {log_file}")

    return study


if __name__ == "__main__":
    import argparse

    p = argparse.ArgumentParser()
    p.add_argument("--model", required=True, help="Hugging Face model id")
    p.add_argument("--trials", type=int, default=15)
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--train-limit", type=int, default=60000)
    args = p.parse_args()

    run_study(args.model, n_trials=args.trials, seed=args.seed, train_limit=args.train_limit)

# coref_automl/dataset_preparation.py
"""
ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„ ì‹œìŠ¤í…œ
- ê¸´ ì‹œí€€ìŠ¤(1024~2048) ì§€ì›
- Kiwi í’ˆì§ˆ ë¶„ì„ ê¸°ë°˜ í•„í„°ë§
- ìƒí˜¸ì°¸ì¡° ìµœì í™”
"""

from __future__ import annotations
import os
import gc
import math
import random
import time
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple
from functools import lru_cache

os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
from datasets import load_dataset, concatenate_datasets, disable_caching, load_from_disk
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
from tqdm import tqdm
def _init_kiwi():
    global KIWI
    from kiwipiepy import Kiwi
    KIWI = Kiwi()  # í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë…ë¦½ ì´ˆê¸°í™”



# ìƒëŒ€ importë¥¼ ì ˆëŒ€ importë¡œ ë³€ê²½ (ì§ì ‘ ì‹¤í–‰ ì§€ì›)
try:
    from .coref_utils import is_noun
    from .bus import BUS
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from coref_automl.coref_utils import is_noun
    from coref_automl.bus import BUS

# disable_caching()

# Kiwi ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
KIWI = Kiwi()

@dataclass
class DatasetSource:
    """ë°ì´í„°ì…‹ ì†ŒìŠ¤ ì„¤ì •"""
    name: str
    source: str
    subset: Optional[str]
    split: str
    domain: str
    quality_weight: float
    description: str
    is_streaming: bool = False

def get_all_dataset_sources() -> List[DatasetSource]:
    """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ì†ŒìŠ¤"""
    return [
        # Wikipedia - ê¸´ ë¬¸ì„œ, ë‹¤ì–‘í•œ ì£¼ì œ
        DatasetSource(
            name="wiki_ko",
            source="wikimedia/wikipedia",
            subset="20231101.ko",
            split="train",
            domain="encyclopedia",
            quality_weight=0.8,
            description="í•œêµ­ì–´ ìœ„í‚¤ë°±ê³¼ - ë‹¤ì–‘í•œ ì£¼ì œì˜ ê¸´ ë¬¸ì„œ"
        ),

        # KLUE MRC - ì§ˆë¬¸-ë‹µë³€, ê¸´ ë§¥ë½
        DatasetSource(
            name="klue_mrc",
            source="klue",
            subset="mrc",
            split="train",
            domain="qa_long",
            quality_weight=1.0,
            description="KLUE MRC - ê¸´ ë§¥ë½ì˜ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°"
        ),

        # KLUE YNAT - ë‰´ìŠ¤ ê¸°ì‚¬
        DatasetSource(
            name="klue_ynat",
            source="klue",
            subset="ynat",
            split="train",
            domain="news_topic",
            quality_weight=0.9,
            description="KLUE YNAT - ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ê³¼ ë‚´ìš©"
        ),

        # KorQuAD - ì§ˆë¬¸-ë‹µë³€
        DatasetSource(
            name="korquad",
            source="squad_kor_v1",
            subset=None,
            split="train",
            domain="qa_general",
            quality_weight=0.7,
            description="KorQuAD - í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹"
        ),

        # KLUE STS - ë¬¸ì¥ ìœ ì‚¬ë„ (ê¸´ í…ìŠ¤íŠ¸ ì¡°í•©ìš©)
        DatasetSource(
            name="klue_sts",
            source="klue",
            subset="sts",
            split="train",
            domain="similarity",
            quality_weight=0.6,
            description="KLUE STS - ë¬¸ì¥ ìœ ì‚¬ë„ (ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ìš©)"
        ),

        # KLUE NLI - ì¶”ë¡  (ê¸´ í…ìŠ¤íŠ¸ ì¡°í•©ìš©)
        DatasetSource(
            name="klue_nli",
            source="klue",
            subset="nli",
            split="train",
            domain="inference",
            quality_weight=0.5,
            description="KLUE NLI - ìì—°ì–´ ì¶”ë¡  (ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ìš©)"
        ),

        # ===== ìƒˆë¡œìš´ 8ê°œ ë°ì´í„°ì…‹ =====

        # 1. SKT KoBEST HellaSwag
        DatasetSource(
            name="kobest_hellaswag",
            source="skt/kobest_v1",
            subset="hellaswag",
            split="train",
            domain="hellaswag",
            quality_weight=0.7,
            description="HellaSwag í•œêµ­ì–´ ë²„ì „"
        ),

        # 2. HPLT Korean (ëŒ€ê·œëª¨ ì›¹ í¬ë¡¤) - ê¸°ë³¸ 50ë§Œ ìƒ˜í”Œë¡œ ì œí•œ
        DatasetSource(
            name="hplt_korean",
            source="HPLT/HPLT2.0_cleaned",
            subset="kor_Hang",
            split="train",
            domain="hplt_general",
            quality_weight=0.6,
            description="HPLT ëŒ€ê·œëª¨ í•œêµ­ì–´ ì›¹ í¬ë¡¤ (50ë§Œ ìƒ˜í”Œ)",
            is_streaming=True
        ),

        # 3. ë²ˆì—­ëœ ë¯¸êµ­ ë‰´ìŠ¤
        DatasetSource(
            name="translated_us_news",
            source="nmixx-fin/ko-trans-us_news_retrieval",
            subset=None,
            split="train",
            domain="translated_news",
            quality_weight=0.7,
            description="ë²ˆì—­ëœ ë¯¸êµ­ ë‰´ìŠ¤"
        ),

        # 4. ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½
        DatasetSource(
            name="finance_news_summ",
            source="nmixx-fin/twice_kr_finance_news_summ",
            subset=None,
            split="train",
            domain="finance_news",
            quality_weight=0.9,
            description="ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½"
        ),

        # 5. ê²½ì œ ë‰´ìŠ¤ BQA
        DatasetSource(
            name="economy_bqa",
            source="nmixx-fin/twice_kr_news_bqa_cls",
            subset=None,
            split="train",
            domain="economy_news",
            quality_weight=0.9,
            description="ê²½ì œ ë‰´ìŠ¤ BQA"
        ),

        # 6. ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„±
        DatasetSource(
            name="finance_sentiment",
            source="nmixx-fin/twice_kr_fin_news_sent_cls",
            subset=None,
            split="train",
            domain="finance_sentiment",
            quality_weight=0.7,
            description="ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„"
        ),

        # 7. ë„¤ì´ë²„ ë‰´ìŠ¤ (ìµœê³  í’ˆì§ˆ)
        DatasetSource(
            name="naver_news_gen",
            source="dev7halo/naver-news-summarization-ko-with-gen",
            subset=None,
            split="train",
            domain="naver_news",
            quality_weight=1.0,
            description="ë„¤ì´ë²„ ë‰´ìŠ¤ ìš”ì•½ (ìµœê³  í’ˆì§ˆ)"
        ),

        # 8. AIR-Bench QA ë‰´ìŠ¤
        DatasetSource(
            name="air_bench_news",
            source="AIR-Bench/qa_news_ko",
            subset=None,
            split="corpus_default",
            domain="qa_news",
            quality_weight=0.9,
            description="AIR-Bench QA ë‰´ìŠ¤",
            is_streaming=True
        ),
    ]

@lru_cache(maxsize=4096)
def analyze_coref_quality_cached(text: str) -> Dict[str, float]:
    """Kiwië¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ Coreference í’ˆì§ˆ ë¶„ì„ (ìºì‹± ì ìš©)"""

    # í˜•íƒœì†Œ ë¶„ì„
    tokens = KIWI.tokenize(text)

    # ëŒ€ëª…ì‚¬ì™€ ê°œì²´ ë¶„ì„
    pronouns = []
    entities = []
    verbs = []
    meaningful_words = []

    for token in tokens:
        # Kiwi íƒœê·¸ ì„¤ëª…:
        # NP: ëŒ€ëª…ì‚¬ (ê·¸, ê·¸ë…€, ì´ê²ƒ ë“±)
        # NNG/NNP/NNB: ì¼ë°˜ëª…ì‚¬/ê³ ìœ ëª…ì‚¬/ì˜ì¡´ëª…ì‚¬
        # VV/VX/VA: ë™ì‚¬/ë³´ì¡°ë™ì‚¬/í˜•ìš©ì‚¬
        if token.tag == 'NP':  # ëŒ€ëª…ì‚¬
            pronouns.append(token.form)
        elif token.tag in ['NNG', 'NNP', 'NNB']:  # ëª…ì‚¬ë¥˜
            if len(token.form) > 1:  # í•œ ê¸€ì ì œì™¸
                entities.append(token.form)
        elif token.tag.startswith(('V', 'VA')):  # ë™ì‚¬/í˜•ìš©ì‚¬
            verbs.append(token.form)

        # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)
        if token.tag.startswith(('N', 'V', 'M', 'VA', 'VV')):
            meaningful_words.append(token.form)

    # í†µê³„ ê³„ì‚°
    pronoun_count = len(pronouns)
    entity_count = len(entities)
    verb_count = len(verbs)
    total_words = len(meaningful_words)

    # ë°€ë„ ê³„ì‚°
    pronoun_density = pronoun_count / max(1, total_words)
    entity_density = entity_count / max(1, total_words)
    verb_density = verb_count / max(1, total_words)

    # ê³ ê¸‰ Coreference ì ìˆ˜ ê³„ì‚°
    # 1. ëŒ€ëª…ì‚¬-ê°œì²´ ìƒí˜¸ì‘ìš© ì ìˆ˜
    coref_interaction = min(1.0, (pronoun_density * 20) + (entity_density * 3) + (pronoun_density * entity_density * 50))

    # 2. í…ìŠ¤íŠ¸ ë³µì¡ë„ ì ìˆ˜ (ë‹¤ì–‘í•œ í’ˆì‚¬ ì‚¬ìš©)
    complexity_score = min(1.0, (entity_density * 10) + (verb_density * 5) + (pronoun_density * 15))

    # 3. Coreference ì í•©ì„± ì ìˆ˜ (ëŒ€ëª…ì‚¬ì™€ ê°œì²´ì˜ ê· í˜•)
    balance_score = 1.0 - abs(pronoun_density - entity_density * 0.3)  # ì´ìƒì ì¸ ë¹„ìœ¨

    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
    quality_score = (coref_interaction * 0.5) + (complexity_score * 0.3) + (balance_score * 0.2)

    return {
        'pronoun_density': pronoun_density,
        'entity_density': entity_density,
        'verb_density': verb_density,
        'coref_score': coref_interaction,
        'complexity_score': complexity_score,
        'balance_score': balance_score,
        'quality_score': quality_score,
        'pronoun_count': pronoun_count,
        'entity_count': entity_count,
        'verb_count': verb_count,
        'total_words': total_words,
        'unique_pronouns': len(set(pronouns)),
        'unique_entities': len(set(entities)),
    }

def analyze_coref_quality(text: str) -> Dict[str, float]:
    """ìºì‹±ëœ í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜"""
    return analyze_coref_quality_cached(text)

def _analyze_batch(text_batch):
    # ë°°ì¹˜ ë‹¨ìœ„ ë¶„ì„ (ê¸°ì¡´ analyze_coref_quality_cached ì¬ì‚¬ìš© ê°€ëŠ¥)
    return [analyze_coref_quality_cached(t) for t in text_batch]

def batch_analyze_quality(texts: List[str], batch_size: int = 2000, max_workers: Optional[int] = None) -> List[Dict[str, float]]:
    if max_workers is None:
        max_workers = max(1, min(os.cpu_count() or 1, 16))  # ê³¼ì„­ìŠ¤í¬ë¦½ì…˜ ë°©ì§€

    print(f"  ğŸ“Š ì´ {len(texts):,}ê°œ ìƒ˜í”Œ í’ˆì§ˆ ë¶„ì„ ì‹œì‘ (ì›Œì»¤: {max_workers}ê°œ)", flush=True)

    CHUNK_SIZE = 20000
    results, submitted = [], 0
    overall_start = time.time()

    from concurrent.futures import ProcessPoolExecutor, as_completed
    with ProcessPoolExecutor(max_workers=max_workers, initializer=_init_kiwi) as ex:
        futures = []
        for c in range(0, len(texts), CHUNK_SIZE):
            chunk = texts[c:c+CHUNK_SIZE]
            for b in range(0, len(chunk), batch_size):
                futures.append(ex.submit(_analyze_batch, chunk[b:b+batch_size]))
                submitted += 1

        done = 0
        last = time.time()
        for f in as_completed(futures):
            results.extend(f.result())
            done += 1
            now = time.time()
            if done % 20 == 0 or now - last > 10:
                pct = 100.0 * done / submitted
                speed = (len(results) / max(1e-9, (now - overall_start)))
                eta = (len(texts) - len(results)) / max(1e-6, speed)
                print(f"    âš¡ ì§„í–‰: {pct:.1f}% | {speed:.1f} ìƒ˜í”Œ/ì´ˆ | ETA {eta/60:.1f}ë¶„", flush=True)
                last = now

    print(f"  âœ… ì „ì²´ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ: {len(results):,}/{len(texts):,}", flush=True)
    return results

def load_and_preprocess_dataset(source: DatasetSource, tokenizer, target_seq_len: int, limit: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""

    start_time = time.time()

    try:
        print(f"ğŸ“¥ Loading {source.name}: {source.description}")

        # ë°ì´í„° ë¡œë“œ
        load_kwargs = {"split": source.split}
        if source.subset:
            load_kwargs["name"] = source.subset

        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ì²˜ë¦¬
        if source.is_streaming:
            load_kwargs["streaming"] = True
            dataset_stream = load_dataset(source.source, **load_kwargs)

            # ìŠ¤íŠ¸ë¦¬ë°: limit ìˆìœ¼ë©´ ì œí•œ, ì—†ìœ¼ë©´ iteratorë¡œ ì§ì ‘ ì²˜ë¦¬
            if limit:
                # ì œí•œì´ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ì— ë¡œë“œ
                samples = []
                for i, sample in enumerate(dataset_stream):
                    if i >= limit:
                        break
                    samples.append(sample)
                from datasets import Dataset
                dataset = Dataset.from_list(samples)
                print(f"  âœ… Loaded {len(dataset)} raw samples (limited)")

                # ë„ë©”ì¸ë³„ ì „ì²˜ë¦¬
                processed_texts = preprocess_domain_texts(dataset, source, tokenizer, target_seq_len, is_iterator=False)
            else:
                # limit ì—†ìŒ: ëŒ€í˜• ë°ì´í„°ì…‹ì€ ê¸°ë³¸ ì œí•œ ì ìš© (50ë§Œ ìƒ˜í”Œ)
                default_streaming_limit = 500000
                print(f"  ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ìµœëŒ€ {default_streaming_limit:,}ê°œ)")

                # ì œí•œëœ iteratorë¡œ ì „ì²˜ë¦¬
                samples = []
                for i, sample in enumerate(dataset_stream):
                    if i >= default_streaming_limit:
                        break
                    samples.append(sample)

                    # 1ë§Œê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                    if (i + 1) % 10000 == 0:
                        print(f"\r  â³ ë¡œë”© ì¤‘: {i+1:,}/{default_streaming_limit:,} ({(i+1)/default_streaming_limit*100:.1f}%)", end="", flush=True)

                print(f"\r  âœ… ë¡œë”© ì™„ë£Œ: {len(samples):,} ìƒ˜í”Œ" + " " * 20)

                from datasets import Dataset
                dataset = Dataset.from_list(samples)

                # ì „ì²˜ë¦¬
                processed_texts = preprocess_domain_texts(dataset, source, tokenizer, target_seq_len, is_iterator=False)
        else:
            dataset = load_dataset(source.source, **load_kwargs)

            # ìƒ˜í”Œ ì œí•œ
            if limit and limit < len(dataset):
                dataset = dataset.select(range(limit))

            print(f"  âœ… Loaded {len(dataset)} raw samples")

            # ë„ë©”ì¸ë³„ ì „ì²˜ë¦¬
            processed_texts = preprocess_domain_texts(dataset, source, tokenizer, target_seq_len, is_iterator=False)

        if not processed_texts:
            print(f"  âš ï¸ No valid texts after preprocessing")
            return None

        elapsed_time = time.time() - start_time
        print(f"  âœ… Preprocessed {len(processed_texts)} texts")
        print(f"  â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({len(processed_texts)/elapsed_time:.1f} ìƒ˜í”Œ/ì´ˆ)")

        # Dataset ê°ì²´ë¡œ ë³€í™˜
        from datasets import Dataset
        return Dataset.from_list([{"text": text} for text in processed_texts])

    except Exception as e:
        print(f"  âŒ Failed to load {source.name}: {e}")
        return None

def preprocess_domain_texts(dataset, source: DatasetSource, tokenizer, target_seq_len: int, is_iterator: bool = False) -> List[str]:
    """ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (iterator ì§€ì›)"""

    processed_texts = []
    skipped = 0
    chunked = 0

    # iteratorì¸ ê²½ìš°ì—ë„ ì§„í–‰ë¥  í‘œì‹œ (ìƒ˜í”Œ ì¹´ìš´íŠ¸ë§Œ)
    if is_iterator:
        print(f"  ğŸ“ ìŠ¤íŠ¸ë¦¬ë° ì „ì²˜ë¦¬ ì¤‘... (ì§„í–‰ ì¤‘)", end="", flush=True)
        iterator = dataset
        show_progress = True
        progress_interval = 1000  # 1000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
    else:
        iterator = tqdm(dataset, desc=f"  ğŸ“ ì „ì²˜ë¦¬", unit="ìƒ˜í”Œ", leave=False)
        show_progress = False

    sample_count = 0
    for item in iterator:
        sample_count += 1

        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: 1000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        if show_progress and sample_count % progress_interval == 0:
            print(f"\r  ğŸ“ ìŠ¤íŠ¸ë¦¬ë° ì „ì²˜ë¦¬ ì¤‘... (ì²˜ë¦¬: {sample_count:,}ê°œ, ìƒì„±: {len(processed_texts):,}ê°œ, ìŠ¤í‚µ: {skipped:,}ê°œ)", end="", flush=True)
        try:
            # ë„ë©”ì¸ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if source.domain == "encyclopedia":
                text = item.get("text", "").strip()
            elif source.domain == "qa_long":
                context = item.get("context", "")
                question = item.get("question", "")
                answer = item.get("answers", {}).get("text", [""])[0] if item.get("answers") else ""
                text = f"{context} {question} {answer}".strip()
            elif source.domain == "news_topic":
                title = item.get("title", "")
                content = item.get("content", item.get("description", ""))
                text = f"{title} {content}".strip()
            elif source.domain == "qa_general":
                context = item.get("context", "")
                question = item.get("question", "")
                answer = item.get("answers", {}).get("text", [""])[0] if item.get("answers") else ""
                text = f"{context} {question} {answer}".strip()
            elif source.domain == "similarity":
                sentence1 = item.get("sentence1", item.get("text", ""))
                sentence2 = item.get("sentence2", "")
                text = f"{sentence1} {sentence2}".strip()
            elif source.domain == "inference":
                premise = item.get("premise", "")
                hypothesis = item.get("hypothesis", "")
                if premise and hypothesis:
                    text = f"{premise} {hypothesis}".strip()
                else:
                    continue

            # ===== ìƒˆë¡œìš´ 8ê°œ ë„ë©”ì¸ ì²˜ë¦¬ =====
            elif source.domain == "hellaswag":
                text = item.get("paragraph", "").strip()
            elif source.domain == "hplt_general":
                text = item.get("text", "").strip()
            elif source.domain == "translated_news":
                text = item.get("trans_text", "").strip()
            elif source.domain == "finance_news":
                text = item.get("text", "").strip()
            elif source.domain == "economy_news":
                text = item.get("text", "").strip()
            elif source.domain == "finance_sentiment":
                text = item.get("text", "").strip()
            elif source.domain == "naver_news":
                text = item.get("document", "").strip()
            elif source.domain == "qa_news":
                text = item.get("text", "").strip()

            else:
                text = item.get("text", "").strip()

            # ê¸°ë³¸ í•„í„°ë§
            if not text or len(text) < 50:
                skipped += 1
                continue

            # í† í° ê¸¸ì´ í•„í„°ë§ (ê°•í™”)
            tokens = tokenizer.encode(text, add_special_tokens=False, truncation=False)
            token_len = len(tokens)

            # ìµœì†Œ ê¸¸ì´ ì²´í¬ (Combined ê¸°ì¤€: 100 í† í°)
            # â˜…â˜…â˜… ê°œì„ : 0.3x ì œì•½(460í† í°) â†’ 100í† í°ìœ¼ë¡œ ì™„í™” (ì§§ì€ ë°ì´í„°ì…‹ í¬í•¨)
            if token_len < 100:
                skipped += 1
                continue

            # ì ì ˆí•œ ê¸¸ì´: ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if token_len <= target_seq_len:
                processed_texts.append(text)
            # ì•½ê°„ ê¸´ ê²½ìš° (1.0 ~ 1.5ë°°): truncate
            elif token_len <= target_seq_len * 1.5:
                truncated_ids = tokens[:target_seq_len]
                truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)
                processed_texts.append(truncated_text)
            # ë§ì´ ê¸´ ê²½ìš° (1.5ë°° ì´ˆê³¼): ì²­í‚¹
            else:
                chunks = chunk_text_with_coref_preservation(text, target_seq_len, tokenizer)
                processed_texts.extend(chunks)
                chunked += 1

        except Exception as e:
            skipped += 1
            continue

    # ìµœì¢… ì§„í–‰ ìƒí™© ì¶œë ¥
    if show_progress:
        print(f"\r  ğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_texts):,}ê°œ ìƒì„± (ì²˜ë¦¬: {sample_count:,}ê°œ, ìŠ¤í‚µ: {skipped:,}, ì²­í‚¹: {chunked})                    ")
    else:
        print(f"  ğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_texts)}ê°œ ìƒì„± (ìŠ¤í‚µ: {skipped}, ì²­í‚¹: {chunked})")

    return processed_texts

def chunk_text_with_coref_preservation(text: str, chunk_size: int, tokenizer, overlap: int = 200) -> List[str]:
    """Coreference ë§¥ë½ì„ ìœ ì§€í•˜ë©° ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹ (ê¸¸ì´ ë³´ì¥)"""

    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    sentences = []
    current = ""
    for char in text:
        current += char
        if char in ['.', '!', '?'] and len(current) > 10:
            sentences.append(current.strip())
            current = ""

    if current.strip():
        sentences.append(current.strip())

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for sentence in sentences:
        sentence_tokens = len(tokenizer.encode(sentence, add_special_tokens=False, truncation=False))

        # ë¬¸ì¥ ìì²´ê°€ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ truncate
        if sentence_tokens > chunk_size:
            # í˜„ì¬ ì²­í¬ë¥¼ ë¨¼ì € ì €ì¥
            if current_chunk and current_tokens > chunk_size * 0.3:
                chunks.append(current_chunk.strip())

            # ê¸´ ë¬¸ì¥ì„ í† í° ë‹¨ìœ„ë¡œ truncate
            sentence_ids = tokenizer.encode(sentence, add_special_tokens=False)
            truncated_ids = sentence_ids[:chunk_size]
            truncated_sentence = tokenizer.decode(truncated_ids, skip_special_tokens=True)
            chunks.append(truncated_sentence)

            # ì²­í¬ ì´ˆê¸°í™”
            current_chunk = ""
            current_tokens = 0
            continue

        # ì¼ë°˜ ì²­í‚¹ ë¡œì§
        if current_tokens + sentence_tokens > chunk_size and current_chunk:
            if current_tokens > chunk_size * 0.3:  # ìµœì†Œ ê¸¸ì´
                chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_tokens = sentence_tokens
        else:
            current_chunk += " " + sentence
            current_tokens += sentence_tokens

    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk and current_tokens > chunk_size * 0.3:
        chunks.append(current_chunk.strip())

    # ìµœì¢… ê²€ì¦: ëª¨ë“  ì²­í¬ê°€ chunk_size ì´í•˜ì¸ì§€ í™•ì¸
    verified_chunks = []
    for chunk in chunks:
        chunk_tokens = len(tokenizer.encode(chunk, add_special_tokens=False, truncation=False))
        if chunk_tokens <= chunk_size:
            verified_chunks.append(chunk)
        else:
            # ì—¬ì „íˆ ê¸¸ë©´ ê°•ì œ truncate
            chunk_ids = tokenizer.encode(chunk, add_special_tokens=False)
            truncated_ids = chunk_ids[:chunk_size]
            truncated_chunk = tokenizer.decode(truncated_ids, skip_special_tokens=True)
            verified_chunks.append(truncated_chunk)

    return verified_chunks

def prepare_coref_optimized_datasets(
    model_name: str = "kakaobank/kf-deberta-base",
    seq_lengths: List[int] = [1024, 1536, 2048],
    save_path: str = "./prepared_datasets",
    quality_threshold: float = 0.3,  # â˜…â˜…â˜… 0.6 â†’ 0.3 ì™„í™” (ë” ë§ì€ ë°ì´í„° í¬í•¨)
    max_samples_per_length: int = None
):
    """
    ìƒí˜¸ì°¸ì¡° ìµœì í™”ëœ ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„
    - Kiwi í’ˆì§ˆ ë¶„ì„ ê¸°ë°˜ í•„í„°ë§
    - ê¸´ ì‹œí€€ìŠ¤ ì§€ì›
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
    """

    import os
    os.makedirs(save_path, exist_ok=True)

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print("ğŸš€ ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„ ì‹œìŠ¤í…œ")
    print(f"ğŸ¯ ëª¨ë¸: {model_name}")
    print(f"ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_lengths}")
    print(f"â­ í’ˆì§ˆ ì„ê³„ê°’: {quality_threshold}")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")
    print("=" * 80)

    sources = get_all_dataset_sources()

    for seq_len in seq_lengths:
        print(f"\nğŸ¯ {seq_len} í† í° ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

        # í† í¬ë‚˜ì´ì € max_length ë™ì  í™•ì¥
        original_max_length = tokenizer.model_max_length
        tokenizer.model_max_length = seq_len
        print(f"   Tokenizer max_length: {original_max_length} â†’ {seq_len}")

        all_datasets = []
        total_samples = 0

        # ê° ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        for source in sources:
            try:
                print(f"\nğŸ“¥ ì²˜ë¦¬ ì¤‘: {source.name}")

                # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (limit ê³„ì‚°: Noneì´ë©´ ë¬´ì œí•œ)
                limit_per_source = max_samples_per_length // len(sources) if max_samples_per_length else None
                dataset = load_and_preprocess_dataset(source, tokenizer, seq_len, limit=limit_per_source)

                if dataset is None or len(dataset) == 0:
                    continue

                print(f"  ğŸ” í’ˆì§ˆ ë¶„ì„ ì¤‘... ({len(dataset)} ìƒ˜í”Œ)")

                # â˜…â˜…â˜… MLM v3 ê°œì„ : ìƒ˜í”Œë§ ì œê±° - ì „ì²´ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ â˜…â˜…â˜…
                # ëª¨ë“  ìƒ˜í”Œ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¶©ë¶„íˆ ë¹ ë¦„)
                print(f"  âš¡ ì „ì²´ ìƒ˜í”Œ í’ˆì§ˆ ë¶„ì„ (ìƒ˜í”Œë§ ì—†ìŒ)")

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì „ì²´)
                texts = [dataset[idx]['text'] for idx in range(len(dataset))]

                # ë³‘ë ¬ í’ˆì§ˆ ë¶„ì„ (ì „ì²´)
                qualities = batch_analyze_quality(texts, batch_size=500, max_workers=None)

                # ì¸ë±ìŠ¤ë„ ì „ì²´
                indices = list(range(len(dataset)))

                # í’ˆì§ˆ í•„í„°ë§
                high_quality_samples = []
                quality_stats = {'analyzed': len(qualities), 'passed': 0, 'avg_quality': 0}

                for idx, quality in zip(indices, qualities):
                    quality_stats['avg_quality'] += quality['quality_score']
                    if quality['quality_score'] >= quality_threshold:
                        high_quality_samples.append(idx)
                        quality_stats['passed'] += 1

                quality_stats['avg_quality'] /= max(1, quality_stats['analyzed'])
                pass_rate = (quality_stats['passed'] / max(1, quality_stats['analyzed'])) * 100

                print(f"  âœ… í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ: {quality_stats['passed']}/{quality_stats['analyzed']} í†µê³¼ ({pass_rate:.1f}%)")
                print(f"    ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {quality_stats['avg_quality']:.3f}")
                # ê³ í’ˆì§ˆ ìƒ˜í”Œë§Œ ì„ íƒ
                if high_quality_samples:
                    filtered_dataset = dataset.select(high_quality_samples)
                    all_datasets.append(filtered_dataset)
                    total_samples += len(filtered_dataset)

                    print(f"  ğŸ¯ ì„ íƒëœ ìƒ˜í”Œ: {len(filtered_dataset)}")

            except Exception as e:
                print(f"  âŒ {source.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ë°ì´í„° í†µí•©
        if all_datasets:
            print(f"\nğŸ”„ {seq_len} í† í° ë°ì´í„°ì…‹ í†µí•© ì¤‘... ({total_samples} ìƒ˜í”Œ)")

            combined_dataset = concatenate_datasets(all_datasets)
            combined_dataset = combined_dataset.shuffle(seed=42)

            # ìµœì¢… ìƒ˜í”Œ ì œí•œ (max_samples_per_lengthê°€ Noneì´ ì•„ë‹ ë•Œë§Œ)
            if max_samples_per_length and len(combined_dataset) > max_samples_per_length:
                combined_dataset = combined_dataset.select(range(max_samples_per_length))

            # í† í°í™”
            print(f"ğŸ”¤ í† í°í™” ì¤‘... ({len(combined_dataset)} ìƒ˜í”Œ)")

            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=seq_len,
                    # return_tensors="pt"
                )

            # ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸°ì™€ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ë™ì  ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨)
            if seq_len >= 2048:
                batch_size = 100
                num_proc = 4
                print(f"  âš™ï¸  í† í°í™” ì„¤ì • (ê¸´ ì‹œí€€ìŠ¤): batch_size={batch_size}, num_proc={num_proc}")
            elif seq_len >= 1536:
                batch_size = 200
                num_proc = 8
                print(f"  âš™ï¸  í† í°í™” ì„¤ì • (ì¤‘ê°„ ì‹œí€€ìŠ¤): batch_size={batch_size}, num_proc={num_proc}")
            else:
                batch_size = 500
                num_proc = 12
                print(f"  âš™ï¸  í† í°í™” ì„¤ì • (ì§§ì€ ì‹œí€€ìŠ¤): batch_size={batch_size}, num_proc={num_proc}")

            def safe_map(ds, fn, **kwargs):
                try:
                    return ds.map(fn, **kwargs)
                except BrokenPipeError:
                    # ì›Œì»¤ ì¤„ì—¬ì„œ 1íšŒ ì¬ì‹œë„
                    kwargs["num_proc"] = 1
                    return ds.map(fn, **kwargs)

            N_SHARDS = 16
            shard_paths = []

            for i in range(N_SHARDS):
                shard = combined_dataset.shard(num_shards=N_SHARDS, index=i, contiguous=True)
                tok = safe_map(
                    shard,
                    tokenize_function,
                    batched=True,
                    batch_size=batch_size,
                    remove_columns=["text"],
                    num_proc=num_proc,
                    writer_batch_size=1000,
                    load_from_cache_file=True,
                    desc=f"Tokenizing shard {i+1}/{N_SHARDS} @ {seq_len}"
                )
                out_dir = f"{save_path}/tmp_{seq_len}_shard_{i}"
                tok.save_to_disk(out_dir)
                shard_paths.append(out_dir)

            tokenized_dataset = concatenate_datasets([load_from_disk(p) for p in shard_paths])
            tokenized_dataset.save_to_disk( f"{save_path}/{model_name.replace('/', '_')}_{seq_len}_coref_optimized")


            # print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_file}")
            print(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹: {len(tokenized_dataset)} ìƒ˜í”Œ Ã— {seq_len} í† í°")

            # ìµœì¢… í’ˆì§ˆ ê²€ì¦
            print("ğŸ¯ ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì¤‘...")

            def preview_text_from_ids(input_ids, tokenizer, max_tokens=300):
                from itertools import compress
                out = tokenizer.prepare_for_model(input_ids, add_special_tokens=False)
                ids = input_ids[:max_tokens]
                return tokenizer.decode(ids, skip_special_tokens=True)

            final_qualities = []
            for i in range(min(100, len(tokenized_dataset))):
                sample = tokenized_dataset[i]
                input_ids = sample['input_ids']
                clean_ids = [x for x in input_ids if x != tokenizer.pad_token_id][:300]
                ids = tokenized_dataset[i]["input_ids"]
                text = preview_text_from_ids(ids, tokenizer, 300)
                quality = analyze_coref_quality(text)
                final_qualities.append(quality)

            avg_final_quality = sum(q['quality_score'] for q in final_qualities) / len(final_qualities)
            avg_pronoun_density = sum(q['pronoun_density'] for q in final_qualities) / len(final_qualities)
            avg_entity_density = sum(q['entity_density'] for q in final_qualities) / len(final_qualities)

            print("ğŸ“Š ìµœì¢… í’ˆì§ˆ ì§€í‘œ:")
            print(f"  ğŸ“ˆ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_final_quality:.3f}")
            print(f"  ğŸ”¤ ëŒ€ëª…ì‚¬ ë°€ë„: {avg_pronoun_density:.3f}")
            print(f"  ğŸ¢ ê°œì²´ ë°€ë„: {avg_entity_density:.3f}")

            rating = 'â­ ìš°ìˆ˜' if avg_final_quality > 0.8 else 'âœ… ì–‘í˜¸' if avg_final_quality > 0.6 else 'âš ï¸ ë³´í†µ'
            print(f"ğŸ† í’ˆì§ˆ ë“±ê¸‰: {rating}")

        else:
            print(f"âš ï¸ {seq_len} í† í° ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")

    print("\nğŸŠ ëª¨ë“  ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {save_path}")
    print("ğŸš€ ì´ì œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    import argparse
    import multiprocessing

    parser = argparse.ArgumentParser(description="ê³ í’ˆì§ˆ Coreference ë°ì´í„°ì…‹ ì‚¬ì „ ì¤€ë¹„")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--seq-lengths", nargs="+", type=int, default=[1024, 1536, 2048], help="ì‹œí€€ìŠ¤ ê¸¸ì´ë“¤")
    parser.add_argument("--save-path", default="./prepared_datasets_mlm_v2", help="ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--quality-threshold", type=float, default=0.6, help="í’ˆì§ˆ ì„ê³„ê°’")
    parser.add_argument("--max-samples", type=lambda x: None if x.lower() == 'none' else int(x), default=None, help="ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (None=ë¬´ì œí•œ)")

    args = parser.parse_args()

    prepare_coref_optimized_datasets(
        model_name=args.model,
        seq_lengths=args.seq_lengths,
        save_path=args.save_path,
        quality_threshold=args.quality_threshold,
        max_samples_per_length=args.max_samples
    )
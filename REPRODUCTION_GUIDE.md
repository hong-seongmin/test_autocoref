# í›ˆë ¨ ì¬í˜„ ê°€ì´ë“œ (Reproduction Guide)

## ğŸ“¦ ì•„ì¹´ì´ë¸Œ ë‚´ìš©

**íŒŒì¼**: `runs_combined_experiment_full.tar.gz` (6.5GB)

### í¬í•¨ëœ íŒŒì¼ë“¤ (95ê°œ)

#### 1. ë¬¸ì„œ ë° í‰ê°€ ê²°ê³¼
- âœ… `README.md` - ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¬¸ì„œ
- âœ… `eval_1536.jsonl`, `eval_2048.jsonl` - í‰ê°€ ê²°ê³¼
- âœ… `eval_1536.log`, `eval_2048.log` - í‰ê°€ ë¡œê·¸

#### 2. ì²´í¬í¬ì¸íŠ¸ë³„ íŒŒì¼ (10ê°œ ì²´í¬í¬ì¸íŠ¸)

ê° ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ í¬í•¨:
- âœ… `config.json` - ëª¨ë¸ ì„¤ì •
- âœ… **`model.safetensors`** - **ëª¨ë¸ ê°€ì¤‘ì¹˜ (713MB)** â† í•µì‹¬!
- âœ… `tokenizer.json` - í† í¬ë‚˜ì´ì € (3.1MB)
- âœ… `tokenizer_config.json` - í† í¬ë‚˜ì´ì € ì„¤ì •
- âœ… `vocab.txt` - ì–´íœ˜ ì‚¬ì „ (1.1MB)
- âœ… `special_tokens_map.json` - íŠ¹ìˆ˜ í† í°
- âœ… `training_args.bin` - í›ˆë ¨ ì¸ì
- âœ… `trainer_state.json` - í›ˆë ¨ ìƒíƒœ

**ì²´í¬í¬ì¸íŠ¸ ëª©ë¡**:
1. checkpoint-396 (epoch 1.0, seq_len=1536)
2. checkpoint-410 (epoch 1.0, seq_len=2048)
3. checkpoint-792 (epoch 2.0, seq_len=1536)
4. checkpoint-820 (epoch 2.0, seq_len=2048)
5. checkpoint-1188 (epoch 3.0, seq_len=1536)
6. checkpoint-1230 (epoch 3.0, seq_len=2048) â­
7. checkpoint-1584 (epoch 4.0, seq_len=1536)
8. checkpoint-1640 (epoch 4.0, seq_len=2048)
9. checkpoint-1980 (epoch 5.0, seq_len=1536) â­â­â­
10. checkpoint-2050 (epoch 5.0, seq_len=2048)

---

## ğŸ”„ í›ˆë ¨ ì¬í˜„ ì ˆì°¨

### 1. ì•„ì¹´ì´ë¸Œ ì••ì¶• í•´ì œ

```bash
tar -xzf runs_combined_experiment_full.tar.gz
```

ì´ì œ ë‹¤ìŒ êµ¬ì¡°ê°€ ìƒì„±ë©ë‹ˆë‹¤:
```
runs/combined_experiment/
â”œâ”€â”€ README.md
â”œâ”€â”€ checkpoint-396/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors  â† 713MB
â”‚   â”œâ”€â”€ tokenizer.json     â† 3.1MB
â”‚   â”œâ”€â”€ vocab.txt          â† 1.1MB
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoint-1980/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

### 2. ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©í•˜ê¸°

#### A. ì¶”ë¡  (Inference)

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline

# ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ (seq_len=1536)
checkpoint = "runs/combined_experiment/checkpoint-1980"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Fill-mask pipeline
fill = pipeline("fill-mask", model=model, tokenizer=tokenizer, device=0)

# Entity coreference ì˜ˆì¸¡
text = "ì‚¼ì„±ì „ìëŠ” ë°˜ë„ì²´ ê¸°ì—…ì´ë‹¤. [MASK]ëŠ” ê¸€ë¡œë²Œ ì‹œì¥ì„ ì„ ë„í•œë‹¤."
results = fill(text, top_k=5)

for i, pred in enumerate(results, 1):
    print(f"{i}. {pred['token_str']}: {pred['score']:.4f}")
```

**ì˜ˆìƒ ê²°ê³¼**:
```
1. ì‚¼ì„±ì „ì: 0.6675  â† Real@1 = 66.75%
2. ì‚¼ì„±: 0.1123
3. íšŒì‚¬: 0.0542
...
```

#### B. Fine-tuning ê³„ì†í•˜ê¸°

```bash
# checkpoint-1230ì—ì„œ ê³„ì† í•™ìŠµ (seq_len=2048)
python scripts/run_entity_coref_finetune.py \
  --checkpoint runs/combined_experiment/checkpoint-1230 \
  --dataset prepared_datasets/entity_coref_v2_2048 \
  --epochs 3 \
  --batch-size 8 \
  --lr 1e-5 \
  --output-dir runs/continued_training
```

#### C. í‰ê°€ (Evaluation)

```bash
# Real Coref í‰ê°€
python scripts/reevaluate_real_coref.py \
  --checkpoint runs/combined_experiment/checkpoint-1980 \
  --lambada-limit 600 \
  --coref-limit 1600 \
  --output eval_results.json
```

**ì˜ˆìƒ ê²°ê³¼** (checkpoint-1980):
```json
{
  "lambada_top1": 0.37,
  "real1": 0.6675,
  "real5": 0.8142
}
```

---

## âœ… ì¬í˜„ ê°€ëŠ¥ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ íŒŒì¼ í™•ì¸

```bash
# 1. ëª¨ë¸ ê°€ì¤‘ì¹˜ í™•ì¸ (713MB Ã— 10ê°œ)
ls -lh runs/combined_experiment/checkpoint-*/model.safetensors
# ì˜ˆìƒ: 10ê°œ íŒŒì¼, ê° 713MB

# 2. ì„¤ì • íŒŒì¼ í™•ì¸
ls runs/combined_experiment/checkpoint-*/config.json
# ì˜ˆìƒ: 10ê°œ íŒŒì¼

# 3. í† í¬ë‚˜ì´ì € í™•ì¸ (3.1MB Ã— 10ê°œ)
ls -lh runs/combined_experiment/checkpoint-*/tokenizer.json
# ì˜ˆìƒ: 10ê°œ íŒŒì¼, ê° 3.1MB

# 4. ì–´íœ˜ ì‚¬ì „ í™•ì¸ (1.1MB Ã— 10ê°œ)
ls -lh runs/combined_experiment/checkpoint-*/vocab.txt
# ì˜ˆìƒ: 10ê°œ íŒŒì¼, ê° 1.1MB
```

### ì¬í˜„ í…ŒìŠ¤íŠ¸

```python
# ê°„ë‹¨í•œ ë¡œë“œ í…ŒìŠ¤íŠ¸
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "runs/combined_experiment/checkpoint-1980"

# 1. ëª¨ë¸ ë¡œë“œ
model = AutoModelForMaskedLM.from_pretrained(checkpoint)
print(f"âœ… Model loaded: {model.config.model_type}")
print(f"   Vocab size: {model.config.vocab_size}")
print(f"   Max position: {model.config.max_position_embeddings}")

# 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
print(f"âœ… Tokenizer loaded: {len(tokenizer)} tokens")

# 3. ê°„ë‹¨í•œ ì¶”ë¡ 
text = "ëŒ€í•œë¯¼êµ­ì€ ì•„ì‹œì•„ì— ìˆë‹¤. [MASK]ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤."
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
print(f"âœ… Inference successful: output shape {outputs.logits.shape}")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
âœ… Model loaded: deberta-v2
   Vocab size: 51201
   Max position: 1536
âœ… Tokenizer loaded: 51201 tokens
âœ… Inference successful: output shape torch.Size([1, seq_len, 51201])
```

---

## ğŸ“Š ì¬í˜„ ì˜ˆìƒ ì„±ëŠ¥

### checkpoint-1980 (ìµœê³  ì„±ëŠ¥)

```
Real@1:      66.75%  (1600 samples)
Real@5:      81.42%  (1600 samples)
LAMBADA@1:   37.00%  (600 samples)
```

### checkpoint-1230 (seq_len=2048)

```
Real@1:      65.13%  (3200 samples)
Real@5:      81.38%  (3200 samples)
LAMBADA@1:   35.00%  (600 samples)
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### ë¹ ì§„ íŒŒì¼ (ì˜ë„ì  ì œì™¸)

âŒ **optimizer.pt** (1.4GB Ã— 10ê°œ = 14GB)
- í›ˆë ¨ ì¬ê°œìš© ì˜µí‹°ë§ˆì´ì € ìƒíƒœ
- ì¶”ë¡ ì—ëŠ” ë¶ˆí•„ìš”
- Fine-tuning ì‹œ ìƒˆë¡œ ì´ˆê¸°í™”ë¨

âŒ **rng_state.pth** (15KB Ã— 10ê°œ)
- ëœë¤ ì‹œë“œ ìƒíƒœ
- ì™„ì „ ë™ì¼ ì¬í˜„ì—ë§Œ í•„ìš”
- ê±°ì˜ ì˜í–¥ ì—†ìŒ

### ì¬í˜„ ì‹œ ìœ ì˜ì‚¬í•­

1. **ë°ì´í„°ì…‹ í•„ìš”**:
   - Fine-tuning ê³„ì†í•˜ë ¤ë©´ `prepared_datasets/entity_coref_*` í•„ìš”
   - ìƒì„± ìŠ¤í¬ë¦½íŠ¸: `scripts/prepare_entity_coref_dataset.py`
   - ë˜ëŠ” V2: `scripts/prepare_entity_coref_v2.py` (ê³ í’ˆì§ˆ)

2. **í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­**:
   - GPU ë©”ëª¨ë¦¬: seq_len=1536 â†’ 24GB, seq_len=2048 â†’ 40GB
   - ì¶”ë¡ ë§Œ: 16GB GPUë¡œ ê°€ëŠ¥

3. **Python í™˜ê²½**:
   ```bash
   pip install transformers torch datasets kiwipiepy
   ```

---

## ğŸ“ ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### Git Repository

```bash
git clone https://github.com/hong-seongmin/test_autocoref.git
cd test_autocoref
```

**í¬í•¨ëœ ìŠ¤í¬ë¦½íŠ¸ë“¤** (ì´ë¯¸ commitë¨):
- `scripts/prepare_entity_coref_dataset.py` - ë°ì´í„°ì…‹ ìƒì„±
- `scripts/prepare_entity_coref_v2.py` - ê³ í’ˆì§ˆ í•„í„°ë§
- `scripts/run_combined_experiment.py` - í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
- `scripts/run_entity_coref_finetune.py` - Fine-tuning
- `scripts/reevaluate_real_coref.py` - í‰ê°€

### ë¬¸ì„œ

- `DATASET_README.md` - ë°ì´í„°ì…‹ ìƒì„¸ ì„¤ëª…
- `QUICK_START.md` - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- `runs/combined_experiment/README.md` - ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¬¸ì„œ

---

## âœ… ê²°ë¡ 

**í›ˆë ¨ ì¬í˜„ ê°€ëŠ¥ ì—¬ë¶€**: **ì˜ˆ (Yes)** âœ…

ì´ ì•„ì¹´ì´ë¸ŒëŠ” ë‹¤ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤:

1. âœ… **ì¶”ë¡  ì¬í˜„**: ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¦‰ì‹œ ì¶”ë¡  ê°€ëŠ¥
2. âœ… **ì„±ëŠ¥ ì¬í˜„**: Real@1=66.75% (checkpoint-1980) ê²€ì¦ ê°€ëŠ¥
3. âœ… **Fine-tuning**: ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ê³„ì† ê°€ëŠ¥
4. âœ… **í‰ê°€ ì¬í˜„**: í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë™ì¼ ë©”íŠ¸ë¦­ ì¸¡ì • ê°€ëŠ¥

**ë¹ ì§„ ê²ƒ**:
- âŒ optimizer.pt (ì¶”ë¡ ì— ë¶ˆí•„ìš”, fine-tuning ì‹œ ìƒˆë¡œ ìƒì„±)
- âŒ ì›ë³¸ ë°ì´í„°ì…‹ (ìŠ¤í¬ë¦½íŠ¸ë¡œ ì¬ìƒì„± ê°€ëŠ¥)

---

**Last Updated**: 2025-10-11
**Archive Size**: 6.5GB
**Total Files**: 95
**Checkpoints**: 10

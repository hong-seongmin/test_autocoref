"""
í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
- KLUE MRC, Wikipedia, Naver News í•„í„°ë§
- WikipediaëŠ” \n\në¡œ ë‹¨ë½ ë¶„ë¦¬
- ìµœì  ëŒ€ëª…ì‚¬ ë°€ë„ 1.5-3.0% ëª©í‘œ
- Entity:Pronoun ë¹„ìœ¨ ê· í˜• ì²´í¬
- long_sequence_automl.pyì˜ --dataset-choiceì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ë©€í‹°í”„ë¡œì„¸ì‹± + 2ë‹¨ê³„ í•„í„°ë§ìœ¼ë¡œ 50~100ë°° ì†ë„ í–¥ìƒ
"""

import os
import sys
import gc
from pathlib import Path
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
from typing import Dict, Any, List
from multiprocessing import Pool, cpu_count
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ============================================================================
# Phase 1 & 2: Quick Prefilter (ë¹ ë¥¸ ì‚¬ì „ í•„í„°)
# ============================================================================

def quick_prefilter_klue_mrc(text: str) -> bool:
    """ë¹ ë¥¸ ì‚¬ì „ í•„í„° - KLUE MRCìš© (Kiwi ì‚¬ìš© ì•ˆ í•¨)"""
    # ê¸¸ì´ ì²´í¬ (800-3000ì)
    if len(text) < 800 or len(text) > 3000:
        return False

    # ê°„ë‹¨í•œ ëŒ€ëª…ì‚¬ íŒ¨í„´ ì²´í¬
    simple_pronouns = ['ê·¸ëŠ”', 'ê·¸ê°€', 'ê·¸ì˜', 'ê·¸ë…€', 'ì´ëŠ”', 'ì´ê°€', 'ì €ëŠ”', 'ê·¸ê²ƒ']
    pronoun_count = sum(text.count(p) for p in simple_pronouns)

    if pronoun_count < 3:  # ìµœì†Œ 3ê°œ ì´ìƒ (ìƒí–¥)
        return False

    # íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ë¹„ìœ¨ ì²´í¬
    special_ratio = sum(c in '[](){}#@*' for c in text) / len(text)
    if special_ratio > 0.05:  # 5% ì´ìƒì´ë©´ ë…¸ì´ì¦ˆ
        return False

    return True


def quick_prefilter_wikipedia_paragraph(text: str, seq_len: int = 2048) -> bool:
    """ë¹ ë¥¸ ì‚¬ì „ í•„í„° - Wikipedia ë‹¨ë½ìš© (ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ìµœì í™”)"""
    # ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ëª©í‘œ ë²”ìœ„ ì„¤ì •
    if seq_len <= 1536:
        min_chars, max_chars = 500, 1500
    else:  # 2048
        min_chars, max_chars = 700, 2000

    # ê¸¸ì´ ì²´í¬ (ë™ì )
    if len(text) < min_chars or len(text) > max_chars:
        return False

    # ì œëª©/ëª©ë¡ ì œì™¸
    if text.strip().startswith(('==', '##', '*', '-', '|')):
        return False

    # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ë“¤ë§Œ ìˆìœ¼ë©´ ì œì™¸
    if text.count('.') < 2:  # ìµœì†Œ 2ë¬¸ì¥ ì´ìƒ
        return False

    # ê°„ë‹¨í•œ ëŒ€ëª…ì‚¬ íŒ¨í„´ ì²´í¬
    simple_pronouns = ['ê·¸ëŠ”', 'ê·¸ê°€', 'ê·¸ì˜', 'ê·¸ë…€', 'ì´ëŠ”', 'ì´ê²ƒ', 'ê·¸ê²ƒ']
    pronoun_count = sum(text.count(p) for p in simple_pronouns)

    if pronoun_count < 2:  # ìµœì†Œ 2ê°œ ì´ìƒ
        return False

    # ë¦¬ìŠ¤íŠ¸/í‘œ í˜•ì‹ ì œì™¸
    if text.count('\n') > len(text) / 50:  # ë„ˆë¬´ ë§ì€ ì¤„ë°”ê¿ˆ
        return False

    return True


def quick_prefilter_naver_news(text: str) -> bool:
    """ë¹ ë¥¸ ì‚¬ì „ í•„í„° - Naver Newsìš© (ì—„ê²©)"""
    # ê¸¸ì´ ì²´í¬ (1000ì ì´ìƒë§Œ)
    if len(text) < 1000:
        return False

    # ë§¤ìš° ì—„ê²©í•œ ëŒ€ëª…ì‚¬ ì²´í¬
    simple_pronouns = ['ê·¸ëŠ”', 'ê·¸ê°€', 'ê·¸ì˜', 'ê·¸ë…€ëŠ”', 'ê·¸ë…€ê°€', 'ê·¸ê²ƒì€']
    pronoun_count = sum(text.count(p) for p in simple_pronouns)

    if pronoun_count < 5:  # ìµœì†Œ 5ê°œ ì´ìƒ
        return False

    return True


# ============================================================================
# Phase 3: Kiwi Multiprocessing
# ============================================================================

def analyze_coref_quality_worker(text: str) -> Dict[str, Any]:
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ìš© í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ê°€ ìì²´ Kiwi ìƒì„±)"""
    if not text or len(text) < 50:
        return None

    # í”„ë¡œì„¸ìŠ¤ë³„ Kiwi ì¸ìŠ¤í„´ìŠ¤ (ìºì‹±)
    if not hasattr(analyze_coref_quality_worker, '_kiwi'):
        analyze_coref_quality_worker._kiwi = Kiwi()

    kiwi = analyze_coref_quality_worker._kiwi

    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì• 1500ìë§Œ (ë‹¨ë½ì´ë¯€ë¡œ ì¶©ë¶„)
    text = text[:1500]

    try:
        tokens = kiwi.tokenize(text)
    except:
        return None

    pronouns = []
    entities = []
    verbs = []
    meaningful_words = []

    for token in tokens:
        if token.tag == 'NP':  # ëŒ€ëª…ì‚¬
            pronouns.append(token.form)
        elif token.tag in ['NNG', 'NNP', 'NNB']:  # ëª…ì‚¬ë¥˜
            if len(token.form) > 1:
                entities.append(token.form)
        elif token.tag.startswith(('VV', 'VA')):  # ë™ì‚¬/í˜•ìš©ì‚¬
            verbs.append(token.form)

        if token.tag.startswith(('N', 'V', 'M', 'VA', 'VV')):
            meaningful_words.append(token.form)

    total_words = len(meaningful_words)
    if total_words == 0:
        return None

    # Entity:Pronoun ë¹„ìœ¨ ê³„ì‚°
    pronoun_entity_ratio = len(pronouns) / max(1, len(entities))

    return {
        'pronoun_count': len(pronouns),
        'entity_count': len(entities),
        'verb_count': len(verbs),
        'total_words': total_words,
        'pronoun_density': len(pronouns) / max(1, total_words),
        'entity_density': len(entities) / max(1, total_words),
        'unique_pronouns': len(set(pronouns)),
        'unique_entities': len(set(entities)),
        'pronoun_entity_ratio': pronoun_entity_ratio,
    }


def batch_analyze_parallel(texts: List[str], num_workers: int = 20, chunksize: int = 50) -> List[Dict[str, Any]]:
    """ë³‘ë ¬ ë°°ì¹˜ ë¶„ì„"""
    with Pool(processes=num_workers) as pool:
        results = pool.map(analyze_coref_quality_worker, texts, chunksize=chunksize)
    return results


# ============================================================================
# Improved Filtering Functions
# ============================================================================

def filter_coref_quality(quality: Dict[str, Any], dataset_type: str) -> bool:
    """
    Coreference ìµœì  í’ˆì§ˆ í•„í„°ë§
    ëª©í‘œ: ëŒ€ëª…ì‚¬ ë°€ë„ 1.0-4.0%
    ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì¡°ì •: Pronoun:Entity ë¹„ìœ¨ 0.01-0.1 (ì‹¤ì œ ë°ì´í„°ëŠ” 1-5%)
    """
    if quality is None:
        return False

    # ê³µí†µ ê¸°ì¤€ (ì™„í™”)
    if quality['pronoun_count'] < 2:  # ìµœì†Œ 2ê°œ ëŒ€ëª…ì‚¬ (3â†’2 ì™„í™”)
        return False

    if quality['entity_count'] < 5:  # ìµœì†Œ 5ê°œ entity
        return False

    # í•µì‹¬: ëŒ€ëª…ì‚¬ ë°€ë„ 1.0-4.0% (1.5â†’1.0 ì™„í™”)
    pronoun_density = quality['pronoun_density'] * 100  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
    if pronoun_density < 1.0 or pronoun_density > 5.0:  # ìƒí•œë„ ì™„í™”
        return False

    # Entity:Pronoun ë¹„ìœ¨ ì²´í¬ (0.01-0.15, ì¦‰ 1-15%)
    # ì‹¤ì œ ë°ì´í„°: í‰ê·  0.024 (2.4%), ì¤‘ì•™ê°’ 0.014 (1.4%)
    # ë„ˆë¬´ ë‚®ìœ¼ë©´ ëŒ€ëª…ì‚¬ ì—†ìŒ, ë„ˆë¬´ ë†’ìœ¼ë©´ entity ë¶€ì¡±
    if quality['pronoun_entity_ratio'] < 0.01 or quality['pronoun_entity_ratio'] > 0.15:
        return False

    # ë°ì´í„°ì…‹ë³„ ì¶”ê°€ ê¸°ì¤€
    if dataset_type == 'klue_mrc':
        if quality['entity_count'] < 8:  # KLUEëŠ” entity ë§ìŒ
            return False
        if quality['unique_pronouns'] < 2:
            return False

    elif dataset_type == 'wikipedia':
        # WikipediaëŠ” entityê°€ ë§¤ìš° ë§ìŒ (í‰ê·  100ê°œ)
        # ë„ˆë¬´ ë†’ì€ ê¸°ì¤€ì€ ë¹„í˜„ì‹¤ì 
        if quality['unique_pronouns'] < 2:  # unique_entities ì œê±°
            return False

    elif dataset_type == 'naver_news':
        # Naver News: ì ì ˆí•œ ê¸°ì¤€ (15-25% í†µê³¼ìœ¨)
        # ì‹¤ì œ ë°ì´í„°: ì¤‘ì•™ê°’ 0.43%, 90% 1.32%
        if pronoun_density < 0.8:  # 0.8% ì´ìƒ (ì™„í™”)
            return False
        # ë¹„ìœ¨ ì²´í¬ ì™„í™”
        if quality['pronoun_entity_ratio'] < 0.005:  # 0.5% ì´ìƒ
            return False

    return True


# ============================================================================
# Dataset Preparation Functions
# ============================================================================

def prepare_klue_mrc_dataset(tokenizer, seq_len: int, save_dir: str, num_workers: int = 20):
    """KLUE MRC í•„í„°ë§ ë° í† í°í™” (ê°œì„  ë²„ì „)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š KLUE MRC ë°ì´í„°ì…‹ ì¤€ë¹„ (seq_len={seq_len})")
    print("=" * 80)

    # ë°ì´í„° ë¡œë“œ
    dataset = load_dataset("klue", "mrc", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")

    # 1ë‹¨ê³„: ë¹ ë¥¸ í•„í„°ë§
    print("âš¡ 1ë‹¨ê³„: ë¹ ë¥¸ ì‚¬ì „ í•„í„°ë§...")
    quick_filtered = []
    quick_filtered_texts = []

    for i, sample in enumerate(dataset):
        context = sample['context']
        if quick_prefilter_klue_mrc(context):
            quick_filtered.append(i)
            quick_filtered_texts.append(context)

    print(f"  âœ… 1ë‹¨ê³„ í†µê³¼: {len(quick_filtered)}/{len(dataset)} ({len(quick_filtered)/len(dataset)*100:.1f}%)")

    if not quick_filtered_texts:
        print("âš ï¸ 1ë‹¨ê³„ì—ì„œ ëª¨ë“  ìƒ˜í”Œì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return None

    # 2ë‹¨ê³„: Kiwi ë³‘ë ¬ ë¶„ì„
    print(f"ğŸš€ 2ë‹¨ê³„: Kiwi ë³‘ë ¬ ë¶„ì„ ({num_workers} ì›Œì»¤)...")
    qualities = batch_analyze_parallel(quick_filtered_texts, num_workers=num_workers, chunksize=100)

    # 3ë‹¨ê³„: ìµœì¢… í•„í„°ë§ (ê°œì„ ëœ ê¸°ì¤€)
    print("ğŸ” 3ë‹¨ê³„: ìµœì¢… í•„í„°ë§ (ëª©í‘œ: ëŒ€ëª…ì‚¬ ë°€ë„ 1.5-3.0%)...")
    filtered_texts = []
    pronoun_densities = []

    for text, quality in zip(quick_filtered_texts, qualities):
        if filter_coref_quality(quality, 'klue_mrc'):
            filtered_texts.append(text)
            pronoun_densities.append(quality['pronoun_density'] * 100)

    print(f"âœ… ìµœì¢… í•„í„°ë§ ì™„ë£Œ: {len(filtered_texts)} ìƒ˜í”Œ")
    if pronoun_densities:
        import numpy as np
        print(f"ğŸ“Š ëŒ€ëª…ì‚¬ ë°€ë„ í‰ê· : {np.mean(pronoun_densities):.2f}% (ë²”ìœ„: {np.min(pronoun_densities):.2f}-{np.max(pronoun_densities):.2f}%)")

    if not filtered_texts:
        print("âš ï¸ í•„í„°ë§ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # Datasetìœ¼ë¡œ ë³€í™˜
    text_dataset = Dataset.from_dict({"text": filtered_texts})

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del quick_filtered_texts, qualities
    gc.collect()

    # í† í°í™” (ìµœì í™”)
    print(f"ğŸ”¤ í† í°í™” ì¤‘ (seq_len={seq_len}, num_proc={num_workers})...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=seq_len,
        )

    tokenized_dataset = text_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=2000,
        remove_columns=["text"],
        num_proc=num_workers,
        desc="í† í°í™” ì§„í–‰"
    )

    # ì €ì¥
    save_path = Path(save_dir) / f"klue_mrc_filtered_{seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)

    tokenized_dataset.save_to_disk(str(save_path))

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}")

    return str(save_path)


def prepare_wikipedia_dataset(tokenizer, seq_len: int, save_dir: str, max_samples: int = 50000, num_workers: int = 20):
    """
    Wikipedia í•„í„°ë§ ë° í† í°í™” (ê°œì„  ë²„ì „)
    - \n\në¡œ ë‹¨ë½ ë¶„ë¦¬
    - ê° ë‹¨ë½ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€
    """

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Wikipedia ë°ì´í„°ì…‹ ì¤€ë¹„ (seq_len={seq_len})")
    print(f"ğŸ”§ ê°œì„ : \\n\\në¡œ ë‹¨ë½ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬")
    print("=" * 80)

    # ë°ì´í„° ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë°)
    dataset = load_dataset("wikimedia/wikipedia", "20231101.ko", split="train", streaming=True)

    # í•„í„°ë§ (ì¡°ê¸° ì¢…ë£Œ í¬í•¨)
    print(f"ğŸ” í•„í„°ë§ ì¤‘ (ìµœëŒ€ {max_samples}ê°œ ë‹¨ë½, ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”)...")

    batch_size = 500
    batch_paragraphs = []
    filtered_texts = []
    scanned_docs = 0
    scanned_paragraphs = 0

    # ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ëª©í‘œ ë²”ìœ„ ì„¤ì •
    if seq_len <= 1536:
        min_chars, max_chars = 500, 1500
    else:  # 2048
        min_chars, max_chars = 700, 2000

    for sample in dataset:
        text = sample['text']
        scanned_docs += 1

        # â˜… í•µì‹¬ ê°œì„  1: \n\në¡œ ë‹¨ë½ ë¶„ë¦¬
        raw_paragraphs = text.split('\n\n')

        # â˜… í•µì‹¬ ê°œì„  2: ì§§ì€ ë‹¨ë½ì„ ë‹¤ìŒ ë‹¨ë½ê³¼ ë³‘í•©í•˜ì—¬ ë§¥ë½ ìœ ì§€
        merged_paragraphs = []
        current_merged = ""

        for para in raw_paragraphs:
            para = para.strip()
            if not para:
                continue

            # ë³‘í•© ë¡œì§: í˜„ì¬ ëˆ„ì  í…ìŠ¤íŠ¸ê°€ ìµœì†Œ ê¸¸ì´ ë¯¸ë§Œì´ë©´ ê³„ì† ë³‘í•©
            if not current_merged:
                current_merged = para
            elif len(current_merged) < min_chars:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë³‘í•© (ë§¥ë½ ìœ ì§€)
                current_merged += "\n\n" + para
            else:
                # ì ì ˆí•œ ê¸¸ì´ë©´ ì™„ì„±ëœ ë‹¨ë½ìœ¼ë¡œ ì¶”ê°€
                # ë‹¨, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì²´í¬
                if len(current_merged) <= max_chars:
                    merged_paragraphs.append(current_merged)
                current_merged = para

        # ë§ˆì§€ë§‰ ë‚¨ì€ ë‹¨ë½ ì²˜ë¦¬
        if current_merged and len(current_merged) >= min_chars and len(current_merged) <= max_chars:
            merged_paragraphs.append(current_merged)

        # ë³‘í•©ëœ ë‹¨ë½ë“¤ ì²˜ë¦¬
        for para in merged_paragraphs:
            scanned_paragraphs += 1

            # 1ë‹¨ê³„: ë¹ ë¥¸ í•„í„° (ë‹¨ë½ìš©, ì‹œí€€ìŠ¤ ê¸¸ì´ ì „ë‹¬)
            if quick_prefilter_wikipedia_paragraph(para, seq_len):
                batch_paragraphs.append(para)

            # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ëª©í‘œ ë„ë‹¬ ì‹œ ì²˜ë¦¬
            if len(batch_paragraphs) >= batch_size or len(filtered_texts) >= max_samples:
                if batch_paragraphs:
                    # 2ë‹¨ê³„: ë³‘ë ¬ Kiwi ë¶„ì„
                    qualities = batch_analyze_parallel(batch_paragraphs, num_workers=num_workers, chunksize=50)

                    # 3ë‹¨ê³„: ìµœì¢… í•„í„°ë§
                    for p, q in zip(batch_paragraphs, qualities):
                        if filter_coref_quality(q, 'wikipedia'):
                            filtered_texts.append(p)

                            # ì¡°ê¸° ì¢…ë£Œ
                            if len(filtered_texts) >= max_samples:
                                print(f"âœ… ëª©í‘œ ë‹¬ì„±! {len(filtered_texts)} ë‹¨ë½ ìˆ˜ì§‘")
                                print(f"   (ìŠ¤ìº”: ë¬¸ì„œ {scanned_docs}ê°œ, ë‹¨ë½ {scanned_paragraphs}ê°œ)")
                                break

                    batch_paragraphs = []

                if len(filtered_texts) >= max_samples:
                    break

        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if scanned_docs % 1000 == 0:
            print(f"  ë¬¸ì„œ: {scanned_docs}, ë‹¨ë½: {scanned_paragraphs}, ìˆ˜ì§‘: {len(filtered_texts)}/{max_samples}")

        if len(filtered_texts) >= max_samples:
            break

    # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
    if batch_paragraphs and len(filtered_texts) < max_samples:
        qualities = batch_analyze_parallel(batch_paragraphs, num_workers=num_workers)
        for p, q in zip(batch_paragraphs, qualities):
            if filter_coref_quality(q, 'wikipedia'):
                filtered_texts.append(p)
                if len(filtered_texts) >= max_samples:
                    break

    print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_texts)} ë‹¨ë½")
    print(f"   (ìŠ¤ìº”: ë¬¸ì„œ {scanned_docs}ê°œ, ë‹¨ë½ {scanned_paragraphs}ê°œ)")

    if not filtered_texts:
        print("âš ï¸ í•„í„°ë§ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # í’ˆì§ˆ í†µê³„
    print("ğŸ“Š í’ˆì§ˆ ê²€ì¦ ì¤‘...")
    sample_qualities = batch_analyze_parallel(filtered_texts[:100], num_workers=num_workers)
    sample_densities = [q['pronoun_density'] * 100 for q in sample_qualities if q]
    if sample_densities:
        import numpy as np
        print(f"ğŸ“Š ëŒ€ëª…ì‚¬ ë°€ë„ (ìƒ˜í”Œ 100ê°œ): í‰ê·  {np.mean(sample_densities):.2f}% (ë²”ìœ„: {np.min(sample_densities):.2f}-{np.max(sample_densities):.2f}%)")

    # Datasetìœ¼ë¡œ ë³€í™˜
    text_dataset = Dataset.from_dict({"text": filtered_texts})

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del filtered_texts
    gc.collect()

    # í† í°í™”
    print(f"ğŸ”¤ í† í°í™” ì¤‘ (seq_len={seq_len}, num_proc={num_workers})...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=seq_len,
        )

    tokenized_dataset = text_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=2000,
        remove_columns=["text"],
        num_proc=num_workers,
        desc="í† í°í™” ì§„í–‰"
    )

    # ì €ì¥
    save_path = Path(save_dir) / f"wikipedia_filtered_{seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)

    tokenized_dataset.save_to_disk(str(save_path))

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}")

    return str(save_path)


def prepare_naver_news_dataset(tokenizer, seq_len: int, save_dir: str, num_workers: int = 20):
    """Naver News í•„í„°ë§ ë° í† í°í™” (ê°œì„  ë²„ì „ - ì ì ˆí•œ ê¸°ì¤€)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Naver News ë°ì´í„°ì…‹ ì¤€ë¹„ (seq_len={seq_len})")
    print("=" * 80)

    # ë°ì´í„° ë¡œë“œ
    dataset = load_dataset("daekeun-ml/naver-news-summarization-ko", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")

    # 1ë‹¨ê³„: ë¹ ë¥¸ í•„í„°ë§ (ì™„í™”)
    print("âš¡ 1ë‹¨ê³„: ë¹ ë¥¸ ì‚¬ì „ í•„í„°ë§ (ì™„í™”ëœ ê¸°ì¤€)...")
    quick_filtered_texts = []

    for sample in dataset:
        document = sample['document']
        # ì™„í™”ëœ ê¸¸ì´ ê¸°ì¤€ (800ì ì´ìƒ)
        if len(document) >= 800:
            quick_filtered_texts.append(document)

    print(f"  âœ… 1ë‹¨ê³„ í†µê³¼: {len(quick_filtered_texts)}/{len(dataset)} ({len(quick_filtered_texts)/len(dataset)*100:.1f}%)")

    if not quick_filtered_texts:
        print("âš ï¸ 1ë‹¨ê³„ì—ì„œ ëª¨ë“  ìƒ˜í”Œì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return None

    # 2ë‹¨ê³„: Kiwi ë³‘ë ¬ ë¶„ì„
    print(f"ğŸš€ 2ë‹¨ê³„: Kiwi ë³‘ë ¬ ë¶„ì„ ({num_workers} ì›Œì»¤)...")
    qualities = batch_analyze_parallel(quick_filtered_texts, num_workers=num_workers)

    # 3ë‹¨ê³„: ìµœì¢… í•„í„°ë§ (ì ì ˆí•œ ê¸°ì¤€: 15-25% í†µê³¼ìœ¨ ëª©í‘œ)
    print("ğŸ” 3ë‹¨ê³„: ìµœì¢… í•„í„°ë§ (ì ì ˆí•œ ê¸°ì¤€: ëŒ€ëª…ì‚¬ â‰¥2, ë°€ë„ â‰¥0.8%)...")
    filtered_texts = []
    pronoun_densities = []

    for text, quality in zip(quick_filtered_texts, qualities):
        if filter_coref_quality(quality, 'naver_news'):
            filtered_texts.append(text)
            pronoun_densities.append(quality['pronoun_density'] * 100)

    print(f"âœ… ìµœì¢… í•„í„°ë§ ì™„ë£Œ: {len(filtered_texts)} ìƒ˜í”Œ")
    if pronoun_densities:
        import numpy as np
        print(f"ğŸ“Š ëŒ€ëª…ì‚¬ ë°€ë„ í‰ê· : {np.mean(pronoun_densities):.2f}% (ë²”ìœ„: {np.min(pronoun_densities):.2f}-{np.max(pronoun_densities):.2f}%)")

    if not filtered_texts:
        print(f"âš ï¸ í•„í„°ë§ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # Datasetìœ¼ë¡œ ë³€í™˜
    text_dataset = Dataset.from_dict({"text": filtered_texts})

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del quick_filtered_texts, qualities
    gc.collect()

    # í† í°í™”
    print(f"ğŸ”¤ í† í°í™” ì¤‘ (seq_len={seq_len}, num_proc={num_workers})...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=seq_len,
        )

    tokenized_dataset = text_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=2000,
        remove_columns=["text"],
        num_proc=num_workers,
        desc="í† í°í™” ì§„í–‰"
    )

    # ì €ì¥
    save_path = Path(save_dir) / f"naver_news_filtered_{seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)

    tokenized_dataset.save_to_disk(str(save_path))

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}")

    return str(save_path)


def combine_and_save_datasets(filtered_texts_dict: Dict[str, List[str]], tokenizer, seq_len: int,
                               save_dir: str, output_name: str, num_workers: int = 20):
    """
    ì—¬ëŸ¬ ë°ì´í„°ì…‹ì˜ í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©í•˜ì—¬ ì €ì¥

    Args:
        filtered_texts_dict: {dataset_name: [filtered_texts]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        tokenizer: í† í¬ë‚˜ì´ì €
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        output_name: ì¶œë ¥ ë°ì´í„°ì…‹ ì´ë¦„
        num_workers: ì›Œì»¤ ìˆ˜
    """
    print("\n" + "=" * 80)
    print(f"ğŸ”— ë°ì´í„°ì…‹ ê²°í•©: {output_name} (seq_len={seq_len})")
    print("=" * 80)

    # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
    combined_texts = []
    for dataset_name, texts in filtered_texts_dict.items():
        print(f"  + {dataset_name}: {len(texts)}ê°œ")
        combined_texts.extend(texts)

    print(f"ğŸ“Š ì´ {len(combined_texts)}ê°œ ìƒ˜í”Œ ê²°í•©")

    if not combined_texts:
        print("âš ï¸ ê²°í•©í•  ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # Datasetìœ¼ë¡œ ë³€í™˜
    text_dataset = Dataset.from_dict({"text": combined_texts})

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del combined_texts
    gc.collect()

    # í† í°í™”
    print(f"ğŸ”¤ í† í°í™” ì¤‘ (seq_len={seq_len}, num_proc={num_workers})...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=seq_len,
        )

    tokenized_dataset = text_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=2000,
        remove_columns=["text"],
        num_proc=num_workers,
        desc="í† í°í™” ì§„í–‰"
    )

    # ì €ì¥
    save_path = Path(save_dir) / f"{output_name}_{seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)

    tokenized_dataset.save_to_disk(str(save_path))

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}")

    return str(save_path)


def main():
    parser = argparse.ArgumentParser(description="í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± (ê°œì„  ë²„ì „)")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--seq-lengths", nargs="+", type=int, default=[1536, 2048], help="ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 1536, 2048)")
    parser.add_argument("--save-dir", default="./prepared_datasets", help="ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--datasets", nargs="+", choices=["klue_mrc", "wikipedia", "naver_news", "all"],
                        default=["all"], help="ìƒì„±í•  ë°ì´í„°ì…‹ (ê¸°ë³¸: all)")
    parser.add_argument("--wiki-samples", type=int, default=50000, help="Wikipedia ìµœëŒ€ ë‹¨ë½ ìˆ˜ (ê¸°ë³¸: 50000, ì¶©ë¶„í•œ ì–‘)")
    parser.add_argument("--num-workers", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: CPU ì½”ì–´ - 4)")
    parser.add_argument("--combine-datasets", action="store_true", help="ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ ê²°í•© (ê¸°ë³¸: ê°ê° ë¶„ë¦¬)")
    parser.add_argument("--output-name", type=str, default=None, help="ì¶œë ¥ ë°ì´í„°ì…‹ ì´ë¦„ (ê¸°ë³¸: ìë™ ìƒì„±)")

    args = parser.parse_args()

    # ì›Œì»¤ ìˆ˜ ì„¤ì •
    if args.num_workers is None:
        args.num_workers = max(4, cpu_count() - 4)  # 4ê°œ ì½”ì–´ëŠ” ì‹œìŠ¤í…œìš©ìœ¼ë¡œ ë‚¨ê¹€

    print(f"âš™ï¸  ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {args.num_workers} (CPU ì½”ì–´: {cpu_count()})")

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ: {args.model}")
    tokenizer = AutoTokenizer.from_pretrained(args.model)

    # ë°ì´í„°ì…‹ ì„ íƒ
    datasets_to_create = args.datasets
    if "all" in datasets_to_create:
        datasets_to_create = ["klue_mrc", "wikipedia", "naver_news"]

    print("\n" + "=" * 80)
    print("ğŸš€ í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ (ê°œì„  ë²„ì „)")
    print("=" * 80)
    print(f"ëª¨ë¸: {args.model}")
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_lengths}")
    print(f"ì €ì¥ ê²½ë¡œ: {save_dir}")
    print(f"ìƒì„±í•  ë°ì´í„°ì…‹: {datasets_to_create}")
    print(f"\nğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print(f"  âœ… Wikipedia: \\n\\në¡œ ë‹¨ë½ ë¶„ë¦¬ (ê¸´ ë¬¸ì„œ â†’ ì ì ˆí•œ ë‹¨ë½)")
    print(f"  âœ… ëª©í‘œ ëŒ€ëª…ì‚¬ ë°€ë„: 1.5-3.0%")
    print(f"  âœ… Entity:Pronoun ë¹„ìœ¨ ì²´í¬: 15-50%")
    print(f"  âœ… 2ë‹¨ê³„ í•„í„°ë§ (ë¹ ë¥¸ ì‚¬ì „ í•„í„° + Kiwi ë¶„ì„)")
    print(f"  âœ… Kiwi ë©€í‹°í”„ë¡œì„¸ì‹± ({args.num_workers} ì›Œì»¤)")
    print(f"  âœ… í† í°í™” ë³‘ë ¬í™” ({args.num_workers} í”„ë¡œì„¸ìŠ¤)")
    print(f"ì˜ˆìƒ ì†ë„: 50~100ë°° í–¥ìƒ ğŸš€")

    # ì¶œë ¥ ì´ë¦„ ì„¤ì •
    if args.output_name is None:
        if args.combine_datasets:
            args.output_name = "combined_coref"
        # ê°œë³„ ì €ì¥ ëª¨ë“œëŠ” ê° í•¨ìˆ˜ì—ì„œ ìë™ ì´ë¦„ ì‚¬ìš©

    # ê²°í•© ëª¨ë“œ ì•ˆë‚´
    if args.combine_datasets:
        print(f"ğŸ”— ê²°í•© ëª¨ë“œ: ëª¨ë“  ë°ì´í„°ì…‹ì„ '{args.output_name}' ì´ë¦„ìœ¼ë¡œ ê²°í•©")
    else:
        if args.output_name:
            print(f"ğŸ“ ì»¤ìŠ¤í…€ ì´ë¦„: '{args.output_name}' ì‚¬ìš©")

    created_paths = {}

    import time
    overall_start = time.time()

    for seq_len in args.seq_lengths:
        print(f"\n{'='*80}")
        print(f"ğŸ¯ ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len} ì²˜ë¦¬")
        print(f"{'='*80}")

        seq_paths = []
        seq_start = time.time()

        # ê²°í•© ëª¨ë“œ: í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
        filtered_texts_dict = {} if args.combine_datasets else None

        # KLUE MRC
        if "klue_mrc" in datasets_to_create:
            try:
                start = time.time()
                if args.combine_datasets:
                    # ê²°í•© ëª¨ë“œ: í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                    print("\n" + "=" * 80)
                    print(f"ğŸ“Š KLUE MRC í•„í„°ë§ (ê²°í•©ìš©, seq_len={seq_len})")
                    print("=" * 80)
                    from datasets import load_dataset as hf_load_dataset
                    dataset = hf_load_dataset("klue", "mrc", split="train")
                    quick_filtered_texts = [s['context'] for s in dataset if quick_prefilter_klue_mrc(s['context'])]
                    print(f"âš¡ 1ë‹¨ê³„ í†µê³¼: {len(quick_filtered_texts)}/{len(dataset)}")
                    qualities = batch_analyze_parallel(quick_filtered_texts, num_workers=args.num_workers, chunksize=100)
                    filtered_texts = [t for t, q in zip(quick_filtered_texts, qualities) if filter_coref_quality(q, 'klue_mrc')]
                    print(f"âœ… ìµœì¢…: {len(filtered_texts)} ìƒ˜í”Œ")
                    filtered_texts_dict['klue_mrc'] = filtered_texts
                else:
                    # ë¶„ë¦¬ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹
                    path = prepare_klue_mrc_dataset(tokenizer, seq_len, str(save_dir), args.num_workers)
                    if path:
                        seq_paths.append(path)
                elapsed = time.time() - start
                print(f"â±ï¸  KLUE MRC ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            except Exception as e:
                print(f"âŒ KLUE MRC ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        # Wikipedia
        if "wikipedia" in datasets_to_create:
            try:
                start = time.time()
                if args.combine_datasets:
                    # ê²°í•© ëª¨ë“œ: í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                    print("\n" + "=" * 80)
                    print(f"ğŸ“Š Wikipedia í•„í„°ë§ (ê²°í•©ìš©, seq_len={seq_len})")
                    print("=" * 80)
                    # Wikipedia í•„í„°ë§ ë¡œì§ (ê°„ì†Œí™”)
                    from datasets import load_dataset as hf_load_dataset
                    wiki_dataset = hf_load_dataset("wikimedia/wikipedia", "20231101.ko", split="train", streaming=True)
                    filtered_texts = []
                    batch_paragraphs = []

                    if seq_len <= 1536:
                        min_chars, max_chars = 500, 1500
                    else:
                        min_chars, max_chars = 700, 2000

                    for i, sample in enumerate(wiki_dataset):
                        if len(filtered_texts) >= args.wiki_samples:
                            break
                        raw_paragraphs = sample['text'].split('\n\n')
                        merged_paragraphs = []
                        current_merged = ""
                        for para in raw_paragraphs:
                            para = para.strip()
                            if not para:
                                continue
                            if not current_merged:
                                current_merged = para
                            elif len(current_merged) < min_chars:
                                current_merged += "\n\n" + para
                            else:
                                if len(current_merged) <= max_chars:
                                    merged_paragraphs.append(current_merged)
                                current_merged = para
                        if current_merged and min_chars <= len(current_merged) <= max_chars:
                            merged_paragraphs.append(current_merged)

                        for para in merged_paragraphs:
                            if quick_prefilter_wikipedia_paragraph(para, seq_len):
                                batch_paragraphs.append(para)
                            if len(batch_paragraphs) >= 500:
                                qualities = batch_analyze_parallel(batch_paragraphs, num_workers=args.num_workers, chunksize=50)
                                for p, q in zip(batch_paragraphs, qualities):
                                    if filter_coref_quality(q, 'wikipedia'):
                                        filtered_texts.append(p)
                                        if len(filtered_texts) >= args.wiki_samples:
                                            break
                                batch_paragraphs = []
                                if len(filtered_texts) >= args.wiki_samples:
                                    break
                        if (i + 1) % 1000 == 0:
                            print(f"  ë¬¸ì„œ: {i+1}, ìˆ˜ì§‘: {len(filtered_texts)}/{args.wiki_samples}")

                    print(f"âœ… ìµœì¢…: {len(filtered_texts)} ìƒ˜í”Œ")
                    filtered_texts_dict['wikipedia'] = filtered_texts
                else:
                    # ë¶„ë¦¬ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹
                    path = prepare_wikipedia_dataset(tokenizer, seq_len, str(save_dir), args.wiki_samples, args.num_workers)
                    if path:
                        seq_paths.append(path)
                elapsed = time.time() - start
                print(f"â±ï¸  Wikipedia ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            except Exception as e:
                print(f"âŒ Wikipedia ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        # Naver News
        if "naver_news" in datasets_to_create:
            try:
                start = time.time()
                if args.combine_datasets:
                    # ê²°í•© ëª¨ë“œ: í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                    print("\n" + "=" * 80)
                    print(f"ğŸ“Š Naver News í•„í„°ë§ (ê²°í•©ìš©, seq_len={seq_len})")
                    print("=" * 80)
                    from datasets import load_dataset as hf_load_dataset
                    dataset = hf_load_dataset("daekeun-ml/naver-news-summarization-ko", split="train")
                    quick_filtered_texts = [s['document'] for s in dataset if len(s['document']) >= 800]
                    print(f"âš¡ 1ë‹¨ê³„ í†µê³¼: {len(quick_filtered_texts)}/{len(dataset)}")
                    qualities = batch_analyze_parallel(quick_filtered_texts, num_workers=args.num_workers)
                    filtered_texts = [t for t, q in zip(quick_filtered_texts, qualities) if filter_coref_quality(q, 'naver_news')]
                    print(f"âœ… ìµœì¢…: {len(filtered_texts)} ìƒ˜í”Œ")
                    filtered_texts_dict['naver_news'] = filtered_texts
                else:
                    # ë¶„ë¦¬ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹
                    path = prepare_naver_news_dataset(tokenizer, seq_len, str(save_dir), args.num_workers)
                    if path:
                        seq_paths.append(path)
                elapsed = time.time() - start
                print(f"â±ï¸  Naver News ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            except Exception as e:
                print(f"âŒ Naver News ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        # ê²°í•© ëª¨ë“œ: ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©í•˜ì—¬ ì €ì¥
        if args.combine_datasets and filtered_texts_dict:
            try:
                path = combine_and_save_datasets(
                    filtered_texts_dict, tokenizer, seq_len,
                    str(save_dir), args.output_name, args.num_workers
                )
                if path:
                    seq_paths.append(path)
            except Exception as e:
                print(f"âŒ ë°ì´í„°ì…‹ ê²°í•© ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        created_paths[seq_len] = seq_paths

        seq_elapsed = time.time() - seq_start
        print(f"\nâ±ï¸  ì‹œí€€ìŠ¤ {seq_len} ì´ ì²˜ë¦¬ ì‹œê°„: {seq_elapsed:.1f}ì´ˆ")

    overall_elapsed = time.time() - overall_start

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {overall_elapsed:.1f}ì´ˆ ({overall_elapsed/60:.1f}ë¶„)")

    for seq_len, paths in created_paths.items():
        print(f"\nì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}:")
        if paths:
            for path in paths:
                print(f"  - {path}")
        else:
            print(f"  (ìƒì„±ëœ ë°ì´í„°ì…‹ ì—†ìŒ)")

    # ì‚¬ìš© ë°©ë²• ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“– ì‚¬ìš© ë°©ë²•")
    print("=" * 80)

    if created_paths:
        # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ê²½ë¡œë“¤ì„ ì˜ˆì‹œë¡œ ì‚¬ìš©
        first_seq = list(created_paths.keys())[0]
        example_paths = created_paths[first_seq]

        if example_paths:
            print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”:")
            print()

            # ë‹¨ì¼ ë°ì´í„°ì…‹
            print("# 1. ë‹¨ì¼ ë°ì´í„°ì…‹ ì‚¬ìš©:")
            print(f"python -m coref_automl.long_sequence_automl \\")
            print(f"    --model {args.model} \\")
            print(f"    --seq-lengths {' '.join(map(str, args.seq_lengths))} \\")
            print(f"    --trials 10 \\")
            print(f"    --dataset-choice {example_paths[0]}")

            if len(example_paths) > 1:
                # ì—¬ëŸ¬ ë°ì´í„°ì…‹
                print("\n# 2. ì—¬ëŸ¬ ë°ì´í„°ì…‹ í•¨ê»˜ ì‚¬ìš©:")
                print(f"python -m coref_automl.long_sequence_automl \\")
                print(f"    --model {args.model} \\")
                print(f"    --seq-lengths {' '.join(map(str, args.seq_lengths))} \\")
                print(f"    --trials 10 \\")
                for path in example_paths:
                    print(f"    --dataset-choice {path} \\")
                print()

            print("\nğŸ’¡ Tip: --dataset-choiceë¥¼ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•˜ë©´ Optunaê°€ ìë™ìœ¼ë¡œ ìµœì ì˜ ë°ì´í„°ì…‹ì„ ì„ íƒí•©ë‹ˆë‹¤!")


if __name__ == "__main__":
    main()

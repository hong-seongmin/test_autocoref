#!/usr/bin/env python3
"""
Checkpoint Evaluation Script
=============================

Standalone script to evaluate any checkpoint with Real@1, Real@5, and LAMBADA@1 metrics.

Usage:
    python scripts/evaluate_checkpoint.py --checkpoint runs/mlm_v2_scratch_1536/checkpoint-655
    python scripts/evaluate_checkpoint.py --checkpoint runs/mlm_v2_scratch_2048/checkpoint-440 --seq-len 2048
"""

import argparse
import json
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import torch
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, pipeline

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from coref_automl.tune import (
    build_eval_from_lambada,
    build_real_coref_eval_set,
    eval_lambada_topk,
    eval_real_coref_combined,
)


def detect_seq_len_from_checkpoint(checkpoint_path: str) -> Optional[int]:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œí€€ìŠ¤ ê¸¸ì´ ìë™ ê°ì§€"""
    config_path = Path(checkpoint_path) / "config.json"
    if config_path.exists():
        try:
            with open(config_path, encoding="utf-8") as f:
                config = json.load(f)
            seq_len = config.get("max_position_embeddings")
            if seq_len:
                return int(seq_len)
        except Exception as e:
            print(f"âš ï¸  Config ì½ê¸° ì‹¤íŒ¨: {e}")

    try:
        config = AutoConfig.from_pretrained(checkpoint_path)
        seq_len = getattr(config, "max_position_embeddings", None)
        if seq_len:
            return int(seq_len)
    except:
        pass

    return None


def load_checkpoint(checkpoint_path: str, seq_len: int):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (DeBERTa í™•ì¥ ì„ë² ë”© ì§€ì›)"""
    print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")

    load_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    tokenizer.model_max_length = seq_len

    # DeBERTa ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
    is_deberta = "deberta" in checkpoint_path.lower() or seq_len > 512

    if is_deberta and seq_len > 512:
        print("   DeBERTa with extended embeddings detected...")
        print("   Loading with ignore_mismatched_sizes=True...")

        model = AutoModelForMaskedLM.from_pretrained(checkpoint_path, ignore_mismatched_sizes=True)

        # rel_embeddings ìˆ˜ë™ ë¡œë“œ
        from safetensors import safe_open
        checkpoint_file = Path(checkpoint_path) / "model.safetensors"
        if checkpoint_file.exists():
            with safe_open(str(checkpoint_file), framework='pt', device='cpu') as f:
                if 'deberta.encoder.rel_embeddings.weight' in f.keys():
                    rel_embed_weight = f.get_tensor('deberta.encoder.rel_embeddings.weight')
                    print(f"   rel_embeddings ìˆ˜ë™ ë¡œë“œ: {rel_embed_weight.shape}")

                    new_size, embed_dim = rel_embed_weight.shape
                    new_rel = torch.nn.Embedding(new_size, embed_dim)
                    new_rel.weight.data = rel_embed_weight.clone()
                    model.deberta.encoder.rel_embeddings = new_rel.to(model.device)

        model.config.max_position_embeddings = seq_len
    else:
        model = AutoModelForMaskedLM.from_pretrained(checkpoint_path)

    load_elapsed = time.time() - load_start
    param_count = sum(p.numel() for p in model.parameters()) / 1e6

    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_elapsed:.1f}ì´ˆ)")
    print(f"   íŒŒë¼ë¯¸í„°: {param_count:.1f}M")
    print(f"   Tokenizer max_length: {tokenizer.model_max_length}")

    return model, tokenizer


def run_evaluation(model, tokenizer, seq_len: int):
    """Real@1, Real@5, LAMBADA@1 í‰ê°€ ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print("ğŸ“Š í‰ê°€ ì‹œì‘")
    print("=" * 80)

    eval_start = time.time()

    # Fill-mask pipeline
    device = 0 if torch.cuda.is_available() else -1
    fill = pipeline("fill-mask", model=model, tokenizer=tokenizer, device=device)
    mask_token = tokenizer.mask_token or "[MASK]"

    # 1. LAMBADA í‰ê°€
    print("\nğŸ“– [1/3] LAMBADA í‰ê°€ (600 ìƒ˜í”Œ)...")
    lbd_start = time.time()
    eval_lbd = build_eval_from_lambada(limit=600, seed=42)
    lbd_t1 = eval_lambada_topk(
        fill,
        eval_lbd,
        mask_token=mask_token,
        k=1,
        batch_size=64,
        seq_len=seq_len,
    )
    lbd_elapsed = time.time() - lbd_start
    print(f"   âœ“ LAMBADA@1 = {lbd_t1:.4f} ({lbd_t1*100:.2f}%) - {lbd_elapsed:.1f}ì´ˆ")

    # 2. Real Coref í‰ê°€ ì„¸íŠ¸ êµ¬ì¶•
    print("\nğŸ”— [2/3] Real Coref í‰ê°€ ì„¸íŠ¸ êµ¬ì¶• (1600 ìƒ˜í”Œ)...")
    coref_build_start = time.time()
    eval_coref = build_real_coref_eval_set(
        limit=1600,
        seed=999,
        max_seq_len=seq_len
    )
    coref_build_elapsed = time.time() - coref_build_start
    actual_samples = len(eval_coref)
    print(f"   âœ“ {actual_samples} ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ ({coref_build_elapsed:.1f}ì´ˆ)")

    # 3. Real@1 & Real@5 ê³„ì‚° (ìµœì í™”: í•œ ë²ˆì— ê³„ì‚°)
    print("\nğŸ”— [3/3] Real@1 & Real@5 ê³„ì‚° (ìµœì í™”: í•œ ë²ˆì˜ ì¶”ë¡ ìœ¼ë¡œ ê³„ì‚°)...")
    real_start = time.time()
    real1, real5 = eval_real_coref_combined(
        fill,
        eval_coref,
        mask_token=mask_token,
        batch_size=64,
        seq_len=seq_len,
    )
    real_elapsed = time.time() - real_start
    print(f"   âœ“ Real@1 = {real1:.4f} ({real1*100:.2f}%)")
    print(f"   âœ“ Real@5 = {real5:.4f} ({real5*100:.2f}%)")
    print(f"   âœ“ ì†Œìš” ì‹œê°„: {real_elapsed:.1f}ì´ˆ (ìµœì í™”ë¡œ ì•½ 50% ë‹¨ì¶•)")

    eval_elapsed = time.time() - eval_start

    # ê²°ê³¼ ë°˜í™˜
    results = {
        "lambada_top1": lbd_t1,
        "real1": real1,
        "real5": real5,
        "coref_samples": actual_samples,
        "eval_time_seconds": eval_elapsed,
        "timings": {
            "lambada": lbd_elapsed,
            "coref_build": coref_build_elapsed,
            "real_combined": real_elapsed,  # Real@1 + Real@5 ë™ì‹œ ê³„ì‚°
        }
    }

    return results


def print_results(checkpoint_path: str, seq_len: int, results: dict):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    print(f"ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
    print(f"Coref ìƒ˜í”Œ: {results['coref_samples']}")
    print()
    print(f"LAMBADA@1: {results['lambada_top1']:.4f} ({results['lambada_top1']*100:.2f}%)")
    print(f"Real@1:    {results['real1']:.4f} ({results['real1']*100:.2f}%)")
    print(f"Real@5:    {results['real5']:.4f} ({results['real5']*100:.2f}%)")
    print()
    print(f"í‰ê°€ ì‹œê°„: {results['eval_time_seconds']:.1f}ì´ˆ")

    # ì´ì „ ë² ìŠ¤íŠ¸ì™€ ë¹„êµ
    print("\n" + "â”€" * 80)
    print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ")
    print("â”€" * 80)
    print("ì´ì „ ë² ìŠ¤íŠ¸ (checkpoint-1600, Entity+MLM, seq_len=2048):")
    print("  - Real@1: 67.78%")
    print("  - Real@5: 82.44%")
    print()
    print(f"í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ (seq_len={seq_len}):")
    print(f"  - Real@1: {results['real1']*100:.2f}%")
    print(f"  - Real@5: {results['real5']*100:.2f}%")

    # ë³€í™”ìœ¨ ê³„ì‚°
    prev_real1 = 0.6778
    prev_real5 = 0.8244
    real1_delta = (results['real1'] - prev_real1) * 100
    real5_delta = (results['real5'] - prev_real5) * 100

    print()
    print(f"ë³€í™”:")
    print(f"  - Real@1: {real1_delta:+.2f}%p")
    print(f"  - Real@5: {real5_delta:+.2f}%p")

    # ì¢…í•© ìŠ¤ì½”ì–´ (0.4*Real@1 + 0.3*Real@5 + 0.3*LAMBADA@1)
    score = 0.4 * results['real1'] + 0.3 * results['real5'] + 0.3 * results['lambada_top1']
    prev_score = 0.4 * prev_real1 + 0.3 * prev_real5 + 0.3 * 0.6613  # ì´ì „ ë² ìŠ¤íŠ¸ì˜ LAMBADA@1

    print()
    print(f"ì¢…í•© ìŠ¤ì½”ì–´ (0.4*Real@1 + 0.3*Real@5 + 0.3*LAMBADA@1):")
    print(f"  - ì´ì „: {prev_score:.4f}")
    print(f"  - í˜„ì¬: {score:.4f}")
    print(f"  - ë³€í™”: {(score - prev_score)*100:+.2f}%p")
    print("=" * 80)


def save_results(checkpoint_path: str, seq_len: int, results: dict, output_dir: Optional[str] = None):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if output_dir is None:
        output_dir = Path(checkpoint_path).parent
    else:
        output_dir = Path(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_name = Path(checkpoint_path).name
    output_file = output_dir / f"{checkpoint_name}_eval_results.json"

    full_results = {
        "checkpoint": str(checkpoint_path),
        "seq_len": seq_len,
        "evaluated_at": datetime.now().isoformat(),
        **results
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(full_results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    return output_file


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate checkpoint with Real@1, Real@5, and LAMBADA@1 metrics"
    )
    parser.add_argument(
        "--checkpoint",
        required=True,
        help="Path to checkpoint directory (e.g., runs/mlm_v2_scratch_1536/checkpoint-655)"
    )
    parser.add_argument(
        "--seq-len",
        type=int,
        default=None,
        help="Sequence length (default: auto-detect from checkpoint)"
    )
    parser.add_argument(
        "--output-dir",
        default=None,
        help="Output directory for results (default: same as checkpoint directory)"
    )

    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ğŸ” Checkpoint Evaluation")
    print("=" * 80)
    print(f"ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")

    start_time = time.time()

    # 1. ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì§€
    if args.seq_len is None:
        args.seq_len = detect_seq_len_from_checkpoint(args.checkpoint)
        if args.seq_len is None:
            print("âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --seq-lenìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”.")
            sys.exit(1)
        print(f"âœ“ ê°ì§€ëœ seq_len: {args.seq_len}")
    else:
        print(f"âœ“ ì§€ì •ëœ seq_len: {args.seq_len}")

    # 2. ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_checkpoint(args.checkpoint, args.seq_len)

    # 3. í‰ê°€ ì‹¤í–‰
    results = run_evaluation(model, tokenizer, args.seq_len)

    # 4. ê²°ê³¼ ì¶œë ¥
    print_results(args.checkpoint, args.seq_len, results)

    # 5. ê²°ê³¼ ì €ì¥
    output_file = save_results(args.checkpoint, args.seq_len, results, args.output_dir)

    # ì „ì²´ ì‹œê°„
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  ì „ì²´ ì†Œìš” ì‹œê°„: {timedelta(seconds=int(total_time))}")


if __name__ == "__main__":
    main()

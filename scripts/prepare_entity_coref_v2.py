#!/usr/bin/env python3
"""
Entity Coreference Dataset V2 - High Quality News-based
=======================================================

ê³ í’ˆì§ˆ ë‰´ìŠ¤ ë°ì´í„°ë¡œ Entity Coreference ë°ì´í„°ì…‹ ìƒì„± (ê°œìˆ˜ ì œí•œ ì—†ìŒ)

ë°ì´í„° ì†ŒìŠ¤ (ê°œì²´ ë°˜ë³µë¥  ê¸°ì¤€ ì„ ì •):
1. dev7halo/naver-news-summarization-ko-with-gen (97.7% ë°˜ë³µë¥ ) â­â­â­â­â­
2. HPLT/HPLT2.0_cleaned kor_Hang (81.0% ë°˜ë³µë¥ ) â­â­â­â­â­
3. AIR-Bench/qa_news_ko (91.3% ë°˜ë³µë¥ ) â­â­â­â­
4. nmixx-fin/twice_kr_finance_news_summ (90.3% ë°˜ë³µë¥ ) â­â­â­â­
5. nmixx-fin/twice_kr_news_bqa_cls (89.7% ë°˜ë³µë¥ ) â­â­â­â­

ê°œì„  ì‚¬í•­:
- ê°œì²´ ë¹ˆë„ ì œí•œ (ìµœëŒ€ 1,000íšŒ â†’ í¸í–¥ ë°©ì§€)
- ëŒ€ëª…ì‚¬ í•„í„°ë§ (NP íƒœê·¸ ì œì™¸)
- ê°œìˆ˜ ì œí•œ ì—†ìŒ (ëª¨ë“  ê³ í’ˆì§ˆ ë°ì´í„° í™œìš©)
- ê±°ë¦¬ ë‹¤ì–‘í™” (10-2000ì)
"""

import sys
import json
import gc
import random
import time
import re
import tempfile
import pickle
import os
from pathlib import Path
from collections import Counter
from typing import Dict, List, Any, Tuple, Optional, Iterable
from multiprocessing import Pool, cpu_count
import argparse
from datetime import datetime

from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ============================================================================
# ë°˜ë³µ ê°œì²´ ì°¾ê¸°
# ============================================================================

def find_exact_repetitions(text: str, kiwi) -> Dict[str, List[int]]:
    """
    ì™„ì „íˆ ê°™ì€ ê³ ìœ ëª…ì‚¬ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ê²½ìš°ë§Œ ì°¾ê¸° (ëŒ€ëª…ì‚¬ ì œì™¸)

    Returns:
        {"í™ê¸¸ë™": [0, 45, 100], "ì‚¼ì„±ì „ì": [200, 350]}
    """
    try:
        tokens = kiwi.tokenize(text)
    except:
        return {}

    return find_exact_repetitions_from_tokens(tokens)


def find_exact_repetitions_from_tokens(tokens: list) -> Dict[str, List[int]]:
    """
    í† í° ê²°ê³¼ì—ì„œ ë°˜ë³µ ê°œì²´ ì°¾ê¸° (ìºì‹± ìµœì í™”)

    Args:
        tokens: Kiwi tokenize() ê²°ê³¼ ë˜ëŠ” token dict ë¦¬ìŠ¤íŠ¸

    Returns:
        {"í™ê¸¸ë™": [0, 45, 100], "ì‚¼ì„±ì „ì": [200, 350]}
    """
    # ëŒ€ëª…ì‚¬ ì²´í¬ (ê°•í™”)
    PRONOUN_POS = {"NP"}

    # Token ê°ì²´ì™€ dict ëª¨ë‘ ì§€ì›
    def get_tag(tk):
        return tk.tag if hasattr(tk, 'tag') else tk['tag']

    has_pronoun = any(get_tag(tk) in PRONOUN_POS for tk in tokens)
    if has_pronoun:
        return {}

    entity_positions = {}

    for token in tokens:
        # Token ê°ì²´ì™€ dict ëª¨ë‘ ì§€ì›
        tag = token.tag if hasattr(token, 'tag') else token['tag']
        form = token.form if hasattr(token, 'form') else token['form']
        start = token.start if hasattr(token, 'start') else token['start']

        # NNP (ê³ ìœ ëª…ì‚¬)ë§Œ
        if tag == 'NNP' and len(form) >= 2:
            entity = form

            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            if entity in ['ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ì ', 'ë…„', 'ì›”', 'ì¼']:
                continue

            if entity not in entity_positions:
                entity_positions[entity] = []
            entity_positions[entity].append(start)

    # 2ë²ˆ ì´ìƒ ë“±ì¥í•œ ê²ƒë§Œ
    repeated = {e: pos for e, pos in entity_positions.items() if len(pos) >= 2}

    return repeated


def worker_find_repetitions(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ì›Œì»¤ (Kiwi í† í°í™” í¬í•¨)"""
    text, idx = args

    # í”„ë¡œì„¸ìŠ¤ë³„ Kiwi ì¸ìŠ¤í„´ìŠ¤
    if not hasattr(worker_find_repetitions, '_kiwi'):
        worker_find_repetitions._kiwi = Kiwi()

    kiwi = worker_find_repetitions._kiwi

    repeated = find_exact_repetitions(text, kiwi)

    if repeated:
        return (idx, text, repeated)
    return None


def worker_find_repetitions_from_tokens(args):
    """
    ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ì›Œì»¤ (ìºì‹œëœ í† í° ì‚¬ìš©, Kiwi í˜¸ì¶œ ì—†ìŒ)

    Args:
        (text, tokens, idx)
    """
    text, tokens, idx = args

    repeated = find_exact_repetitions_from_tokens(tokens)

    if repeated:
        return (idx, text, repeated)
    return None


# ============================================================================
# ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§
# ============================================================================

# ë¹ ë¥¸ ì‚¬ì „ í•„í„° (Kiwi í˜¸ì¶œ ì „ 30-40% ê±¸ëŸ¬ëƒ„)
_HANGUL_PATTERN = re.compile('[ê°€-í£]')
_SENTENCE_DELIM = '.?!'

def fast_prefilter(text: str) -> bool:
    """
    ì´ˆê³ ì† ì‚¬ì „ í•„í„° (ì •ê·œì‹ ê¸°ë°˜, Kiwië³´ë‹¤ 100ë°° ë¹ ë¦„)

    Returns:
        True: Kiwi í† í°í™” í•„ìš”, False: ì¦‰ì‹œ íƒˆë½
    """
    # 1. ë¬¸ì¥ ìˆ˜ ì²´í¬ (í•œ ë²ˆì—)
    sentence_count = sum(text.count(c) for c in _SENTENCE_DELIM)
    if sentence_count < 3:
        return False

    # 2. í•œê¸€ ë¹„ìœ¨ ì²´í¬ (ì •ê·œì‹)
    hangul_chars = len(_HANGUL_PATTERN.findall(text))
    hangul_ratio = hangul_chars / len(text) if len(text) > 0 else 0

    # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ëŠ” í•œê¸€ ë¹„ìœ¨ 40% ì´ìƒ
    if hangul_ratio < 0.4:
        return False

    # 3. ìˆ«ì/íŠ¹ìˆ˜ë¬¸ì ê³¼ë‹¤ ì²´í¬
    non_alnum = sum(1 for c in text if not c.isalnum() and c not in ' \n\t')
    if non_alnum / len(text) > 0.3:  # íŠ¹ìˆ˜ë¬¸ì 30% ì´ˆê³¼
        return False

    return True  # Kiwi í† í°í™” í•„ìš”


# Pool initializerë¡œ Kiwi ë¯¸ë¦¬ ë¡œë“œ (ì†ë„ ìµœì í™”)
_kiwi_instance = None

def init_kiwi_worker():
    """ì›Œì»¤ ì´ˆê¸°í™” ì‹œ Kiwi ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    global _kiwi_instance
    _kiwi_instance = Kiwi()


def chunk_iterable(data_list: List[Any], chunk_size: int) -> Iterable[List[Any]]:
    """ê³ ì • í¬ê¸° ë©ì–´ë¦¬ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ"""
    for start in range(0, len(data_list), chunk_size):
        yield data_list[start:start + chunk_size]

def quality_check_worker(text: str) -> Optional[str]:
    """ë³‘ë ¬ ì²˜ë¦¬ìš© í’ˆì§ˆ ì²´í¬ ì›Œì»¤ (2ë‹¨ê³„ ìµœì í™”)"""
    global _kiwi_instance

    try:
        # 1ë‹¨ê³„: ì´ˆê³ ì† ì‚¬ì „ í•„í„° (30-40% ê±¸ëŸ¬ëƒ„)
        if not fast_prefilter(text):
            return None

        # 2ë‹¨ê³„: Kiwi NNP ë¹„ìœ¨ ì²´í¬ (í†µê³¼í•œ ê²ƒë§Œ)
        tokens = _kiwi_instance.tokenize(text)
        nnp_count = sum(1 for t in tokens if t.tag == 'NNP')
        nnp_ratio = nnp_count / max(1, len(tokens))

        if nnp_ratio < 0.05 or nnp_ratio > 0.20:
            return None

        return text
    except:
        return None


# í† í° ê²°ê³¼ ìºì‹±ìš© ì›Œì»¤ (Stage 1 â†’ Stage 2 ì¬ì‚¬ìš©)
def quality_check_worker_with_tokens(text: str) -> Optional[Tuple[str, list]]:
    """
    í’ˆì§ˆ ì²´í¬ + í† í° ê²°ê³¼ ë°˜í™˜ (Stage 2 ì¬ì‚¬ìš©ìš©)

    Returns:
        (text, token_data) or None
        token_data: list of dicts (picklable)
    """
    global _kiwi_instance

    try:
        # 1ë‹¨ê³„: ì´ˆê³ ì† ì‚¬ì „ í•„í„°
        if not fast_prefilter(text):
            return None

        # 2ë‹¨ê³„: Kiwi í† í°í™”
        tokens = _kiwi_instance.tokenize(text)
        nnp_count = sum(1 for t in tokens if t.tag == 'NNP')
        nnp_ratio = nnp_count / max(1, len(tokens))

        if nnp_ratio < 0.05 or nnp_ratio > 0.20:
            return None

        # Token ê°ì²´ë¥¼ picklable dictë¡œ ë³€í™˜ (multiprocessing ì§€ì›)
        token_data = [
            {
                'form': t.form,
                'tag': t.tag,
                'start': t.start,
                'len': t.len
            }
            for t in tokens
        ]

        return (text, token_data)
    except:
        return None


# ============================================================================
# Coref ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ ì¶”ê°€)
# ============================================================================

def create_coref_examples_with_limit(
    text: str,
    repeated_entities: Dict[str, List[int]],
    entity_counter: Counter,
    max_entity_freq: int = 1000
) -> List[Dict[str, Any]]:
    """
    ë°˜ë³µ ê°œì²´ì—ì„œ Coref í•™ìŠµ ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        repeated_entities: {"í™ê¸¸ë™": [0, 45, 100]}
        entity_counter: ì „ì²´ ê°œì²´ ë¹ˆë„ ì¹´ìš´í„°
        max_entity_freq: ê°œì²´ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

    Returns:
        ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    """
    examples = []

    for entity, positions in repeated_entities.items():
        # ê°œì²´ ë¹ˆë„ ì œí•œ ì²´í¬
        if entity_counter[entity] >= max_entity_freq:
            continue

        # ì²« ë²ˆì§¸ëŠ” ì„ í–‰ì‚¬ë¡œ ìœ ì§€
        antecedent_pos = positions[0]

        # 2ë²ˆì§¸ ì´ìƒ ë“±ì¥ì„ [MASK]ë¡œ ì¹˜í™˜ (ìµœëŒ€ 3ê°œ)
        for coref_pos in positions[1:4]:
            # ê°œì²´ë³„ ë¹ˆë„ ì¬ì²´í¬
            if entity_counter[entity] >= max_entity_freq:
                break

            # ê±°ë¦¬ ê³„ì‚°
            distance = coref_pos - antecedent_pos

            # ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ì œì™¸ (10ì ë¯¸ë§Œ)
            if distance < 10:
                continue

            # ë„ˆë¬´ ë©€ë©´ ì œì™¸ (2000ì ì´ˆê³¼)
            if distance > 2000:
                continue

            # [MASK] ìƒì„±
            text_before = text[:coref_pos]
            text_after = text[coref_pos + len(entity):]
            masked_text = text_before + "[MASK]" + text_after

            examples.append({
                "text": masked_text,
                "target": entity,
                "antecedent_pos": antecedent_pos,
                "coref_pos": coref_pos,
                "distance": distance
            })

            # ë¹ˆë„ ì¦ê°€
            entity_counter[entity] += 1

    return examples


# ============================================================================
# ë°ì´í„°ì…‹ë³„ ì²˜ë¦¬
# ============================================================================

def process_naver_news_gen(num_workers: int = 20) -> List[str]:
    """dev7halo/naver-news-summarization-ko-with-gen (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Naver News (ìµœê³  í’ˆì§ˆ)")
    print("=" * 80)

    dataset = load_dataset("dev7halo/naver-news-summarization-ko-with-gen", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['document'] for s in dataset if 800 <= len(s['document']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„° (initializer ì¶”ê°€ë¡œ ë²„ê·¸ ìˆ˜ì •)
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


def process_hplt_korean(num_workers: int = 20) -> List[str]:
    """
    HPLT/HPLT2.0_cleaned kor_Hang (ìŠ¤íŠ¸ë¦¬ë° + ë””ìŠ¤í¬ ì €ì¥, OOM ì™„ì „ ë°©ì§€)

    Returns:
        List[str] - temp file paths (í† í° ìºì‹± ë°ì´í„°)
    """

    print("\n" + "=" * 80)
    print(f"ğŸ“Š HPLT Korean (ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì²˜ë¦¬, ì „ì²´ ë°ì´í„°)")
    print("=" * 80)

    dataset = load_dataset("HPLT/HPLT2.0_cleaned", "kor_Hang", split="train", streaming=True)

    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹: Stage 1 + 2 í†µí•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    optimal_workers = min(60, num_workers)  # OOM ë°©ì§€: ìµœëŒ€ 60ê°œ
    print(f"  ì›Œì»¤: {optimal_workers}ê°œ, ë°°ì¹˜ë‹¹ 100kê°œ (ë©”ëª¨ë¦¬ ì•ˆì „)")
    print(f"  ğŸ’¾ ë°°ì¹˜ ê²°ê³¼ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)")

    BATCH_SIZE = 100_000  # 10ë§Œê°œì”© ë°°ì¹˜ ì²˜ë¦¬
    temp_files = []  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

    current_batch = []
    scanned = 0
    total_passed = 0
    batch_num = 0
    start_time = time.time()

    # Pool 1ë²ˆë§Œ ìƒì„± (ì „ì²´ ìŠ¤íŠ¸ë¦¬ë° ë™ì•ˆ ì¬ì‚¬ìš©)
    with Pool(processes=optimal_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:

        for sample in dataset:
            scanned += 1
            text = sample['text']

            # ê¸¸ì´ í•„í„° (ì¦‰ì‹œ)
            if 800 <= len(text) <= 2500:
                current_batch.append(text)

            # ë°°ì¹˜ê°€ ì°¨ë©´ ì¦‰ì‹œ í’ˆì§ˆ í•„í„°ë§
            if len(current_batch) >= BATCH_SIZE:
                batch_num += 1
                batch_start_time = time.time()

                print(f"\në°°ì¹˜ {batch_num} | ìŠ¤ìº”: {scanned:,} | ë°°ì¹˜ í¬ê¸°: {len(current_batch):,}", flush=True)
                print(f"  í’ˆì§ˆ í•„í„°ë§ ì‹œì‘... (ì›Œì»¤: {optimal_workers}ê°œ)", flush=True)

                # ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§ + í† í° ìºì‹±
                batch_filtered = []
                for idx, result in enumerate(pool.imap_unordered(quality_check_worker_with_tokens, current_batch, chunksize=500), 1):
                    if result is not None:
                        batch_filtered.append(result)

                    # ì§„í–‰ë„ (1,000ê°œë§ˆë‹¤ í‘œì‹œ - ë” ìì£¼)
                    if idx % 1_000 == 0 or idx == 1:
                        batch_elapsed = time.time() - batch_start_time
                        rate = idx / batch_elapsed if batch_elapsed > 0 else 0
                        print(f"  ì²˜ë¦¬: {idx:,} / {len(current_batch):,} ({idx/len(current_batch)*100:.1f}%) | í†µê³¼: {len(batch_filtered):,} | {rate:.0f} docs/s", flush=True)

                total_passed += len(batch_filtered)

                batch_elapsed = time.time() - batch_start_time
                total_elapsed = time.time() - start_time
                avg_rate = scanned / total_elapsed if total_elapsed > 0 else 0
                pass_rate = len(batch_filtered) / len(current_batch) * 100 if current_batch else 0

                # ğŸ’¾ ë””ìŠ¤í¬ì— ì €ì¥ (ë©”ëª¨ë¦¬ í•´ì œ)
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl', prefix='hplt_batch_')
                with open(temp_file.name, 'wb') as f:
                    pickle.dump(batch_filtered, f)
                temp_files.append(temp_file.name)

                print(f"   â†’ ë°°ì¹˜ ì™„ë£Œ: {len(batch_filtered):,}ê°œ í†µê³¼ ({pass_rate:.1f}%) | ëˆ„ì : {total_passed:,}ê°œ | í‰ê· : {avg_rate:.0f} docs/s")
                print(f"   ğŸ’¾ ì €ì¥: {temp_file.name}")

                # ë©”ëª¨ë¦¬ í•´ì œ (í•µì‹¬!)
                current_batch = []
                del batch_filtered
                gc.collect()

        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            batch_num += 1
            print(f"\në§ˆì§€ë§‰ ë°°ì¹˜ {batch_num} | ë°°ì¹˜ í¬ê¸°: {len(current_batch):,}", flush=True)

            batch_filtered = []
            for result in pool.imap_unordered(quality_check_worker_with_tokens, current_batch, chunksize=2000):
                if result is not None:
                    batch_filtered.append(result)

            total_passed += len(batch_filtered)

            pass_rate = len(batch_filtered) / len(current_batch) * 100 if current_batch else 0

            # ğŸ’¾ ë””ìŠ¤í¬ì— ì €ì¥
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl', prefix='hplt_batch_')
            with open(temp_file.name, 'wb') as f:
                pickle.dump(batch_filtered, f)
            temp_files.append(temp_file.name)

            print(f"   â†’ ë§ˆì§€ë§‰ ë°°ì¹˜ ì™„ë£Œ: {len(batch_filtered):,}ê°œ í†µê³¼ ({pass_rate:.1f}%)")
            print(f"   ğŸ’¾ ì €ì¥: {temp_file.name}")

            del current_batch, batch_filtered
            gc.collect()

    total_elapsed = time.time() - start_time
    overall_pass_rate = total_passed / scanned * 100 if scanned > 0 else 0

    print(f"\nâœ… HPLT ì™„ë£Œ: {total_passed:,}ê°œ ì„ ë³„ (ì„ì‹œ íŒŒì¼ {len(temp_files)}ê°œ)")
    print(f"   ìŠ¤ìº”: {scanned:,}ê°œ | í†µê³¼ìœ¨: {overall_pass_rate:.2f}%")
    print(f"â±ï¸  ì´ ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")

    return temp_files


def process_air_bench_news(num_workers: int = 20) -> List[str]:
    """
    AIR-Bench/qa_news_ko (ìŠ¤íŠ¸ë¦¬ë° + ë””ìŠ¤í¬ ì €ì¥, OOM ì™„ì „ ë°©ì§€)

    Returns:
        List[str] - temp file paths (í† í° ìºì‹± ë°ì´í„°)
    """

    print("\n" + "=" * 80)
    print(f"ğŸ“Š AIR-Bench QA News (ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì²˜ë¦¬, ì „ì²´ ë°ì´í„°)")
    print("=" * 80)

    dataset = load_dataset("AIR-Bench/qa_news_ko", split="corpus_default", streaming=True)

    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹: Stage 1 + 2 í†µí•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    optimal_workers = min(60, num_workers)  # OOM ë°©ì§€: ìµœëŒ€ 60ê°œ
    print(f"  ì›Œì»¤: {optimal_workers}ê°œ, ë°°ì¹˜ë‹¹ 100kê°œ (ë©”ëª¨ë¦¬ ì•ˆì „)")
    print(f"  ğŸ’¾ ë°°ì¹˜ ê²°ê³¼ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)")

    BATCH_SIZE = 100_000  # 10ë§Œê°œì”© ë°°ì¹˜ ì²˜ë¦¬
    temp_files = []  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

    current_batch = []
    scanned = 0
    total_passed = 0
    batch_num = 0
    start_time = time.time()

    # Pool 1ë²ˆë§Œ ìƒì„± (ì „ì²´ ìŠ¤íŠ¸ë¦¬ë° ë™ì•ˆ ì¬ì‚¬ìš©)
    with Pool(processes=optimal_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:

        for sample in dataset:
            scanned += 1
            text = sample['text']

            # ê¸¸ì´ í•„í„° (ì¦‰ì‹œ)
            if 800 <= len(text) <= 2500:
                current_batch.append(text)

            # ë°°ì¹˜ê°€ ì°¨ë©´ ì¦‰ì‹œ í’ˆì§ˆ í•„í„°ë§
            if len(current_batch) >= BATCH_SIZE:
                batch_num += 1
                batch_start_time = time.time()

                print(f"\në°°ì¹˜ {batch_num} | ìŠ¤ìº”: {scanned:,} | ë°°ì¹˜ í¬ê¸°: {len(current_batch):,}", flush=True)
                print(f"  í’ˆì§ˆ í•„í„°ë§ ì‹œì‘... (ì›Œì»¤: {optimal_workers}ê°œ)", flush=True)

                # ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§ + í† í° ìºì‹±
                batch_filtered = []
                for idx, result in enumerate(pool.imap_unordered(quality_check_worker_with_tokens, current_batch, chunksize=500), 1):
                    if result is not None:
                        batch_filtered.append(result)

                    # ì§„í–‰ë„ (1,000ê°œë§ˆë‹¤ í‘œì‹œ - ë” ìì£¼)
                    if idx % 1_000 == 0 or idx == 1:
                        batch_elapsed = time.time() - batch_start_time
                        rate = idx / batch_elapsed if batch_elapsed > 0 else 0
                        print(f"  ì²˜ë¦¬: {idx:,} / {len(current_batch):,} ({idx/len(current_batch)*100:.1f}%) | í†µê³¼: {len(batch_filtered):,} | {rate:.0f} docs/s", flush=True)

                total_passed += len(batch_filtered)

                batch_elapsed = time.time() - batch_start_time
                total_elapsed = time.time() - start_time
                avg_rate = scanned / total_elapsed if total_elapsed > 0 else 0
                pass_rate = len(batch_filtered) / len(current_batch) * 100 if current_batch else 0

                # ğŸ’¾ ë””ìŠ¤í¬ì— ì €ì¥ (ë©”ëª¨ë¦¬ í•´ì œ)
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl', prefix='air_batch_')
                with open(temp_file.name, 'wb') as f:
                    pickle.dump(batch_filtered, f)
                temp_files.append(temp_file.name)

                print(f"   â†’ ë°°ì¹˜ ì™„ë£Œ: {len(batch_filtered):,}ê°œ í†µê³¼ ({pass_rate:.1f}%) | ëˆ„ì : {total_passed:,}ê°œ | í‰ê· : {avg_rate:.0f} docs/s")
                print(f"   ğŸ’¾ ì €ì¥: {temp_file.name}")

                # ë©”ëª¨ë¦¬ í•´ì œ (í•µì‹¬!)
                current_batch = []
                del batch_filtered
                gc.collect()

        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            batch_num += 1
            print(f"\në§ˆì§€ë§‰ ë°°ì¹˜ {batch_num} | ë°°ì¹˜ í¬ê¸°: {len(current_batch):,}", flush=True)

            batch_filtered = []
            for result in pool.imap_unordered(quality_check_worker_with_tokens, current_batch, chunksize=2000):
                if result is not None:
                    batch_filtered.append(result)

            total_passed += len(batch_filtered)

            pass_rate = len(batch_filtered) / len(current_batch) * 100 if current_batch else 0

            # ğŸ’¾ ë””ìŠ¤í¬ì— ì €ì¥
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl', prefix='air_batch_')
            with open(temp_file.name, 'wb') as f:
                pickle.dump(batch_filtered, f)
            temp_files.append(temp_file.name)

            print(f"   â†’ ë§ˆì§€ë§‰ ë°°ì¹˜ ì™„ë£Œ: {len(batch_filtered):,}ê°œ í†µê³¼ ({pass_rate:.1f}%)")
            print(f"   ğŸ’¾ ì €ì¥: {temp_file.name}")

            del current_batch, batch_filtered
            gc.collect()

    total_elapsed = time.time() - start_time
    overall_pass_rate = total_passed / scanned * 100 if scanned > 0 else 0

    print(f"\nâœ… AIR-Bench ì™„ë£Œ: {total_passed:,}ê°œ ì„ ë³„ (ì„ì‹œ íŒŒì¼ {len(temp_files)}ê°œ)")
    print(f"   ìŠ¤ìº”: {scanned:,}ê°œ | í†µê³¼ìœ¨: {overall_pass_rate:.2f}%")
    print(f"â±ï¸  ì´ ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")

    return temp_files


def process_finance_news(num_workers: int = 20) -> List[str]:
    """nmixx-fin/twice_kr_finance_news_summ (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Finance News (ê¸ˆìœµ)")
    print("=" * 80)

    dataset = load_dataset("nmixx-fin/twice_kr_finance_news_summ", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['text'] for s in dataset if 800 <= len(s['text']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„° (initializer ì¶”ê°€ë¡œ ë²„ê·¸ ìˆ˜ì •)
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


def process_bqa_news(num_workers: int = 20) -> List[str]:
    """nmixx-fin/twice_kr_news_bqa_cls (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š BQA News (ê²½ì œ)")
    print("=" * 80)

    dataset = load_dataset("nmixx-fin/twice_kr_news_bqa_cls", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['text'] for s in dataset if 800 <= len(s['text']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„° (initializer ì¶”ê°€ë¡œ ë²„ê·¸ ìˆ˜ì •)
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


# ============================================================================
# ë©”ì¸ ì²˜ë¦¬
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Entity Coref Dataset V2 ìƒì„± (ê°œìˆ˜ ì œí•œ ì—†ìŒ)")
    parser.add_argument("--seq-len", type=int, default=2048,
                        help="ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 2048)")
    parser.add_argument("--output-dir", default="./prepared_datasets",
                        help="ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--num-workers", type=int, default=None,
                        help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base",
                        help="í† í¬ë‚˜ì´ì € ëª¨ë¸")
    parser.add_argument("--max-entity-freq", type=int, default=1000,
                        help="ê°œì²´ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 1000, í¸í–¥ ë°©ì§€)")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="ìµœëŒ€ ìˆ˜ì§‘ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: ì œí•œ ì—†ìŒ)")
    parser.add_argument("--token-chunk-size", type=int, default=20000,
                        help="í† í°í™” ë‹¨ê³„ì—ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 20,000)")
    parser.add_argument("--tokenizer-batch-size", type=int, default=4096,
                        help="í† í¬ë‚˜ì´ì € í˜¸ì¶œ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 4,096)")

    args = parser.parse_args()

    # ì›Œì»¤ ìˆ˜
    if args.num_workers is None:
        args.num_workers = max(4, cpu_count() - 4)

    print("\n" + "=" * 80)
    print("ğŸš€ Entity Coreference Dataset V2 ìƒì„±")
    print("=" * 80)
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_len}")
    print(f"ì›Œì»¤ ìˆ˜: {args.num_workers}")
    print(f"ê°œì²´ë‹¹ ìµœëŒ€ ë¹ˆë„: {args.max_entity_freq}")
    if args.max_samples is not None:
        print(f"ìƒ˜í”Œ ìˆ˜ ì œí•œ: {args.max_samples:,}ê°œ")
    else:
        print(f"ê°œìˆ˜ ì œí•œ: ì—†ìŒ (ì „ì²´ ê³ í’ˆì§ˆ ë°ì´í„° í™œìš©)")

    overall_start = time.time()

    # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë³‘ë ¬ í’ˆì§ˆ í•„í„° + í† í° ìºì‹±)
    print("\n" + "=" * 80)
    print("ğŸ“¥ 1ë‹¨ê³„: ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (5ê°œ ë°ì´í„°ì…‹ + ë³‘ë ¬ í•„í„° + í† í° ìºì‹±)")
    print("=" * 80)

    temp_files = []  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ (HPLT + AIR-Bench)
    plain_texts = []  # textë§Œ ì €ì¥ (Naver, Finance, BQA)

    # Dataset 1: Naver News Gen (ìµœê³  í’ˆì§ˆ, textë§Œ)
    texts1 = process_naver_news_gen(args.num_workers)
    plain_texts.extend(texts1)

    # Dataset 2: HPLT Korean (ëŒ€ê·œëª¨, ì „ì²´, ë””ìŠ¤í¬ ì €ì¥)
    temp_files_hplt = process_hplt_korean(args.num_workers)
    temp_files.extend(temp_files_hplt)

    # Dataset 3: AIR-Bench News (100ë§Œ+, ì „ì²´, ë””ìŠ¤í¬ ì €ì¥)
    temp_files_air = process_air_bench_news(args.num_workers)
    temp_files.extend(temp_files_air)

    # Dataset 4: Finance News (textë§Œ)
    texts4 = process_finance_news(args.num_workers)
    plain_texts.extend(texts4)

    # Dataset 5: BQA News (textë§Œ)
    texts5 = process_bqa_news(args.num_workers)
    plain_texts.extend(texts5)

    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸:")
    print(f"   í† í° ìºì‹± (ì„ì‹œ íŒŒì¼): {len(temp_files)}ê°œ íŒŒì¼ (HPLT + AIR-Bench)")
    print(f"   ì¼ë°˜ í…ìŠ¤íŠ¸: {len(plain_texts):,}ê°œ (Naver + Finance + BQA)")

    # 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (í† í° ìºì‹± ìµœì í™”)
    print("\n" + "=" * 80)
    print(f"ğŸ” 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (í† í° ìºì‹± í™œìš©, Kiwi ì¬í˜¸ì¶œ ì œê±°)")
    print("=" * 80)

    step2_start = time.time()

    results = []

    # 2-1: í† í° ìºì‹œëœ ë°ì´í„° ì²˜ë¦¬ (Kiwi í˜¸ì¶œ ì—†ìŒ, ì´ˆê³ ì†)
    print(f"  [2-1] í† í° ìºì‹œ í™œìš©: {len(temp_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ (Kiwi ì¬í˜¸ì¶œ ì—†ìŒ)")
    cache_start = time.time()

    idx_counter = 0
    for file_idx, temp_file in enumerate(temp_files, 1):
        print(f"    íŒŒì¼ {file_idx}/{len(temp_files)}: {temp_file}")

        # íŒŒì¼ì—ì„œ ë°°ì¹˜ ë¡œë“œ
        with open(temp_file, 'rb') as f:
            batch_data = pickle.load(f)

        # ë³‘ë ¬ ì²˜ë¦¬
        with Pool(processes=args.num_workers, maxtasksperchild=1000) as pool:
            batch_results = pool.map(
                worker_find_repetitions_from_tokens,
                [(text, tokens, idx_counter + i) for i, (text, tokens) in enumerate(batch_data)],
                chunksize=500
            )

        batch_results = [r for r in batch_results if r is not None]
        results.extend(batch_results)

        print(f"      â†’ {len(batch_results):,}ê°œ ë°œê²¬")

        idx_counter += len(batch_data)

        # ë©”ëª¨ë¦¬ í•´ì œ
        del batch_data, batch_results
        gc.collect()

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_file)

    cache_elapsed = time.time() - cache_start
    print(f"     ì´ {len(results):,}ê°œ ë°œê²¬ ({cache_elapsed:.1f}ì´ˆ)")

    # 2-2: ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Kiwi í† í°í™” í•„ìš”)
    print(f"  [2-2] ì¼ë°˜ í…ìŠ¤íŠ¸: {len(plain_texts):,}ê°œ ì²˜ë¦¬ (Kiwi í† í°í™”)")
    plain_start = time.time()

    with Pool(processes=args.num_workers, initializer=init_kiwi_worker, maxtasksperchild=1000) as pool:
        plain_results = pool.map(
            worker_find_repetitions,
            [(text, idx_counter + i) for i, text in enumerate(plain_texts)],
            chunksize=500
        )

    plain_results = [r for r in plain_results if r is not None]
    plain_elapsed = time.time() - plain_start
    print(f"     â†’ {len(plain_results):,}ê°œ ë°œê²¬ ({plain_elapsed:.1f}ì´ˆ)")

    results.extend(plain_results)

    step2_elapsed = time.time() - step2_start

    total_texts = idx_counter + len(plain_texts)
    print(f"\nâœ… ë°˜ë³µ ê°œì²´ ë°œê²¬: {len(results):,}/{total_texts:,} í…ìŠ¤íŠ¸ "
          f"({len(results)/total_texts*100:.1f}%)")
    print(f"â±ï¸  ì´ ì‹œê°„: {step2_elapsed:.1f}ì´ˆ (í† í° ìºì‹±ìœ¼ë¡œ {idx_counter:,}ê°œ Kiwi ì¬í˜¸ì¶œ ì œê±°)")

    # 3ë‹¨ê³„: Coref ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)
    print("\n" + "=" * 80)
    print("ğŸ”¨ 3ë‹¨ê³„: Coreference ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)")
    print("=" * 80)

    step3_start = time.time()

    entity_counter = Counter()
    all_examples = []

    for idx, text, repeated in results:
        # max_samples ì œí•œ í™•ì¸
        if args.max_samples is not None and len(all_examples) >= args.max_samples:
            print(f"  âš ï¸  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ë„ë‹¬: {args.max_samples:,}ê°œ - ìˆ˜ì§‘ ì¤‘ë‹¨", flush=True)
            break

        examples = create_coref_examples_with_limit(
            text, repeated, entity_counter, args.max_entity_freq
        )
        all_examples.extend(examples)

        # ì§„í–‰ ìƒí™© (10000ê°œë§ˆë‹¤)
        if len(all_examples) % 10000 == 0:
            print(f"  ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,} (ê³ ìœ  ê°œì²´: {len(entity_counter):,})", flush=True)

            # max_samples ê·¼ì ‘ ì‹œ ê²½ê³ 
            if args.max_samples is not None and len(all_examples) >= args.max_samples * 0.9:
                remaining = args.max_samples - len(all_examples)
                print(f"  â„¹ï¸  ëª©í‘œê¹Œì§€ ë‚¨ì€ ìƒ˜í”Œ: {remaining:,}ê°œ", flush=True)

    step3_elapsed = time.time() - step3_start

    print(f"\nâœ… ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}")
    print(f"âœ… ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}")
    print(f"â±ï¸  ìƒì„± ì‹œê°„: {step3_elapsed:.1f}ì´ˆ")

    # í†µê³„
    print("\n" + "=" * 80)
    print("ğŸ“Š ë°ì´í„°ì…‹ í†µê³„")
    print("=" * 80)

    distances = [ex['distance'] for ex in all_examples]
    targets = [ex['target'] for ex in all_examples]

    print(f"  ê±°ë¦¬ (ì„ í–‰ì‚¬ â†” ìƒí˜¸ì°¸ì¡°):")
    print(f"    í‰ê· : {np.mean(distances):.1f} ë¬¸ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(distances):.1f} ë¬¸ì")
    print(f"    ë²”ìœ„: {np.min(distances)} ~ {np.max(distances)} ë¬¸ì")

    # ê±°ë¦¬ ë¶„í¬
    short = sum(1 for d in distances if d < 200)
    medium = sum(1 for d in distances if 200 <= d < 800)
    long = sum(1 for d in distances if d >= 800)
    print(f"\n  ê±°ë¦¬ ë¶„í¬:")
    print(f"    Short (10-200ì): {short:,} ({short/len(distances)*100:.1f}%)")
    print(f"    Medium (200-800ì): {medium:,} ({medium/len(distances)*100:.1f}%)")
    print(f"    Long (800-2000ì): {long:,} ({long/len(distances)*100:.1f}%)")

    print(f"\n  ê°œì²´(target) ë¶„í¬:")
    print(f"    ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}ê°œ")
    print(f"    Top 10 ë¹ˆë„:")
    for target, count in entity_counter.most_common(10):
        print(f"      '{target}': {count:,}ê°œ ({count/len(targets)*100:.2f}%)")

    # ê°œì²´ ê¸¸ì´ ë¶„í¬
    entity_lengths = [len(t) for t in targets]
    print(f"\n  ê°œì²´ ê¸¸ì´ ë¶„í¬:")
    print(f"    í‰ê· : {np.mean(entity_lengths):.1f} ê¸€ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(entity_lengths):.1f} ê¸€ì")
    print(f"    ë²”ìœ„: {np.min(entity_lengths)} ~ {np.max(entity_lengths)} ê¸€ì")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n  ìƒ˜í”Œ ì˜ˆì‹œ:")
    for i, ex in enumerate(random.sample(all_examples, min(3, len(all_examples))), 1):
        preview = ex['text'][:100] + "..." if len(ex['text']) > 100 else ex['text']
        print(f"    {i}. ê°œì²´: '{ex['target']}' (ê±°ë¦¬: {ex['distance']} ë¬¸ì)")
        print(f"       í…ìŠ¤íŠ¸: {preview}")
        print()

    # 4ë‹¨ê³„: í† í°í™” ë° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ OOM ë°©ì§€)
    print("\n" + "=" * 80)
    print("ğŸ’¾ 4ë‹¨ê³„: í† í°í™” ë° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)")
    print("=" * 80)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    step4_start = time.time()

    total_samples = len(all_examples)

    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ")

    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬/ì†ë„ ê· í˜•)
    token_chunk_size = max(1, args.token_chunk_size)
    tokenizer_batch_size = max(1, args.tokenizer_batch_size)
    num_tok_batches = (total_samples + token_chunk_size - 1) // token_chunk_size

    print(
        f"ë°°ì¹˜ ìˆ˜: {num_tok_batches}ê°œ "
        f"(chunk={token_chunk_size:,}, tokenizer_batch={tokenizer_batch_size:,})"
    )

    # ì„ì‹œ ë””ë ‰í† ë¦¬ (ë°°ì¹˜ ì €ì¥)
    temp_dir = output_dir / f"temp_entity_coref_v2_{args.seq_len}"
    temp_dir.mkdir(parents=True, exist_ok=True)

    tokenized_batches = []

    # í† í¬ë‚˜ì´ì € ë¯¸ë¦¬ ì„¤ì • (fast tokenizer ë‚´ë¶€ ë©€í‹°ìŠ¤ë ˆë”© í™œìš©)
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
    default_token_type = [0] * args.seq_len

    for batch_idx, batch_examples in enumerate(
        chunk_iterable(all_examples, token_chunk_size)
    ):
        current_count = len(batch_examples)
        print(
            f"\në°°ì¹˜ {batch_idx + 1}/{num_tok_batches}: {current_count:,}ê°œ í† í°í™” ì¤‘...",
            flush=True,
        )

        chunk_input_ids = []
        chunk_attention_mask = []
        chunk_token_type_ids = []

        for sub_start in range(0, current_count, tokenizer_batch_size):
            sub_examples = batch_examples[sub_start:sub_start + tokenizer_batch_size]
            sub_texts = [ex["text"] for ex in sub_examples]

            encoded = tokenizer(
                sub_texts,
                truncation=True,
                padding="max_length",
                max_length=args.seq_len,
            )

            chunk_input_ids.extend(encoded["input_ids"])
            chunk_attention_mask.extend(encoded["attention_mask"])

            if "token_type_ids" in encoded:
                chunk_token_type_ids.extend(encoded["token_type_ids"])
            else:
                chunk_token_type_ids.extend(
                    [default_token_type[:] for _ in range(len(sub_examples))]
                )

        chunk_targets = [ex["target"] for ex in batch_examples]
        chunk_antecedent = [ex["antecedent_pos"] for ex in batch_examples]
        chunk_coref = [ex["coref_pos"] for ex in batch_examples]
        chunk_distance = [ex["distance"] for ex in batch_examples]

        batch_dataset = Dataset.from_dict(
            {
                "input_ids": chunk_input_ids,
                "attention_mask": chunk_attention_mask,
                "token_type_ids": chunk_token_type_ids,
                "target": chunk_targets,
                "antecedent_pos": chunk_antecedent,
                "coref_pos": chunk_coref,
                "distance": chunk_distance,
            }
        )

        batch_path = temp_dir / f"batch_{batch_idx}"
        batch_dataset.save_to_disk(str(batch_path))
        tokenized_batches.append(str(batch_path))

        print(f"  â†’ ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: {len(batch_dataset):,}ê°œ", flush=True)

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del (
            batch_dataset,
            chunk_input_ids,
            chunk_attention_mask,
            chunk_token_type_ids,
            chunk_targets,
            chunk_antecedent,
            chunk_coref,
            chunk_distance,
        )
        gc.collect()

    # ë°°ì¹˜ ë³‘í•©
    print(f"\në°°ì¹˜ ë³‘í•© ì¤‘...", flush=True)
    from datasets import concatenate_datasets, load_from_disk

    merged_datasets = [load_from_disk(batch_path) for batch_path in tokenized_batches]
    tokenized = concatenate_datasets(merged_datasets)

    # ìµœì¢… ì €ì¥
    save_path = output_dir / f"entity_coref_v2_{args.seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)
    tokenized.save_to_disk(str(save_path))

    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    import shutil
    shutil.rmtree(temp_dir)

    step4_elapsed = time.time() - step4_start

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(tokenized):,}")
    print(f"â±ï¸  ì‹œê°„: {step4_elapsed:.1f}ì´ˆ")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    meta_path = save_path / "metadata.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump({
            "dataset_type": "entity_coreference_v2",
            "version": "2.0",
            "num_samples": len(all_examples),
            "seq_len": args.seq_len,
            "created_at": datetime.now().isoformat(),
            "source_datasets": [
                "dev7halo/naver-news-summarization-ko-with-gen",
                "HPLT/HPLT2.0_cleaned (kor_Hang)",
                "AIR-Bench/qa_news_ko",
                "nmixx-fin/twice_kr_finance_news_summ",
                "nmixx-fin/twice_kr_news_bqa_cls"
            ],
            "num_unique_entities": len(entity_counter),
            "max_entity_freq": args.max_entity_freq,
            "top_entities": dict(entity_counter.most_common(20)),
            "entity_length_stats": {
                "mean": float(np.mean(entity_lengths)),
                "median": float(np.median(entity_lengths)),
                "min": int(np.min(entity_lengths)),
                "max": int(np.max(entity_lengths))
            },
            "distance_stats": {
                "mean": float(np.mean(distances)),
                "median": float(np.median(distances)),
                "min": int(np.min(distances)),
                "max": int(np.max(distances))
            },
            "distance_distribution": {
                "short_10_200": short,
                "medium_200_800": medium,
                "long_800_2000": long
            }
        }, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ ë©”íƒ€ë°ì´í„°: {meta_path}")

    overall_elapsed = time.time() - overall_start

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì „ì²´ ì‹œê°„: {overall_elapsed:.1f}ì´ˆ ({overall_elapsed/60:.1f}ë¶„)")
    print(f"ğŸ“Š ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}ê°œ")
    print(f"ğŸ¯ ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")

    print("\nğŸ“– ë‹¤ìŒ ë‹¨ê³„:")
    print("=" * 80)
    print("í›ˆë ¨ ì‹¤í–‰:")
    print()
    print(f"PYTHONNOUSERSITE=1 uv run python scripts/run_entity_coref_finetune.py \\")
    print(f"  --checkpoint runs/combined_experiment/checkpoint-1230 \\")
    print(f"  --dataset {save_path} \\")
    print(f"  --epochs 5 \\")
    print(f"  --batch-size 8 \\")
    print(f"  --gradient-accumulation 8 \\")
    print(f"  --lr 1.5e-5 \\")
    print(f"  --eval-steps 200 \\")
    print(f"  --output-dir runs/entity_coref_v2")


if __name__ == "__main__":
    main()

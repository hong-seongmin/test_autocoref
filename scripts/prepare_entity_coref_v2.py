#!/usr/bin/env python3
"""
Entity Coreference Dataset V2 - High Quality News-based
=======================================================

ê³ í’ˆì§ˆ ë‰´ìŠ¤ ë°ì´í„°ë¡œ Entity Coreference ë°ì´í„°ì…‹ ìƒì„± (ê°œìˆ˜ ì œí•œ ì—†ìŒ)

ë°ì´í„° ì†ŒìŠ¤ (ê°œì²´ ë°˜ë³µë¥  ê¸°ì¤€ ì„ ì •):
1. dev7halo/naver-news-summarization-ko-with-gen (97.7% ë°˜ë³µë¥ ) â­â­â­â­â­
2. HPLT/HPLT2.0_cleaned kor_Hang (81.0% ë°˜ë³µë¥ ) â­â­â­â­â­
3. AIR-Bench/qa_news_ko (91.3% ë°˜ë³µë¥ ) â­â­â­â­
4. nmixx-fin/twice_kr_finance_news_summ (90.3% ë°˜ë³µë¥ ) â­â­â­â­
5. nmixx-fin/twice_kr_news_bqa_cls (89.7% ë°˜ë³µë¥ ) â­â­â­â­

ê°œì„  ì‚¬í•­:
- ê°œì²´ ë¹ˆë„ ì œí•œ (ìµœëŒ€ 1,000íšŒ â†’ í¸í–¥ ë°©ì§€)
- ëŒ€ëª…ì‚¬ í•„í„°ë§ (NP íƒœê·¸ ì œì™¸)
- ê°œìˆ˜ ì œí•œ ì—†ìŒ (ëª¨ë“  ê³ í’ˆì§ˆ ë°ì´í„° í™œìš©)
- ê±°ë¦¬ ë‹¤ì–‘í™” (10-2000ì)
"""

import os
import sys
import json
import gc
import random
import time
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple
from multiprocessing import Pool, cpu_count
import argparse
from datetime import datetime

from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ============================================================================
# ë°˜ë³µ ê°œì²´ ì°¾ê¸°
# ============================================================================

def find_exact_repetitions(text: str, kiwi) -> Dict[str, List[int]]:
    """
    ì™„ì „íˆ ê°™ì€ ê³ ìœ ëª…ì‚¬ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ê²½ìš°ë§Œ ì°¾ê¸° (ëŒ€ëª…ì‚¬ ì œì™¸)

    Returns:
        {"í™ê¸¸ë™": [0, 45, 100], "ì‚¼ì„±ì „ì": [200, 350]}
    """
    try:
        tokens = kiwi.tokenize(text)
    except:
        return {}

    # ëŒ€ëª…ì‚¬ ì²´í¬ (ê°•í™”)
    PRONOUN_POS = {"NP"}
    has_pronoun = any(tk.tag in PRONOUN_POS for tk in tokens)
    if has_pronoun:
        return {}

    entity_positions = {}

    for token in tokens:
        # NNP (ê³ ìœ ëª…ì‚¬)ë§Œ
        if token.tag == 'NNP' and len(token.form) >= 2:
            entity = token.form

            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            if entity in ['ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ì ', 'ë…„', 'ì›”', 'ì¼']:
                continue

            if entity not in entity_positions:
                entity_positions[entity] = []
            entity_positions[entity].append(token.start)

    # 2ë²ˆ ì´ìƒ ë“±ì¥í•œ ê²ƒë§Œ
    repeated = {e: pos for e, pos in entity_positions.items() if len(pos) >= 2}

    return repeated


def worker_find_repetitions(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ì›Œì»¤"""
    text, idx = args

    # í”„ë¡œì„¸ìŠ¤ë³„ Kiwi ì¸ìŠ¤í„´ìŠ¤
    if not hasattr(worker_find_repetitions, '_kiwi'):
        worker_find_repetitions._kiwi = Kiwi()

    kiwi = worker_find_repetitions._kiwi

    repeated = find_exact_repetitions(text, kiwi)

    if repeated:
        return (idx, text, repeated)
    return None


# ============================================================================
# ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§
# ============================================================================

def quality_check_worker(text: str):
    """ë³‘ë ¬ ì²˜ë¦¬ìš© í’ˆì§ˆ ì²´í¬ ì›Œì»¤"""
    # í”„ë¡œì„¸ìŠ¤ë³„ Kiwi ì¸ìŠ¤í„´ìŠ¤ (ì¬ì‚¬ìš©)
    if not hasattr(quality_check_worker, '_kiwi'):
        quality_check_worker._kiwi = Kiwi()

    kiwi = quality_check_worker._kiwi

    try:
        # NNP ë¹„ìœ¨ ì²´í¬
        tokens = kiwi.tokenize(text)
        nnp_count = sum(1 for t in tokens if t.tag == 'NNP')
        nnp_ratio = nnp_count / max(1, len(tokens))

        if nnp_ratio < 0.05 or nnp_ratio > 0.20:
            return None

        # ë¬¸ì¥ ìˆ˜ ì²´í¬
        sentences = text.count('.') + text.count('?') + text.count('!')
        if sentences < 3:
            return None

        return text
    except:
        return None


# ============================================================================
# Coref ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ ì¶”ê°€)
# ============================================================================

def create_coref_examples_with_limit(
    text: str,
    repeated_entities: Dict[str, List[int]],
    entity_counter: Counter,
    max_entity_freq: int = 1000
) -> List[Dict[str, Any]]:
    """
    ë°˜ë³µ ê°œì²´ì—ì„œ Coref í•™ìŠµ ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        repeated_entities: {"í™ê¸¸ë™": [0, 45, 100]}
        entity_counter: ì „ì²´ ê°œì²´ ë¹ˆë„ ì¹´ìš´í„°
        max_entity_freq: ê°œì²´ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

    Returns:
        ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    """
    examples = []

    for entity, positions in repeated_entities.items():
        # ê°œì²´ ë¹ˆë„ ì œí•œ ì²´í¬
        if entity_counter[entity] >= max_entity_freq:
            continue

        # ì²« ë²ˆì§¸ëŠ” ì„ í–‰ì‚¬ë¡œ ìœ ì§€
        antecedent_pos = positions[0]

        # 2ë²ˆì§¸ ì´ìƒ ë“±ì¥ì„ [MASK]ë¡œ ì¹˜í™˜ (ìµœëŒ€ 3ê°œ)
        for coref_pos in positions[1:4]:
            # ê°œì²´ë³„ ë¹ˆë„ ì¬ì²´í¬
            if entity_counter[entity] >= max_entity_freq:
                break

            # ê±°ë¦¬ ê³„ì‚°
            distance = coref_pos - antecedent_pos

            # ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ì œì™¸ (10ì ë¯¸ë§Œ)
            if distance < 10:
                continue

            # ë„ˆë¬´ ë©€ë©´ ì œì™¸ (2000ì ì´ˆê³¼)
            if distance > 2000:
                continue

            # [MASK] ìƒì„±
            text_before = text[:coref_pos]
            text_after = text[coref_pos + len(entity):]
            masked_text = text_before + "[MASK]" + text_after

            examples.append({
                "text": masked_text,
                "target": entity,
                "antecedent_pos": antecedent_pos,
                "coref_pos": coref_pos,
                "distance": distance
            })

            # ë¹ˆë„ ì¦ê°€
            entity_counter[entity] += 1

    return examples


# ============================================================================
# ë°ì´í„°ì…‹ë³„ ì²˜ë¦¬
# ============================================================================

def process_naver_news_gen(num_workers: int = 20) -> List[str]:
    """dev7halo/naver-news-summarization-ko-with-gen (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Naver News (ìµœê³  í’ˆì§ˆ)")
    print("=" * 80)

    dataset = load_dataset("dev7halo/naver-news-summarization-ko-with-gen", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['document'] for s in dataset if 800 <= len(s['document']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„°
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


def process_hplt_korean(num_workers: int = 20) -> List[str]:
    """HPLT/HPLT2.0_cleaned kor_Hang (2ë‹¨ê³„ ë³‘ë ¬ í•„í„°ë§, ê°œìˆ˜ ì œí•œ ì—†ìŒ)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š HPLT Korean (ê³ ì† ë³‘ë ¬ í•„í„°ë§, ì „ì²´ ë°ì´í„°)")
    print("=" * 80)

    dataset = load_dataset("HPLT/HPLT2.0_cleaned", "kor_Hang", split="train", streaming=True)

    # 1ë‹¨ê³„: ë¹ ë¥¸ ìˆ˜ì§‘ (ê¸¸ì´ë§Œ ì²´í¬)
    print(f"[1/2] ë¹ ë¥¸ ìˆ˜ì§‘ (ê¸¸ì´ í•„í„°ë§Œ)")

    raw_texts = []
    scanned = 0
    start_time = time.time()

    for sample in dataset:
        scanned += 1
        text = sample['text']

        # ê¸¸ì´ í•„í„°ë§Œ (ì´ˆê³ ì†)
        if 800 <= len(text) <= 2500:
            raw_texts.append(text)

        # ì§„í–‰ ìƒí™©
        if scanned % 50000 == 0:
            elapsed = time.time() - start_time
            rate = scanned / elapsed
            print(f"  ìŠ¤ìº”: {scanned:,} | ìˆ˜ì§‘: {len(raw_texts):,} | ì†ë„: {rate:.0f} docs/s")

    elapsed1 = time.time() - start_time
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(raw_texts):,}ê°œ ìˆ˜ì§‘ ({elapsed1:.1f}ì´ˆ)")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§
    print(f"\n[2/2] ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§ ({num_workers} ì›Œì»¤)")
    step2_start = time.time()

    with Pool(processes=num_workers) as pool:
        results = pool.map(quality_check_worker, raw_texts, chunksize=500)

    # None ì œê±° (ê°œìˆ˜ ì œí•œ ì—†ìŒ)
    filtered_texts = [r for r in results if r is not None]

    elapsed2 = time.time() - step2_start
    total_elapsed = time.time() - start_time

    print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(filtered_texts):,}ê°œ ì„ ë³„ ({elapsed2:.1f}ì´ˆ)")
    print(f"   í’ˆì§ˆ í†µê³¼ìœ¨: {len(filtered_texts)/len(raw_texts)*100:.1f}%")
    print(f"â±ï¸  ì´ ì‹œê°„: {total_elapsed:.1f}ì´ˆ")

    return filtered_texts


def process_air_bench_news(num_workers: int = 20) -> List[str]:
    """AIR-Bench/qa_news_ko (2ë‹¨ê³„ ë³‘ë ¬ í•„í„°ë§, ê°œìˆ˜ ì œí•œ ì—†ìŒ)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š AIR-Bench QA News (ê³ ì† ë³‘ë ¬ í•„í„°ë§, ì „ì²´ ë°ì´í„°)")
    print("=" * 80)

    dataset = load_dataset("AIR-Bench/qa_news_ko", split="corpus_default", streaming=True)

    # 1ë‹¨ê³„: ë¹ ë¥¸ ìˆ˜ì§‘ (ê¸¸ì´ë§Œ ì²´í¬)
    print(f"[1/2] ë¹ ë¥¸ ìˆ˜ì§‘ (ê¸¸ì´ í•„í„°ë§Œ)")

    raw_texts = []
    scanned = 0
    start_time = time.time()

    for sample in dataset:
        scanned += 1
        text = sample['text']

        if 800 <= len(text) <= 2500:
            raw_texts.append(text)

        if scanned % 50000 == 0:
            elapsed = time.time() - start_time
            rate = scanned / elapsed
            print(f"  ìŠ¤ìº”: {scanned:,} | ìˆ˜ì§‘: {len(raw_texts):,} | ì†ë„: {rate:.0f} docs/s")

    elapsed1 = time.time() - start_time
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(raw_texts):,}ê°œ ìˆ˜ì§‘ ({elapsed1:.1f}ì´ˆ)")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§
    print(f"\n[2/2] ë³‘ë ¬ í’ˆì§ˆ í•„í„°ë§ ({num_workers} ì›Œì»¤)")
    step2_start = time.time()

    with Pool(processes=num_workers) as pool:
        results = pool.map(quality_check_worker, raw_texts, chunksize=500)

    # None ì œê±° (ê°œìˆ˜ ì œí•œ ì—†ìŒ)
    filtered_texts = [r for r in results if r is not None]

    elapsed2 = time.time() - step2_start
    total_elapsed = time.time() - start_time

    print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(filtered_texts):,}ê°œ ì„ ë³„ ({elapsed2:.1f}ì´ˆ)")
    print(f"   í’ˆì§ˆ í†µê³¼ìœ¨: {len(filtered_texts)/len(raw_texts)*100:.1f}%")
    print(f"â±ï¸  ì´ ì‹œê°„: {total_elapsed:.1f}ì´ˆ")

    return filtered_texts


def process_finance_news(num_workers: int = 20) -> List[str]:
    """nmixx-fin/twice_kr_finance_news_summ (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Finance News (ê¸ˆìœµ)")
    print("=" * 80)

    dataset = load_dataset("nmixx-fin/twice_kr_finance_news_summ", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['text'] for s in dataset if 800 <= len(s['text']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„°
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


def process_bqa_news(num_workers: int = 20) -> List[str]:
    """nmixx-fin/twice_kr_news_bqa_cls (ë³‘ë ¬ í’ˆì§ˆ í•„í„°)"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š BQA News (ê²½ì œ)")
    print("=" * 80)

    dataset = load_dataset("nmixx-fin/twice_kr_news_bqa_cls", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # 1ë‹¨ê³„: ê¸¸ì´ í•„í„° (800-2500ì)
    texts = [s['text'] for s in dataset if 800 <= len(s['text']) <= 2500]
    print(f"   ê¸¸ì´ í•„í„° í›„: {len(texts):,}ê°œ")

    # 2ë‹¨ê³„: ë³‘ë ¬ í’ˆì§ˆ í•„í„°
    print(f"   í’ˆì§ˆ í•„í„°ë§ ì¤‘ ({num_workers} ì›Œì»¤)...")
    with Pool(processes=num_workers) as pool:
        results = pool.map(quality_check_worker, texts, chunksize=100)

    filtered = [r for r in results if r is not None]

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered):,}ê°œ")
    return filtered


# ============================================================================
# ë©”ì¸ ì²˜ë¦¬
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Entity Coref Dataset V2 ìƒì„± (ê°œìˆ˜ ì œí•œ ì—†ìŒ)")
    parser.add_argument("--seq-len", type=int, default=2048,
                        help="ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 2048)")
    parser.add_argument("--output-dir", default="./prepared_datasets",
                        help="ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--num-workers", type=int, default=None,
                        help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base",
                        help="í† í¬ë‚˜ì´ì € ëª¨ë¸")
    parser.add_argument("--max-entity-freq", type=int, default=1000,
                        help="ê°œì²´ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 1000, í¸í–¥ ë°©ì§€)")

    args = parser.parse_args()

    # ì›Œì»¤ ìˆ˜
    if args.num_workers is None:
        args.num_workers = max(4, cpu_count() - 4)

    print("\n" + "=" * 80)
    print("ğŸš€ Entity Coreference Dataset V2 ìƒì„±")
    print("=" * 80)
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_len}")
    print(f"ì›Œì»¤ ìˆ˜: {args.num_workers}")
    print(f"ê°œì²´ë‹¹ ìµœëŒ€ ë¹ˆë„: {args.max_entity_freq}")
    print(f"ê°œìˆ˜ ì œí•œ: ì—†ìŒ (ì „ì²´ ê³ í’ˆì§ˆ ë°ì´í„° í™œìš©)")

    overall_start = time.time()

    # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë³‘ë ¬ í’ˆì§ˆ í•„í„° ì ìš©)
    print("\n" + "=" * 80)
    print("ğŸ“¥ 1ë‹¨ê³„: ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (5ê°œ ë°ì´í„°ì…‹ + ë³‘ë ¬ í•„í„°)")
    print("=" * 80)

    all_texts = []

    # Dataset 1: Naver News Gen (ìµœê³  í’ˆì§ˆ)
    texts1 = process_naver_news_gen(args.num_workers)
    all_texts.extend(texts1)

    # Dataset 2: HPLT Korean (ëŒ€ê·œëª¨, ì „ì²´)
    texts2 = process_hplt_korean(args.num_workers)
    all_texts.extend(texts2)

    # Dataset 3: AIR-Bench News (100ë§Œ+, ì „ì²´)
    texts3 = process_air_bench_news(args.num_workers)
    all_texts.extend(texts3)

    # Dataset 4: Finance News
    texts4 = process_finance_news(args.num_workers)
    all_texts.extend(texts4)

    # Dataset 5: BQA News
    texts5 = process_bqa_news(args.num_workers)
    all_texts.extend(texts5)

    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸: {len(all_texts):,}ê°œ (ê³ í’ˆì§ˆ í•„í„° ì ìš© ì™„ë£Œ)")

    # 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (ë³‘ë ¬)
    print("\n" + "=" * 80)
    print(f"ğŸ” 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (ë³‘ë ¬ ì²˜ë¦¬, {args.num_workers} ì›Œì»¤)")
    print("=" * 80)

    step2_start = time.time()

    # ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=args.num_workers) as pool:
        results = pool.map(
            worker_find_repetitions,
            [(text, i) for i, text in enumerate(all_texts)],
            chunksize=100
        )

    # None ì œê±°
    results = [r for r in results if r is not None]

    step2_elapsed = time.time() - step2_start

    print(f"âœ… ë°˜ë³µ ê°œì²´ ë°œê²¬: {len(results):,}/{len(all_texts):,} í…ìŠ¤íŠ¸ "
          f"({len(results)/len(all_texts)*100:.1f}%)")
    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {step2_elapsed:.1f}ì´ˆ")

    # 3ë‹¨ê³„: Coref ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)
    print("\n" + "=" * 80)
    print("ğŸ”¨ 3ë‹¨ê³„: Coreference ìƒ˜í”Œ ìƒì„± (ê°œì²´ ë¹ˆë„ ì œí•œ)")
    print("=" * 80)

    step3_start = time.time()

    entity_counter = Counter()
    all_examples = []

    for idx, text, repeated in results:
        examples = create_coref_examples_with_limit(
            text, repeated, entity_counter, args.max_entity_freq
        )
        all_examples.extend(examples)

        # ì§„í–‰ ìƒí™© (10000ê°œë§ˆë‹¤)
        if len(all_examples) % 10000 == 0:
            print(f"  ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,} (ê³ ìœ  ê°œì²´: {len(entity_counter):,})")

    step3_elapsed = time.time() - step3_start

    print(f"\nâœ… ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}")
    print(f"âœ… ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}")
    print(f"â±ï¸  ìƒì„± ì‹œê°„: {step3_elapsed:.1f}ì´ˆ")

    # í†µê³„
    print("\n" + "=" * 80)
    print("ğŸ“Š ë°ì´í„°ì…‹ í†µê³„")
    print("=" * 80)

    distances = [ex['distance'] for ex in all_examples]
    targets = [ex['target'] for ex in all_examples]

    print(f"  ê±°ë¦¬ (ì„ í–‰ì‚¬ â†” ìƒí˜¸ì°¸ì¡°):")
    print(f"    í‰ê· : {np.mean(distances):.1f} ë¬¸ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(distances):.1f} ë¬¸ì")
    print(f"    ë²”ìœ„: {np.min(distances)} ~ {np.max(distances)} ë¬¸ì")

    # ê±°ë¦¬ ë¶„í¬
    short = sum(1 for d in distances if d < 200)
    medium = sum(1 for d in distances if 200 <= d < 800)
    long = sum(1 for d in distances if d >= 800)
    print(f"\n  ê±°ë¦¬ ë¶„í¬:")
    print(f"    Short (10-200ì): {short:,} ({short/len(distances)*100:.1f}%)")
    print(f"    Medium (200-800ì): {medium:,} ({medium/len(distances)*100:.1f}%)")
    print(f"    Long (800-2000ì): {long:,} ({long/len(distances)*100:.1f}%)")

    print(f"\n  ê°œì²´(target) ë¶„í¬:")
    print(f"    ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}ê°œ")
    print(f"    Top 10 ë¹ˆë„:")
    for target, count in entity_counter.most_common(10):
        print(f"      '{target}': {count:,}ê°œ ({count/len(targets)*100:.2f}%)")

    # ê°œì²´ ê¸¸ì´ ë¶„í¬
    entity_lengths = [len(t) for t in targets]
    print(f"\n  ê°œì²´ ê¸¸ì´ ë¶„í¬:")
    print(f"    í‰ê· : {np.mean(entity_lengths):.1f} ê¸€ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(entity_lengths):.1f} ê¸€ì")
    print(f"    ë²”ìœ„: {np.min(entity_lengths)} ~ {np.max(entity_lengths)} ê¸€ì")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n  ìƒ˜í”Œ ì˜ˆì‹œ:")
    for i, ex in enumerate(random.sample(all_examples, min(3, len(all_examples))), 1):
        preview = ex['text'][:100] + "..." if len(ex['text']) > 100 else ex['text']
        print(f"    {i}. ê°œì²´: '{ex['target']}' (ê±°ë¦¬: {ex['distance']} ë¬¸ì)")
        print(f"       í…ìŠ¤íŠ¸: {preview}")
        print()

    # 4ë‹¨ê³„: í† í°í™” ë° ì €ì¥
    print("\n" + "=" * 80)
    print("ğŸ’¾ 4ë‹¨ê³„: í† í°í™” ë° ì €ì¥")
    print("=" * 80)

    tokenizer = AutoTokenizer.from_pretrained(args.model)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    step4_start = time.time()

    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    texts = [ex['text'] for ex in all_examples]

    # Dataset ìƒì„±
    dataset = Dataset.from_dict({"text": texts})

    # í† í°í™”
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=args.seq_len,
        )

    tokenized = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=2000,
        remove_columns=["text"],
        num_proc=args.num_workers,
        desc=f"í† í°í™” (seq_len={args.seq_len})"
    )

    # ì €ì¥
    save_path = output_dir / f"entity_coref_v2_{args.seq_len}"
    save_path.mkdir(parents=True, exist_ok=True)
    tokenized.save_to_disk(str(save_path))

    step4_elapsed = time.time() - step4_start

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(tokenized):,}")
    print(f"â±ï¸  ì‹œê°„: {step4_elapsed:.1f}ì´ˆ")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    meta_path = save_path / "metadata.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump({
            "dataset_type": "entity_coreference_v2",
            "version": "2.0",
            "num_samples": len(all_examples),
            "seq_len": args.seq_len,
            "created_at": datetime.now().isoformat(),
            "source_datasets": [
                "dev7halo/naver-news-summarization-ko-with-gen",
                "HPLT/HPLT2.0_cleaned (kor_Hang)",
                "AIR-Bench/qa_news_ko",
                "nmixx-fin/twice_kr_finance_news_summ",
                "nmixx-fin/twice_kr_news_bqa_cls"
            ],
            "num_unique_entities": len(entity_counter),
            "max_entity_freq": args.max_entity_freq,
            "top_entities": dict(entity_counter.most_common(20)),
            "entity_length_stats": {
                "mean": float(np.mean(entity_lengths)),
                "median": float(np.median(entity_lengths)),
                "min": int(np.min(entity_lengths)),
                "max": int(np.max(entity_lengths))
            },
            "distance_stats": {
                "mean": float(np.mean(distances)),
                "median": float(np.median(distances)),
                "min": int(np.min(distances)),
                "max": int(np.max(distances))
            },
            "distance_distribution": {
                "short_10_200": short,
                "medium_200_800": medium,
                "long_800_2000": long
            }
        }, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ ë©”íƒ€ë°ì´í„°: {meta_path}")

    overall_elapsed = time.time() - overall_start

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì „ì²´ ì‹œê°„: {overall_elapsed:.1f}ì´ˆ ({overall_elapsed/60:.1f}ë¶„)")
    print(f"ğŸ“Š ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}ê°œ")
    print(f"ğŸ¯ ê³ ìœ  ê°œì²´ ìˆ˜: {len(entity_counter):,}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")

    print("\nğŸ“– ë‹¤ìŒ ë‹¨ê³„:")
    print("=" * 80)
    print("í›ˆë ¨ ì‹¤í–‰:")
    print()
    print(f"PYTHONNOUSERSITE=1 uv run python scripts/run_entity_coref_finetune.py \\")
    print(f"  --checkpoint runs/combined_experiment/checkpoint-1230 \\")
    print(f"  --dataset {save_path} \\")
    print(f"  --epochs 5 \\")
    print(f"  --batch-size 8 \\")
    print(f"  --gradient-accumulation 8 \\")
    print(f"  --lr 1.5e-5 \\")
    print(f"  --eval-steps 200 \\")
    print(f"  --output-dir runs/entity_coref_v2")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Entity Coreference Fine-tuning
===============================

ê¸°ì¡´ MLM ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œì‘í•˜ì—¬ Entity Replacement Coref ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ

íŠ¹ì§•:
- ì²´í¬í¬ì¸íŠ¸ ìë™ ë¡œë“œ (ì‹œí€€ìŠ¤ ê¸¸ì´ ë³´ì¡´)
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
- ETA ê³„ì‚°
- ì¤‘ê°„ í‰ê°€ (Coref@5, Coref F1)
- ìƒì„¸ ë¡œê·¸
"""

import os
import sys
import json
import time
import math
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

import torch
from datasets import load_from_disk, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForMaskedLM,
    AutoConfig,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    pipeline,
    TrainerCallback,
)
from transformers.trainer_callback import TrainerControl, TrainerState
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# coref_automl í‰ê°€ í•¨ìˆ˜ import
from coref_automl.tune import (
    build_eval_from_lambada as build_lambada_eval,
    build_real_coref_eval_set,
    eval_lambada_topk as eval_lambada,
    eval_real_coref_top1,
    eval_real_coref_top5,
)


# ============================================================================
# ì²´í¬í¬ì¸íŠ¸ ê°ì§€
# ============================================================================

def detect_seq_len_from_checkpoint(checkpoint_path: str) -> Optional[int]:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì§€"""
    config_path = Path(checkpoint_path) / "config.json"
    if config_path.exists():
        try:
            with open(config_path, encoding="utf-8") as f:
                config = json.load(f)
            seq_len = config.get("max_position_embeddings")
            if seq_len:
                print(f"âœ“ ê°ì§€ëœ seq_len={seq_len} from checkpoint")
                return int(seq_len)
        except Exception as e:
            print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ config ì½ê¸° ì‹¤íŒ¨: {e}")

    # HuggingFace ëª¨ë¸
    try:
        config = AutoConfig.from_pretrained(checkpoint_path)
        seq_len = getattr(config, "max_position_embeddings", None)
        if seq_len:
            print(f"âœ“ ê°ì§€ëœ seq_len={seq_len} from model")
            return int(seq_len)
    except:
        pass

    return None


# ============================================================================
# í‰ê°€ í•¨ìˆ˜ë“¤ì€ coref_automl.tuneì—ì„œ import
# ============================================================================


# ============================================================================
# ì‹¤ì‹œê°„ ì½œë°±
# ============================================================================

class DetailedProgressCallback(TrainerCallback):
    """ì‹¤ì‹œê°„ ìƒí™© í‘œì‹œ ì½œë°± with Real@1/Real@5 í‰ê°€"""

    def __init__(self, model, tokenizer, seq_len, output_dir):
        self.start_time = None
        self.step_times = []
        self.last_log_step = 0
        self.model = model
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.output_dir = output_dir
        self.eval_history = []  # í‰ê°€ ê²°ê³¼ ì €ì¥

    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        self.start_time = time.time()
        print("\n" + "=" * 80)
        print("ğŸš€ í›ˆë ¨ ì‹œì‘!")
        print("=" * 80)
        print(f"ì´ ìŠ¤í… ìˆ˜: {state.max_steps}")
        print(f"ë°°ì¹˜ í¬ê¸°: {args.per_device_train_batch_size}")
        print(f"Gradient accumulation: {args.gradient_accumulation_steps}")
        print(f"í‰ê°€ ê°„ê²©: {args.eval_steps} ìŠ¤í…")
        num_evals = state.max_steps // args.eval_steps
        print(f"ì˜ˆìƒ í‰ê°€ íšŸìˆ˜: {num_evals}íšŒ")
        print("=" * 80 + "\n")

    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        current_time = time.time()
        elapsed = current_time - self.start_time

        # 10 ìŠ¤í…ë§ˆë‹¤ ìƒì„¸ ì •ë³´ ì¶œë ¥
        if state.global_step % 10 == 0 and state.global_step != self.last_log_step:
            self.last_log_step = state.global_step

            # í‰ê·  ì‹œê°„ ê³„ì‚°
            if len(self.step_times) > 0:
                avg_step_time = sum(self.step_times[-50:]) / len(self.step_times[-50:])
            else:
                avg_step_time = elapsed / max(1, state.global_step)

            self.step_times.append(avg_step_time)

            # ì§„í–‰ë¥ 
            progress = state.global_step / state.max_steps * 100

            # ETA
            remaining_steps = state.max_steps - state.global_step
            eta_seconds = avg_step_time * remaining_steps
            eta = timedelta(seconds=int(eta_seconds))

            # í˜„ì¬ loss
            loss = state.log_history[-1].get('loss', 0.0) if state.log_history else 0.0

            # ë‹¤ìŒ í‰ê°€ê¹Œì§€ ë‚¨ì€ ìŠ¤í…
            steps_to_next_eval = args.eval_steps - (state.global_step % args.eval_steps)
            if steps_to_next_eval == args.eval_steps:
                steps_to_next_eval = 0

            print(f"ğŸ“Š [Step {state.global_step:4d}/{state.max_steps}] "
                  f"ì§„í–‰: {progress:5.1f}% | Loss: {loss:.4f} | "
                  f"ì†ë„: {1/avg_step_time:.1f} steps/s | ETA: {eta} | "
                  f"ë‹¤ìŒ í‰ê°€: {steps_to_next_eval}ìŠ¤í… í›„")

    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        print("\n" + "=" * 80)
        print(f"ğŸ“Š ì¤‘ê°„ í‰ê°€ ì‹œì‘ (Step {state.global_step})")
        print("=" * 80)

        eval_start = time.time()

        # Fill-mask pipeline ìƒì„±
        device = 0 if torch.cuda.is_available() else -1
        fill = pipeline("fill-mask", model=self.model, tokenizer=self.tokenizer, device=device)
        mask_token = self.tokenizer.mask_token or "[MASK]"

        # LAMBADA í‰ê°€ (ê°„ë‹¨íˆ 100ê°œë§Œ)
        print("\nğŸ“– [1/3] LAMBADA í‰ê°€ (100 ìƒ˜í”Œ)...")
        lbd_eval = build_lambada_eval(limit=100, seed=42)
        lbd_t1 = eval_lambada(fill, lbd_eval, mask_token=mask_token, k=1, batch_size=64, seq_len=self.seq_len)
        print(f"   âœ“ LAMBADA@1 = {lbd_t1:.4f} ({lbd_t1*100:.2f}%)")

        # Real Coref í‰ê°€ (200ê°œ ìƒ˜í”Œ)
        print("\nğŸ”— [2/3] Real Coref ì„¸íŠ¸ êµ¬ì¶• (200 ìƒ˜í”Œ)...")
        coref_limit = 200
        eval_coref = build_real_coref_eval_set(limit=coref_limit, seed=999, max_seq_len=self.seq_len)
        print(f"   âœ“ {len(eval_coref)} ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ")

        # Real@1
        print("\nğŸ”— [3a/3] Real@1 ê³„ì‚°...")
        real1 = eval_real_coref_top1(fill, eval_coref, mask_token=mask_token, batch_size=64, seq_len=self.seq_len)
        print(f"   âœ“ Real@1 = {real1:.4f} ({real1*100:.2f}%)")

        # Real@5
        print("\nğŸ”— [3b/3] Real@5 ê³„ì‚°...")
        real5 = eval_real_coref_top5(fill, eval_coref, mask_token=mask_token, batch_size=64, seq_len=self.seq_len)
        print(f"   âœ“ Real@5 = {real5:.4f} ({real5*100:.2f}%)")

        eval_elapsed = time.time() - eval_start

        # ê²°ê³¼ ì €ì¥
        eval_result = {
            'step': state.global_step,
            'lambada_top1': lbd_t1,
            'real1': real1,
            'real5': real5,
            'time': eval_elapsed
        }
        self.eval_history.append(eval_result)

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "â”€" * 80)
        print(f"âœ… í‰ê°€ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {eval_elapsed:.1f}ì´ˆ)")
        print("â”€" * 80)
        print(f"   LAMBADA@1: {lbd_t1*100:.2f}%")
        print(f"   Real@1:    {real1*100:.2f}%")
        print(f"   Real@5:    {real5*100:.2f}%")

        # ì´ì „ í‰ê°€ì™€ ë¹„êµ
        if len(self.eval_history) > 1:
            prev = self.eval_history[-2]
            real1_delta = (real1 - prev['real1']) * 100
            real5_delta = (real5 - prev['real5']) * 100
            print(f"\n   ë³€í™” (ì´ì „ í‰ê°€ ëŒ€ë¹„):")
            print(f"   Real@1: {real1_delta:+.2f}%p")
            print(f"   Real@5: {real5_delta:+.2f}%p")

        print("â”€" * 80 + "\n")

        # ì¤‘ê°„ í‰ê°€ ê²°ê³¼ ì €ì¥
        eval_log_path = Path(self.output_dir) / "eval_history.json"
        with open(eval_log_path, 'w', encoding='utf-8') as f:
            json.dump(self.eval_history, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ì¤‘ê°„ í‰ê°€ ê²°ê³¼ ì €ì¥: {eval_log_path}\n")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        del fill
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        if logs and 'eval_loss' in logs:
            print(f"   Eval Loss: {logs.get('eval_loss', 0.0):.4f}")


# ============================================================================
# ë©”ì¸
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Entity Coreference Fine-tuning")
    parser.add_argument("--checkpoint", required=True,
                        help="ì‹œì‘ ì²´í¬í¬ì¸íŠ¸ (ì˜ˆ: runs/combined_experiment/checkpoint-410)")
    parser.add_argument("--dataset", required=True,
                        help="Coref ë°ì´í„°ì…‹ ê²½ë¡œ (ì˜ˆ: prepared_datasets/entity_coref_2048)")
    parser.add_argument("--seq-len", type=int, default=None,
                        help="ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìë™ ê°ì§€)")
    parser.add_argument("--epochs", type=int, default=5,
                        help="ì—í­ ìˆ˜ (ê¸°ë³¸: 5)")
    parser.add_argument("--batch-size", type=int, default=16,
                        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 16)")
    parser.add_argument("--gradient-accumulation", type=int, default=2,
                        help="Gradient accumulation steps (ê¸°ë³¸: 2)")
    parser.add_argument("--lr", type=float, default=2e-5,
                        help="í•™ìŠµë¥  (ê¸°ë³¸: 2e-5)")
    parser.add_argument("--warmup-ratio", type=float, default=0.1,
                        help="Warmup ë¹„ìœ¨ (ê¸°ë³¸: 0.1)")
    parser.add_argument("--eval-steps", type=int, default=200,
                        help="í‰ê°€ ê°„ê²© (ê¸°ë³¸: 200)")
    parser.add_argument("--output-dir", default="./runs/entity_coref_finetune",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--run-name", default=None,
                        help="ì‹¤í–‰ ì´ë¦„ (ê¸°ë³¸: ìë™ ìƒì„±)")

    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ğŸš€ Entity Coreference Fine-tuning")
    print("=" * 80)
    print(f"ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")
    print(f"ë°ì´í„°ì…‹: {args.dataset}")
    print(f"ì—í­: {args.epochs}")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"í•™ìŠµë¥ : {args.lr}")

    overall_start = time.time()

    # 1. ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì§€
    if args.seq_len is None:
        args.seq_len = detect_seq_len_from_checkpoint(args.checkpoint)
        if args.seq_len is None:
            print("âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --seq-lenìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”.")
            sys.exit(1)

    print(f"âœ“ ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_len}")

    # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    load_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)

    # â˜… DeBERTa ì²´í¬í¬ì¸íŠ¸ì¸ì§€ í™•ì¸
    is_deberta_checkpoint = "deberta" in args.checkpoint.lower() or args.seq_len > 512

    if is_deberta_checkpoint and args.seq_len > 512:
        # DeBERTa with extended sequence length
        print(f"   ì²´í¬í¬ì¸íŠ¸ê°€ í™•ì¥ëœ ì‹œí€€ìŠ¤ ê¸¸ì´({args.seq_len})ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤...")
        print(f"   ignore_mismatched_sizes=Trueë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")

        model = AutoModelForMaskedLM.from_pretrained(args.checkpoint, ignore_mismatched_sizes=True)

        # rel_embeddings ìˆ˜ë™ ë¡œë“œ
        from safetensors import safe_open
        checkpoint_path = Path(args.checkpoint) / "model.safetensors"
        if checkpoint_path.exists():
            with safe_open(str(checkpoint_path), framework='pt', device='cpu') as f:
                if 'deberta.encoder.rel_embeddings.weight' in f.keys():
                    rel_embed_weight = f.get_tensor('deberta.encoder.rel_embeddings.weight')
                    print(f"   rel_embeddings ìˆ˜ë™ ë¡œë“œ: {rel_embed_weight.shape}")

                    new_size, embed_dim = rel_embed_weight.shape
                    new_rel = torch.nn.Embedding(new_size, embed_dim)
                    new_rel.weight.data = rel_embed_weight.clone()
                    model.deberta.encoder.rel_embeddings = new_rel.to(model.device)

        model.config.max_position_embeddings = args.seq_len
        print(f"   âœ“ ëª¨ë¸ max_position_embeddings: {model.config.max_position_embeddings}")
    else:
        model = AutoModelForMaskedLM.from_pretrained(args.checkpoint)

    # í† í¬ë‚˜ì´ì € max_length ì—…ë°ì´íŠ¸
    tokenizer.model_max_length = args.seq_len

    load_elapsed = time.time() - load_start
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_elapsed:.1f}ì´ˆ)")
    print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    print(f"   Tokenizer max_length: {tokenizer.model_max_length}")

    # 3. ë°ì´í„°ì…‹ ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset_start = time.time()

    dataset = load_from_disk(args.dataset)

    # Train/eval split (90/10)
    split = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = split['train']
    eval_dataset = split['test']

    dataset_elapsed = time.time() - dataset_start
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ ({dataset_elapsed:.1f}ì´ˆ)")
    print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}")
    print(f"   í‰ê°€ ìƒ˜í”Œ: {len(eval_dataset):,}")

    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=0.15
    )

    # 4. Training arguments
    if args.run_name is None:
        args.run_name = f"coref_finetune_{args.seq_len}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    output_dir = Path(args.output_dir) / args.run_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # ì´ ìŠ¤í… ê³„ì‚°
    total_steps = (len(train_dataset) // (args.batch_size * args.gradient_accumulation)) * args.epochs
    num_evals = total_steps // args.eval_steps

    print(f"\nğŸ“Š í›ˆë ¨ ì„¤ì •:")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    print(f"   í‰ê°€ ê°„ê²©: {args.eval_steps} ìŠ¤í…")
    print(f"   ì˜ˆìƒ í‰ê°€ íšŸìˆ˜: {num_evals}íšŒ")
    print(f"   Effective batch size: {args.batch_size * args.gradient_accumulation}")

    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.lr,
        weight_decay=0.01,
        warmup_ratio=args.warmup_ratio,
        logging_steps=10,
        eval_steps=args.eval_steps,
        eval_strategy="steps",
        save_steps=args.eval_steps,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        fp16=torch.cuda.is_available(),
        report_to="none",
        dataloader_num_workers=4,
    )

    # 5. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[DetailedProgressCallback(model, tokenizer, args.seq_len, output_dir)],
    )

    # 6. í›ˆë ¨
    print("\n" + "=" * 80)
    print("ğŸš€ Fine-tuning ì‹œì‘!")
    print("=" * 80)

    train_start = time.time()
    trainer.train()
    train_elapsed = time.time() - train_start

    print("\n" + "=" * 80)
    print(f"âœ… Fine-tuning ì™„ë£Œ! ({train_elapsed/60:.1f}ë¶„)")
    print("=" * 80)

    # 7. ìµœì¢… í‰ê°€ (Real Coref metrics)
    print("\n" + "=" * 80)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ì‹œì‘...")
    print("=" * 80)
    print("âœ“ í‰ê°€ ë°©ì‹: Real Coref (Task-aligned)")
    print("  - ë°˜ë³µ ëª…ì‚¬ â†’ ëª…ì‚¬ ìì²´ ì˜ˆì¸¡")
    print("  - ëŒ€ëª…ì‚¬ ì œì™¸ (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ íŒ¨í„´)")
    print("  - Wikipedia streaming (seed=999)")
    print("=" * 80)

    eval_start = time.time()

    device = 0 if torch.cuda.is_available() else -1
    fill = pipeline("fill-mask", model=model, tokenizer=tokenizer, device=device)
    mask_token = tokenizer.mask_token or "[MASK]"

    # LAMBADA í‰ê°€
    print("\nğŸ“– [1/3] LAMBADA í‰ê°€ (600 ìƒ˜í”Œ)...")
    lbd_start = time.time()
    eval_lbd = build_lambada_eval(limit=600, seed=42)
    lbd_t1 = eval_lambada(
        fill,
        eval_lbd,
        mask_token=mask_token,
        k=1,
        batch_size=64,
        seq_len=args.seq_len,
    )
    lbd_elapsed = time.time() - lbd_start
    print(f"   âœ“ LAMBADA@1 = {lbd_t1:.4f} ({lbd_elapsed:.1f}ì´ˆ)")

    # Real Coref í‰ê°€ ì„¸íŠ¸ êµ¬ì¶•
    print("\nğŸ”— [2/3] Real Coref í‰ê°€ ì„¸íŠ¸ êµ¬ì¶•...")
    print("   ìƒ˜í”Œ ìˆ˜: LAMBADA 600ê°œ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ëœ coref ìƒ˜í”Œ")
    print("   ë°ì´í„° ì†ŒìŠ¤: Wikipedia streaming (seed=999)")
    coref_build_start = time.time()

    # seq_lenì— ë”°ë¼ coref_limit ì¡°ì •
    coref_limit = 1600 if args.seq_len == 1536 else 1600
    eval_coref = build_real_coref_eval_set(
        limit=coref_limit,
        seed=999,  # í›ˆë ¨ ë°ì´í„°ì™€ ë‹¤ë¥¸ seed
        max_seq_len=args.seq_len
    )
    coref_build_elapsed = time.time() - coref_build_start
    actual_samples = len(eval_coref)
    print(f"   âœ“ Real Coref ì„¸íŠ¸: {actual_samples} ìƒ˜í”Œ ({coref_build_elapsed:.1f}ì´ˆ)")

    # Real@1 ê³„ì‚°
    print("\nğŸ”— [3a/3] Real@1 ê³„ì‚°...")
    real1_start = time.time()
    real1 = eval_real_coref_top1(
        fill,
        eval_coref,
        mask_token=mask_token,
        batch_size=64,
        seq_len=args.seq_len,
    )
    real1_elapsed = time.time() - real1_start
    print(f"   âœ“ Real@1 = {real1:.4f} ({real1_elapsed:.1f}ì´ˆ)")

    # Real@5 ê³„ì‚°
    print("\nğŸ”— [3b/3] Real@5 ê³„ì‚°...")
    real5_start = time.time()
    real5 = eval_real_coref_top5(
        fill,
        eval_coref,
        mask_token=mask_token,
        batch_size=64,
        seq_len=args.seq_len,
    )
    real5_elapsed = time.time() - real5_start
    print(f"   âœ“ Real@5 = {real5:.4f} ({real5_elapsed:.1f}ì´ˆ)")

    eval_elapsed = time.time() - eval_start

    # 8. ê²°ê³¼ ì €ì¥
    results = {
        "checkpoint": args.checkpoint,
        "dataset": args.dataset,
        "seq_len": args.seq_len,
        "epochs": args.epochs,
        "lambada_top1": lbd_t1,
        "real1": real1,
        "real5": real5,
        "coref_samples": actual_samples,
        "train_time_minutes": train_elapsed / 60,
        "eval_time_seconds": eval_elapsed,
        "finished_at": datetime.now().isoformat()
    }

    results_path = output_dir / "final_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 80)
    print("âœ… ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    print(f"LAMBADA@1: {lbd_t1:.4f} ({lbd_t1*100:.2f}%)")
    print(f"Real@1:    {real1:.4f} ({real1*100:.2f}%)")
    print(f"Real@5:    {real5:.4f} ({real5*100:.2f}%)")
    print(f"Coref ìƒ˜í”Œ: {actual_samples}")
    print(f"\ní›ˆë ¨ ì‹œê°„: {train_elapsed/60:.1f}ë¶„")
    print(f"í‰ê°€ ì‹œê°„: {eval_elapsed:.1f}ì´ˆ")
    print(f"\nì €ì¥ ê²½ë¡œ: {output_dir}")
    print(f"ê²°ê³¼ íŒŒì¼: {results_path}")

    overall_elapsed = time.time() - overall_start
    print(f"\nâ±ï¸  ì „ì²´ ì‹œê°„: {overall_elapsed/60:.1f}ë¶„")

    # ë¹„êµ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ")
    print("=" * 80)
    print("ì´ì „ ë² ìŠ¤íŠ¸ (checkpoint-1600, seq_len=2048):")
    print("  - Real@1: 67.78%")
    print("  - Real@5: 82.44%")
    print(f"\ní˜„ì¬ ëª¨ë¸ (seq_len={args.seq_len}):")
    print(f"  - Real@1: {real1*100:.2f}%")
    print(f"  - Real@5: {real5*100:.2f}%")

    # ê°œì„ ë¥  ê³„ì‚°
    prev_real1 = 0.6778
    prev_real5 = 0.8244
    real1_improvement = ((real1 - prev_real1) / prev_real1) * 100
    real5_improvement = ((real5 - prev_real5) / prev_real5) * 100

    print(f"\në³€í™”:")
    print(f"  - Real@1: {real1_improvement:+.2f}%")
    print(f"  - Real@5: {real5_improvement:+.2f}%")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Entity Replacement Coreference ë°ì´í„°ì…‹ ìƒì„±
============================================

ë°˜ë³µëœ ê°œì²´(entity)ë¥¼ ì°¾ì•„ì„œ ë‘ ë²ˆì§¸ ë“±ì¥ì„ ëŒ€ëª…ì‚¬ë¡œ ì¹˜í™˜í•˜ì—¬
ëª…í™•í•œ ìƒí˜¸ì°¸ì¡° í•™ìŠµ ë°ì´í„° ìƒì„±

ì „ëµ:
1. ì™„ì „ ë°˜ë³µ ê°œì²´ ì°¾ê¸° (í™ê¸¸ë™...í™ê¸¸ë™)
2. ë‘ ë²ˆì§¸ë¥¼ "ê·¸", "ì´", "ê·¸ê²ƒ" ë“±ìœ¼ë¡œ ì¹˜í™˜
3. [MASK]ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±

ë°ì´í„° ì†ŒìŠ¤:
- Wikipedia (ë°˜ë³µ íŒ¨í„´ ë§ìŒ)
- KLUE MRC (ê°œì²´ ë°˜ë³µ ë§ìŒ)
- Naver News (ì¸ëª…/ì¡°ì§ëª… ë°˜ë³µ ë§ìŒ)
"""

import os
import sys
import json
import gc
import random
from pathlib import Path
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
from typing import Dict, Any, List, Tuple
from multiprocessing import Pool, cpu_count
import argparse
from datetime import datetime
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ============================================================================
# ë°˜ë³µ ê°œì²´ ì°¾ê¸°
# ============================================================================

def find_exact_repetitions(text: str, kiwi) -> Dict[str, List[int]]:
    """
    ì™„ì „íˆ ê°™ì€ ê³ ìœ ëª…ì‚¬ê°€ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ê²½ìš°ë§Œ ì°¾ê¸°

    Returns:
        {"í™ê¸¸ë™": [0, 45, 100], "ì‚¼ì„±ì „ì": [200, 350]}
    """
    try:
        tokens = kiwi.tokenize(text)
    except:
        return {}

    entity_positions = {}

    for token in tokens:
        # NNP (ê³ ìœ ëª…ì‚¬)ë§Œ
        if token.tag == 'NNP' and len(token.form) >= 2:
            entity = token.form

            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            if entity in ['ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ì ', 'ë…„', 'ì›”', 'ì¼']:
                continue

            if entity not in entity_positions:
                entity_positions[entity] = []
            entity_positions[entity].append(token.start)

    # 2ë²ˆ ì´ìƒ ë“±ì¥í•œ ê²ƒë§Œ
    repeated = {e: pos for e, pos in entity_positions.items() if len(pos) >= 2}

    return repeated


def worker_find_repetitions(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ì›Œì»¤"""
    text, idx = args

    # í”„ë¡œì„¸ìŠ¤ë³„ Kiwi ì¸ìŠ¤í„´ìŠ¤
    if not hasattr(worker_find_repetitions, '_kiwi'):
        worker_find_repetitions._kiwi = Kiwi()

    kiwi = worker_find_repetitions._kiwi

    repeated = find_exact_repetitions(text, kiwi)

    if repeated:
        return (idx, text, repeated)
    return None


# ============================================================================
# Coref ìƒ˜í”Œ ìƒì„±
# ============================================================================

def create_coref_examples(text: str, repeated_entities: Dict[str, List[int]]) -> List[Dict[str, Any]]:
    """
    ë°˜ë³µ ê°œì²´ì—ì„œ Coref í•™ìŠµ ìƒ˜í”Œ ìƒì„± (Option 2: ê°œì²´ ì§ì ‘ ì˜ˆì¸¡)

    ì „ëµ:
    - ì›ë³¸: "í™ê¸¸ë™ì€ í•™ìƒì´ë‹¤. í™ê¸¸ë™ì€ ê³µë¶€í•œë‹¤."
    - ìƒì„±: "í™ê¸¸ë™ì€ í•™ìƒì´ë‹¤. [MASK]ëŠ” ê³µë¶€í•œë‹¤."
    - ì •ë‹µ: "í™ê¸¸ë™" (ê°œì²´ ìì²´)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        repeated_entities: {"í™ê¸¸ë™": [0, 45, 100]}

    Returns:
        [{
            "text": "í™ê¸¸ë™ì€ í•™ìƒì´ë‹¤. [MASK]ëŠ” ê³µë¶€í•œë‹¤.",
            "target": "í™ê¸¸ë™",
            "antecedent_pos": 0,
            "coref_pos": 45,
            "distance": 45
        }, ...]
    """
    examples = []

    for entity, positions in repeated_entities.items():
        # ì²« ë²ˆì§¸ëŠ” ì„ í–‰ì‚¬ë¡œ ìœ ì§€
        antecedent_pos = positions[0]

        # 2ë²ˆì§¸ ì´ìƒ ë“±ì¥ì„ [MASK]ë¡œ ì¹˜í™˜ (ìµœëŒ€ 3ê°œ)
        for coref_pos in positions[1:4]:
            # ê±°ë¦¬ ê³„ì‚°
            distance = coref_pos - antecedent_pos

            # ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ì œì™¸ (10ì ë¯¸ë§Œ)
            if distance < 10:
                continue

            # ë„ˆë¬´ ë©€ë©´ ì œì™¸ (2000ì ì´ˆê³¼)
            if distance > 2000:
                continue

            # [MASK] ìƒì„±
            # coref_posì˜ entityë¥¼ [MASK]ë¡œ ì§ì ‘ ì¹˜í™˜
            text_before = text[:coref_pos]
            text_after = text[coref_pos + len(entity):]
            masked_text = text_before + "[MASK]" + text_after

            examples.append({
                "text": masked_text,
                "target": entity,  # â˜… í•µì‹¬: ê°œì²´ ìì²´ê°€ ì •ë‹µ
                "antecedent_pos": antecedent_pos,
                "coref_pos": coref_pos,
                "distance": distance
            })

    return examples


# ============================================================================
# ë°ì´í„°ì…‹ë³„ ì²˜ë¦¬
# ============================================================================

def process_wikipedia(max_samples: int = 100000, num_workers: int = 20) -> List[str]:
    """Wikipediaì—ì„œ ë°˜ë³µ ê°œì²´ ìˆëŠ” í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Wikipedia ì²˜ë¦¬ ì¤‘...")
    print("=" * 80)

    dataset = load_dataset("wikimedia/wikipedia", "20231101.ko", split="train", streaming=True)

    texts_with_repetitions = []
    scanned = 0

    print(f"ğŸ” ìŠ¤ìº” ì¤‘ (ëª©í‘œ: {max_samples:,}ê°œ ë¬¸ì„œ)...")

    start_time = time.time()

    for sample in dataset:
        scanned += 1
        text = sample['text']

        # ê¸¸ì´ í•„í„°
        if len(text) < 300 or len(text) > 3000:
            continue

        # \n\në¡œ ë‹¨ë½ ë¶„ë¦¬
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para = para.strip()
            if len(para) < 300:
                continue

            texts_with_repetitions.append(para)

            if len(texts_with_repetitions) >= max_samples:
                break

        if len(texts_with_repetitions) >= max_samples:
            break

        # ì§„í–‰ ìƒí™©
        if scanned % 1000 == 0:
            elapsed = time.time() - start_time
            rate = scanned / elapsed
            remaining = (max_samples - len(texts_with_repetitions)) / rate if rate > 0 else 0
            print(f"  ìŠ¤ìº”: {scanned:,} ë¬¸ì„œ | ìˆ˜ì§‘: {len(texts_with_repetitions):,}/{max_samples:,} | "
                  f"ì†ë„: {rate:.1f} docs/s | ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")

    print(f"âœ… Wikipedia ìˆ˜ì§‘ ì™„ë£Œ: {len(texts_with_repetitions):,}ê°œ ë‹¨ë½")
    return texts_with_repetitions


def process_klue_mrc(num_workers: int = 20) -> List[str]:
    """KLUE MRCì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š KLUE MRC ì²˜ë¦¬ ì¤‘...")
    print("=" * 80)

    dataset = load_dataset("klue", "mrc", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # contextë§Œ ì¶”ì¶œ (ê¸¸ì´ í•„í„°)
    texts = [s['context'] for s in dataset if 300 <= len(s['context']) <= 3000]

    print(f"âœ… KLUE MRC ìˆ˜ì§‘ ì™„ë£Œ: {len(texts):,}ê°œ")
    return texts


def process_naver_news(num_workers: int = 20) -> List[str]:
    """Naver Newsì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""

    print("\n" + "=" * 80)
    print(f"ğŸ“Š Naver News ì²˜ë¦¬ ì¤‘...")
    print("=" * 80)

    dataset = load_dataset("daekeun-ml/naver-news-summarization-ko", split="train")
    print(f"âœ… ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")

    # documentë§Œ ì¶”ì¶œ (ê¸¸ì´ í•„í„°)
    texts = [s['document'] for s in dataset if 300 <= len(s['document']) <= 3000]

    print(f"âœ… Naver News ìˆ˜ì§‘ ì™„ë£Œ: {len(texts):,}ê°œ")
    return texts


# ============================================================================
# ë©”ì¸ ì²˜ë¦¬
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Entity Replacement Coreference ë°ì´í„°ì…‹ ìƒì„±")
    parser.add_argument("--seq-lengths", nargs="+", type=int, default=[1536, 2048],
                        help="ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--datasets", nargs="+",
                        choices=["wikipedia", "klue_mrc", "naver_news", "all"],
                        default=["all"], help="ì‚¬ìš©í•  ë°ì´í„°ì…‹")
    parser.add_argument("--output-dir", default="./prepared_datasets",
                        help="ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--target-samples", type=int, default=40000,
                        help="ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 40,000)")
    parser.add_argument("--wiki-docs", type=int, default=100000,
                        help="Wikipedia ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸: 100,000)")
    parser.add_argument("--num-workers", type=int, default=None,
                        help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--model", default="kakaobank/kf-deberta-base",
                        help="í† í¬ë‚˜ì´ì € ëª¨ë¸")

    args = parser.parse_args()

    # ì›Œì»¤ ìˆ˜
    if args.num_workers is None:
        args.num_workers = max(4, cpu_count() - 4)

    print("\n" + "=" * 80)
    print("ğŸš€ Entity Replacement Coreference ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 80)
    print(f"ëª©í‘œ ìƒ˜í”Œ ìˆ˜: {args.target_samples:,}")
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_lengths}")
    print(f"ì›Œì»¤ ìˆ˜: {args.num_workers}")
    print(f"ë°ì´í„°ì…‹: {args.datasets}")

    overall_start = time.time()

    # ë°ì´í„°ì…‹ ì„ íƒ
    datasets_to_use = args.datasets
    if "all" in datasets_to_use:
        datasets_to_use = ["wikipedia", "klue_mrc", "naver_news"]

    # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    print("\n" + "=" * 80)
    print("ğŸ“¥ 1ë‹¨ê³„: ì›ë³¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘")
    print("=" * 80)

    all_texts = []

    if "wikipedia" in datasets_to_use:
        wiki_texts = process_wikipedia(args.wiki_docs, args.num_workers)
        all_texts.extend(wiki_texts)

    if "klue_mrc" in datasets_to_use:
        klue_texts = process_klue_mrc(args.num_workers)
        all_texts.extend(klue_texts)

    if "naver_news" in datasets_to_use:
        news_texts = process_naver_news(args.num_workers)
        all_texts.extend(news_texts)

    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸: {len(all_texts):,}ê°œ")

    # 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (ë³‘ë ¬)
    print("\n" + "=" * 80)
    print(f"ğŸ” 2ë‹¨ê³„: ë°˜ë³µ ê°œì²´ ì°¾ê¸° (ë³‘ë ¬ ì²˜ë¦¬, {args.num_workers} ì›Œì»¤)")
    print("=" * 80)

    step2_start = time.time()

    # ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=args.num_workers) as pool:
        results = pool.map(
            worker_find_repetitions,
            [(text, i) for i, text in enumerate(all_texts)],
            chunksize=100
        )

    # None ì œê±°
    results = [r for r in results if r is not None]

    step2_elapsed = time.time() - step2_start

    print(f"âœ… ë°˜ë³µ ê°œì²´ ë°œê²¬: {len(results):,}/{len(all_texts):,} í…ìŠ¤íŠ¸ "
          f"({len(results)/len(all_texts)*100:.1f}%)")
    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {step2_elapsed:.1f}ì´ˆ")

    # 3ë‹¨ê³„: Coref ìƒ˜í”Œ ìƒì„±
    print("\n" + "=" * 80)
    print("ğŸ”¨ 3ë‹¨ê³„: Coreference ìƒ˜í”Œ ìƒì„±")
    print("=" * 80)

    step3_start = time.time()

    all_examples = []
    for idx, text, repeated in results:
        examples = create_coref_examples(text, repeated)
        all_examples.extend(examples)

        if len(all_examples) >= args.target_samples:
            break

    # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ë§Œí¼ ìë¥´ê¸°
    all_examples = all_examples[:args.target_samples]

    step3_elapsed = time.time() - step3_start

    print(f"âœ… ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}")
    print(f"â±ï¸  ìƒì„± ì‹œê°„: {step3_elapsed:.1f}ì´ˆ")

    # í†µê³„
    print("\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
    distances = [ex['distance'] for ex in all_examples]
    targets = [ex['target'] for ex in all_examples]

    import numpy as np
    print(f"  ê±°ë¦¬ (ì„ í–‰ì‚¬ â†” ìƒí˜¸ì°¸ì¡°):")
    print(f"    í‰ê· : {np.mean(distances):.1f} ë¬¸ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(distances):.1f} ë¬¸ì")
    print(f"    ë²”ìœ„: {np.min(distances)} ~ {np.max(distances)} ë¬¸ì")

    print(f"\n  ê°œì²´(target) ë¶„í¬:")
    from collections import Counter
    target_counts = Counter(targets)
    print(f"    ê³ ìœ  ê°œì²´ ìˆ˜: {len(target_counts):,}ê°œ")
    print(f"    Top 10 ë¹ˆë„:")
    for target, count in target_counts.most_common(10):
        print(f"      '{target}': {count:,}ê°œ ({count/len(targets)*100:.1f}%)")

    # ê°œì²´ ê¸¸ì´ ë¶„í¬
    entity_lengths = [len(t) for t in targets]
    print(f"\n  ê°œì²´ ê¸¸ì´ ë¶„í¬:")
    print(f"    í‰ê· : {np.mean(entity_lengths):.1f} ê¸€ì")
    print(f"    ì¤‘ì•™ê°’: {np.median(entity_lengths):.1f} ê¸€ì")
    print(f"    ë²”ìœ„: {np.min(entity_lengths)} ~ {np.max(entity_lengths)} ê¸€ì")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n  ìƒ˜í”Œ ì˜ˆì‹œ:")
    for i, ex in enumerate(random.sample(all_examples, min(3, len(all_examples))), 1):
        preview = ex['text'][:100] + "..." if len(ex['text']) > 100 else ex['text']
        print(f"    {i}. ê°œì²´: '{ex['target']}' (ê±°ë¦¬: {ex['distance']} ë¬¸ì)")
        print(f"       í…ìŠ¤íŠ¸: {preview}")
        print()

    # 4ë‹¨ê³„: ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì €ì¥
    print("\n" + "=" * 80)
    print("ğŸ’¾ 4ë‹¨ê³„: í† í°í™” ë° ì €ì¥")
    print("=" * 80)

    tokenizer = AutoTokenizer.from_pretrained(args.model)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    for seq_len in args.seq_lengths:
        print(f"\nğŸ“¦ ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len} ì²˜ë¦¬ ì¤‘...")
        step4_start = time.time()

        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        texts = [ex['text'] for ex in all_examples]

        # Dataset ìƒì„±
        dataset = Dataset.from_dict({"text": texts})

        # í† í°í™”
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=seq_len,
            )

        tokenized = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=2000,
            remove_columns=["text"],
            num_proc=args.num_workers,
            desc=f"í† í°í™” (seq_len={seq_len})"
        )

        # ì €ì¥
        save_path = output_dir / f"entity_coref_{seq_len}"
        save_path.mkdir(parents=True, exist_ok=True)
        tokenized.save_to_disk(str(save_path))

        step4_elapsed = time.time() - step4_start

        print(f"  âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        print(f"  ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(tokenized):,}")
        print(f"  â±ï¸  ì‹œê°„: {step4_elapsed:.1f}ì´ˆ")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta_path = save_path / "metadata.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump({
                "dataset_type": "entity_replacement_coreference",
                "num_samples": len(all_examples),
                "seq_len": seq_len,
                "created_at": datetime.now().isoformat(),
                "source_datasets": datasets_to_use,
                "num_unique_entities": len(target_counts),
                "top_entities": dict(target_counts.most_common(20)),
                "entity_length_stats": {
                    "mean": float(np.mean(entity_lengths)),
                    "median": float(np.median(entity_lengths)),
                    "min": int(np.min(entity_lengths)),
                    "max": int(np.max(entity_lengths))
                },
                "distance_stats": {
                    "mean": float(np.mean(distances)),
                    "median": float(np.median(distances)),
                    "min": int(np.min(distances)),
                    "max": int(np.max(distances))
                }
            }, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ“ ë©”íƒ€ë°ì´í„°: {meta_path}")

    overall_elapsed = time.time() - overall_start

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì „ì²´ ì‹œê°„: {overall_elapsed:.1f}ì´ˆ ({overall_elapsed/60:.1f}ë¶„)")
    print(f"ğŸ“Š ìƒì„±ëœ ìƒ˜í”Œ: {len(all_examples):,}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ:")
    for seq_len in args.seq_lengths:
        print(f"  - {output_dir}/entity_coref_{seq_len}/")

    print("\nğŸ“– ë‹¤ìŒ ë‹¨ê³„:")
    print("=" * 80)
    print("Fine-tuning ì‹¤í–‰:")
    print()
    print("# seq_len=2048")
    print(f"uv run python scripts/run_entity_coref_finetune.py \\")
    print(f"  --checkpoint runs/combined_experiment/checkpoint-410 \\")
    print(f"  --dataset {output_dir}/entity_coref_2048 \\")
    print(f"  --seq-len 2048 \\")
    print(f"  --epochs 3")
    print()
    print("# seq_len=1536")
    print(f"uv run python scripts/run_entity_coref_finetune.py \\")
    print(f"  --checkpoint runs/combined_experiment/checkpoint-396 \\")
    print(f"  --dataset {output_dir}/entity_coref_1536 \\")
    print(f"  --seq-len 1536 \\")
    print(f"  --epochs 3")


if __name__ == "__main__":
    main()

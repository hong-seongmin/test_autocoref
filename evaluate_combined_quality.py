"""
ê²°í•©ëœ ë°ì´í„°ì…‹ í’ˆì§ˆ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- combined_coref_1536, combined_coref_2048 ë¶„ì„
"""

from datasets import load_from_disk
from transformers import AutoTokenizer
from kiwipiepy import Kiwi
import numpy as np
import random

def analyze_text_quality(text: str, kiwi: Kiwi) -> dict:
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„"""
    if not text or len(text) < 50:
        return None

    # Kiwi ë¶„ì„
    text_sample = text[:1500]
    try:
        tokens = kiwi.tokenize(text_sample)
    except:
        return None

    pronouns = []
    entities = []
    verbs = []
    meaningful_words = []

    for token in tokens:
        if token.tag == 'NP':  # ëŒ€ëª…ì‚¬
            pronouns.append(token.form)
        elif token.tag in ['NNG', 'NNP', 'NNB']:  # ëª…ì‚¬ë¥˜
            if len(token.form) > 1:
                entities.append(token.form)
        elif token.tag.startswith(('VV', 'VA')):  # ë™ì‚¬/í˜•ìš©ì‚¬
            verbs.append(token.form)

        if token.tag.startswith(('N', 'V', 'M', 'VA', 'VV')):
            meaningful_words.append(token.form)

    total_words = len(meaningful_words)
    if total_words == 0:
        return None

    return {
        'text_len': len(text),
        'pronoun_count': len(pronouns),
        'entity_count': len(entities),
        'verb_count': len(verbs),
        'total_words': total_words,
        'pronoun_density': len(pronouns) / total_words * 100,
        'entity_density': len(entities) / total_words * 100,
        'unique_pronouns': len(set(pronouns)),
        'unique_entities': len(set(entities)),
        'pronoun_entity_ratio': len(pronouns) / max(1, len(entities)),
        'pronouns': pronouns,
        'entities': entities,
    }


def decode_and_analyze(dataset, tokenizer, kiwi, seq_len: int, sample_size: int = 200):
    """í† í°í™”ëœ ë°ì´í„°ì…‹ì„ ë””ì½”ë”©í•˜ì—¬ ë¶„ì„"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š combined_coref_{seq_len} í’ˆì§ˆ ë¶„ì„ (ìƒ˜í”Œ: {sample_size}ê°œ)")
    print(f"{'='*80}")

    # ëœë¤ ìƒ˜í”Œë§
    total_samples = len(dataset)
    if sample_size > total_samples:
        sample_size = total_samples

    indices = random.sample(range(total_samples), sample_size)

    print(f"âœ… ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ")
    print(f"ğŸ” ë¶„ì„ ìƒ˜í”Œ ìˆ˜: {sample_size}ê°œ")
    print(f"\në¶„ì„ ì¤‘...")

    # í…ìŠ¤íŠ¸ ë””ì½”ë”©
    qualities = []
    text_lengths = []
    decoded_texts = []

    for i, idx in enumerate(indices):
        sample = dataset[idx]

        # ë””ì½”ë”©
        input_ids = sample['input_ids']
        # PAD í† í° ì œê±°
        non_pad_ids = [tid for tid in input_ids if tid != tokenizer.pad_token_id]
        decoded = tokenizer.decode(non_pad_ids, skip_special_tokens=True)

        text_lengths.append(len(decoded))
        decoded_texts.append(decoded)

        # í’ˆì§ˆ ë¶„ì„
        quality = analyze_text_quality(decoded, kiwi)
        if quality:
            qualities.append(quality)

        if (i + 1) % 50 == 0:
            print(f"  ì§„í–‰: {i+1}/{sample_size}")

    # í†µê³„ ê³„ì‚°
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ: {len(qualities)}ê°œ ìœ íš¨ ìƒ˜í”Œ")

    if not qualities:
        print("âš ï¸ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # ê¸°ë³¸ í†µê³„
    print(f"\n{'='*80}")
    print("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    print(f"{'='*80}")
    print(f"í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜):")
    print(f"  í‰ê· : {np.mean(text_lengths):.0f}ì")
    print(f"  ì¤‘ì•™ê°’: {np.median(text_lengths):.0f}ì")
    print(f"  ë²”ìœ„: {np.min(text_lengths):.0f} ~ {np.max(text_lengths):.0f}ì")

    # ëŒ€ëª…ì‚¬ í†µê³„
    pronoun_counts = [q['pronoun_count'] for q in qualities]
    pronoun_densities = [q['pronoun_density'] for q in qualities]

    print(f"\nëŒ€ëª…ì‚¬ ê°œìˆ˜:")
    print(f"  í‰ê· : {np.mean(pronoun_counts):.1f}ê°œ")
    print(f"  ì¤‘ì•™ê°’: {np.median(pronoun_counts):.0f}ê°œ")
    print(f"  ë²”ìœ„: {np.min(pronoun_counts):.0f} ~ {np.max(pronoun_counts):.0f}ê°œ")

    print(f"\nëŒ€ëª…ì‚¬ ë°€ë„:")
    print(f"  í‰ê· : {np.mean(pronoun_densities):.2f}%")
    print(f"  ì¤‘ì•™ê°’: {np.median(pronoun_densities):.2f}%")
    print(f"  ë²”ìœ„: {np.min(pronoun_densities):.2f}% ~ {np.max(pronoun_densities):.2f}%")
    print(f"  ëª©í‘œ ë²”ìœ„(1.5-3.0%) ë‚´: {sum(1.5 <= d <= 3.0 for d in pronoun_densities)}/{len(pronoun_densities)} ({sum(1.5 <= d <= 3.0 for d in pronoun_densities)/len(pronoun_densities)*100:.1f}%)")

    # Entity í†µê³„
    entity_counts = [q['entity_count'] for q in qualities]
    entity_densities = [q['entity_density'] for q in qualities]

    print(f"\nEntity ê°œìˆ˜:")
    print(f"  í‰ê· : {np.mean(entity_counts):.1f}ê°œ")
    print(f"  ì¤‘ì•™ê°’: {np.median(entity_counts):.0f}ê°œ")
    print(f"  ë²”ìœ„: {np.min(entity_counts):.0f} ~ {np.max(entity_counts):.0f}ê°œ")

    # Pronoun:Entity ë¹„ìœ¨
    ratios = [q['pronoun_entity_ratio'] for q in qualities]
    print(f"\nPronoun:Entity ë¹„ìœ¨:")
    print(f"  í‰ê· : {np.mean(ratios):.3f} ({np.mean(ratios)*100:.1f}%)")
    print(f"  ì¤‘ì•™ê°’: {np.median(ratios):.3f} ({np.median(ratios)*100:.1f}%)")
    print(f"  ë²”ìœ„: {np.min(ratios):.3f} ~ {np.max(ratios):.3f}")
    print(f"  ëª©í‘œ ë²”ìœ„(0.01-0.15) ë‚´: {sum(0.01 <= r <= 0.15 for r in ratios)}/{len(ratios)} ({sum(0.01 <= r <= 0.15 for r in ratios)/len(ratios)*100:.1f}%)")

    # Unique í†µê³„
    unique_pronouns = [q['unique_pronouns'] for q in qualities]
    unique_entities = [q['unique_entities'] for q in qualities]

    print(f"\nUnique ëŒ€ëª…ì‚¬:")
    print(f"  í‰ê· : {np.mean(unique_pronouns):.1f}ê°œ")
    print(f"  ì¤‘ì•™ê°’: {np.median(unique_pronouns):.0f}ê°œ")

    print(f"\nUnique Entity:")
    print(f"  í‰ê· : {np.mean(unique_entities):.1f}ê°œ")
    print(f"  ì¤‘ì•™ê°’: {np.median(unique_entities):.0f}ê°œ")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n{'='*80}")
    print("ğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì˜ˆì‹œ (3ê°œ)")
    print(f"{'='*80}")

    for i in range(min(3, len(decoded_texts))):
        text = decoded_texts[i]
        quality = qualities[i] if i < len(qualities) else None

        print(f"\n[ìƒ˜í”Œ {i+1}]")
        print(f"ê¸¸ì´: {len(text)}ì")
        if quality:
            print(f"ëŒ€ëª…ì‚¬: {quality['pronoun_count']}ê°œ (ë°€ë„: {quality['pronoun_density']:.2f}%)")
            print(f"Entity: {quality['entity_count']}ê°œ")
            print(f"ë¹„ìœ¨: {quality['pronoun_entity_ratio']:.3f}")
            print(f"ëŒ€ëª…ì‚¬ ì˜ˆì‹œ: {', '.join(quality['pronouns'][:10])}")
        print(f"\ní…ìŠ¤íŠ¸ (ì• 300ì):")
        print(f"{text[:300]}...")

    return qualities


def main():
    print("=" * 80)
    print("ğŸ” ê²°í•©ëœ ë°ì´í„°ì…‹ í’ˆì§ˆ í‰ê°€")
    print("=" * 80)

    # ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„°ì…‹ ë¡œë”©...")
    ds_1536 = load_from_disk('prepared_datasets/combined_coref_1536')
    ds_2048 = load_from_disk('prepared_datasets/combined_coref_2048')

    print("\nğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë”©...")
    tokenizer = AutoTokenizer.from_pretrained('kakaobank/kf-deberta-base')

    print("\nğŸ“¥ Kiwi ì´ˆê¸°í™”...")
    kiwi = Kiwi()

    # ê¸°ë³¸ ì •ë³´
    print(f"\n{'='*80}")
    print("ğŸ“Š ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´")
    print(f"{'='*80}")
    print(f"combined_coref_1536: {len(ds_1536):,}ê°œ ìƒ˜í”Œ (íŒŒì¼ í¬ê¸°: 446MB)")
    print(f"combined_coref_2048: {len(ds_2048):,}ê°œ ìƒ˜í”Œ (íŒŒì¼ í¬ê¸°: 616MB)")
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(ds_1536) + len(ds_2048):,}ê°œ")

    # ë¶„ì„ (ê°ê° 200ê°œ ìƒ˜í”Œ)
    qualities_1536 = decode_and_analyze(ds_1536, tokenizer, kiwi, 1536, sample_size=200)
    qualities_2048 = decode_and_analyze(ds_2048, tokenizer, kiwi, 2048, sample_size=200)

    # ì¢…í•© í‰ê°€
    print(f"\n{'='*80}")
    print("ğŸ¯ ì¢…í•© í‰ê°€")
    print(f"{'='*80}")

    if qualities_1536 and qualities_2048:
        all_densities = [q['pronoun_density'] for q in qualities_1536 + qualities_2048]
        all_ratios = [q['pronoun_entity_ratio'] for q in qualities_1536 + qualities_2048]

        print(f"\nì „ì²´ í‰ê· :")
        print(f"  ëŒ€ëª…ì‚¬ ë°€ë„: {np.mean(all_densities):.2f}% (ëª©í‘œ: 1.5-3.0%)")
        print(f"  Pronoun:Entity ë¹„ìœ¨: {np.mean(all_ratios):.3f} ({np.mean(all_ratios)*100:.1f}%) (ëª©í‘œ: 0.01-0.15)")

        # í’ˆì§ˆ íŒì •
        density_ok = 1.5 <= np.mean(all_densities) <= 3.5
        ratio_ok = 0.01 <= np.mean(all_ratios) <= 0.15

        print(f"\ní’ˆì§ˆ í‰ê°€:")
        status_density = "âœ… ì–‘í˜¸" if density_ok else "âš ï¸ ë²”ìœ„ ë²—ì–´ë‚¨"
        status_ratio = "âœ… ì–‘í˜¸" if ratio_ok else "âš ï¸ ë²”ìœ„ ë²—ì–´ë‚¨"
        print(f"  ëŒ€ëª…ì‚¬ ë°€ë„: {status_density}")
        print(f"  Pronoun:Entity ë¹„ìœ¨: {status_ratio}")

        if density_ok and ratio_ok:
            print(f"\nâœ… ì „ì²´ í‰ê°€: ìš°ìˆ˜ - Coreference resolution í•™ìŠµì— ì í•©")
        elif density_ok or ratio_ok:
            print(f"\nâš ï¸ ì „ì²´ í‰ê°€: ë³´í†µ - ì¼ë¶€ ê°œì„  í•„ìš”")
        else:
            print(f"\nâŒ ì „ì²´ í‰ê°€: ê°œì„  í•„ìš” - í•„í„°ë§ ê¸°ì¤€ ì¬ì¡°ì • ê¶Œì¥")

    print(f"\n{'='*80}")
    print("í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*80}")


if __name__ == "__main__":
    random.seed(42)
    main()
